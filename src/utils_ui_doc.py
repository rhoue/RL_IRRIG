"""
Utilitaires pour la documentation de l'interface utilisateur.

Ce module contient les fonctions qui g√©n√®rent le contenu des onglets de documentation
de l'application Streamlit. Chaque fonction correspond √† un onglet de documentation
et retourne le contenu markdown/HTML √† afficher.
"""

import streamlit as st


def render_doc_irrigation_intelligente():
    """
    Affiche le contenu de l'onglet de documentation : Irrigation Intelligente.
    """
    st.markdown('<h2 class="section-header">üíß Pourquoi s\'int√©resser √† l\'irrigation intelligente ?</h2>', unsafe_allow_html=True)
    
    #st.image("images/logo_uttop.jpg", width=200)
        
    st.markdown("""
    Dans un contexte de changement climatique, l'eau devient une ressource rare, co√ªteuse et de plus en plus incertaine. 
    Les agriculteurs doivent arbitrer entre :
    
    - **Maintenir un niveau d'humidit√© adapt√© √† la culture** : assurer une croissance optimale et un rendement satisfaisant
    - **√âconomiser l'eau** : respecter les quotas et r√©duire les co√ªts
    - **√âviter la lixiviation des nutriments** : pr√©venir les pertes en cas d'arrosage excessif
    
    ### Pratiques actuelles
    
    Aujourd'hui, de nombreux irrigants prennent leurs d√©cisions √† partir de :
    - **Seuils simples de tension** : ex. "si la tension d√©passe 80 cbar, j'irrigue"
    - **Calendriers d'irrigation** : programmes fixes bas√©s sur l'exp√©rience
    - **Exp√©rience personnelle** : intuition et connaissance du terrain
    """)
    
    st.markdown("### Opportunit√©s technologiques")
    
    st.markdown("""
    <div style="background-color: #E3F2FD; padding: 20px; border-radius: 10px; border-left: 4px solid #2196F3; margin: 15px 0;">
    <p style="margin: 0 0 10px 0;"><strong>Avec l'essor de :</strong></p>
    <ul style="margin: 0 0 15px 0; padding-left: 20px;">
    <li><strong>Tensiom√®tres</strong> : mesure de la tension matricielle du sol</li>
    <li><strong>Pr√©visions m√©t√©o</strong> : donn√©es de pluie et d'√©vapotranspiration</li>
    <li><strong>Techniques d'IA</strong> : apprentissage automatique et optimisation</li>
    </ul>
    <p style="margin: 10px 0 0 0;"><strong>La question devient :</strong></p>
    <blockquote style="margin: 15px 0 0 0; padding: 10px 15px; background-color: #BBDEFB; border-left: 3px solid #2196F3; border-radius: 5px; font-style: italic;">
    <strong>"Peut-on apprendre automatiquement une politique d'irrigation qui utilise les tensions mesur√©es, 
    respecte la physique du sol, et s'adapte √† la parcelle r√©elle ?"</strong>
    </blockquote>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("""
    ### Approche propos√©e
    
    Le travail pr√©sent√© ici combine :
    - **Mod√®les physiques** : simulation du bilan hydrique du sol
    - **Mod√®les neuronaux** : Neural ODE / Neural CDE pour la correction
    - **Apprentissage par renforcement (RL)** : pour piloter l'irrigation √† partir des s√©ries temporelles de tension de l'eau ($\\psi_t$)
    """)


def render_doc_variables_etat():
    """
    Affiche le contenu de l'onglet de documentation : Variables d'√âtat.
    """
    st.markdown('<h2 class="section-header">üìä Les variables cl√©s : ce que l\'on mesure vraiment</h2>', unsafe_allow_html=True)
    
    st.markdown("""
    Une saison culturale est d√©crite jour par jour : $t = 0,1,\\dots,T$
    
    ### Variable effectivement mesur√©e : la tension de l'eau
    
    Le tensiom√®tre donne directement :
    
    $$\\psi_t \\quad (\\text{cbar})$$
    
    La tension est la force que la plante doit exercer pour extraire l'eau.
    C'est la **variable observ√©e**, et c'est aussi ce que "ressent" r√©ellement la culture.
    
    ### Variable interne non observ√©e : la r√©serve en eau du sol
    
    Le mod√®le physique du bilan hydrique travaille pourtant avec une variable interne :
    
    $$S_t \\quad (\\text{mm})$$
    
    C'est la quantit√© d'eau stock√©e dans la zone racinaire.
    Mais **nous ne l'observons jamais directement**.
    
    ### Comment relier les deux ?
    
    On utilise une **courbe de r√©tention** propre au sol :
    
    $$\\psi_t = f_{\\text{retention}}(S_t) \\quad\\text{et id√©alement}\\quad S_t = f_{\\text{retention}}^{-1}(\\psi_t)$$
    
    - Le tensiom√®tre mesure $\\psi_t$.
    - Le mod√®le reconstruit une estimation de $S_t$.
    - Le bilan hydrique agit sur $S_t$.
    - Puis on reconvertit en $\\psi_{t+1}$ pour comparaison.
    
    C'est une architecture √† **√©tats cach√©s**, courante en agro-hydrologie.
    """)
    
    st.markdown("### Param√®tres de sol (p√©dophysique)")
    st.markdown("""
    - $Z_r$ : profondeur de la zone racinaire (mm)
    - $\\theta_s, \\ \\theta_{fc}, \\ \\theta_{wp}$ : saturation, capacit√© au champ, point de fl√©trissement
    - $S_{\\max} = \\theta_s \\cdot Z_r$, $S_{fc} = \\theta_{fc} \\cdot Z_r$, $S_{wp} = \\theta_{wp} \\cdot Z_r$
    """)
    
    st.markdown("### Flux entrants")
    st.markdown("""
    - $I_t$ : dose d'irrigation (mm)
    - $\\eta_I$ : efficacit√© d'irrigation (0‚Äì1)
    - $R_t$ : pluie (mm)
    - $G_t$ : remont√©e capillaire (optionnelle, mm)
    """)
    
    st.markdown("### Flux sortants")
    st.markdown("""
    - $ET0_t$ : √©vapotranspiration de r√©f√©rence (mm/j)
    - $Kc_t$ : coefficient cultural (adimensionnel) - repr√©sente la demande en eau de la culture selon son stade de d√©veloppement
    - $f_{ET}(\\psi_t)$ : facteur de stress hydrique (0‚Äì1)
    - $ETc_t = Kc_t \\cdot ET0_t \\cdot f_{ET}(\\psi_t)$
    - $D(S_t)$ : drainage/percolation (mm)
    - $Q_t$ : ruissellement (optionnel, mm)
    """)
    
    st.markdown("### Dynamique physique (bilan hydrique)")
    st.markdown("""
    $$S_{t+1} = S_t + \\eta_I I_t + R_t + G_t - ETc_t - D(S_t) - Q_t$$
    
    $$\\psi_{t+1} = f_{\\text{retention}}(S_{t+1})$$
    
    Cette √©quation d√©crit l'√©volution temporelle de la r√©serve en eau du sol en fonction des flux entrants 
    (irrigation $I_t$, pluie $R_t$, remont√©e capillaire $G_t$) et des flux sortants 
    (√©vapotranspiration $ETc_t$, drainage $D(S_t)$, ruissellement $Q_t$).
    """)
    
    st.markdown("### Contraintes op√©rationnelles")
    st.markdown("""
    - $I_{\\max}$ : dose journali√®re max
    - Quotas d'eau saisonniers
    - Fen√™tres d'irrigation (heures/jours)
    - Pas de temps (journalier ; infra-journalier possible avec CDE)
    """)
    
    st.markdown("### Unit√©s (rappel)")
    st.markdown("""
    - $\\psi$ : cbar
    - $S, I, R, G, D, Q$ : mm
    - $ET0$ : mm/j, $Kc$ : adimensionnel
    """)


def render_doc_apprentissage_renforcement():
    """
    Affiche le contenu de l'onglet de documentation : Apprentissage par Renforcement.
    """
    st.markdown('<h2 class="section-header">ü§ñ Rappel : qu\'est-ce que l\'Apprentissage par Renforcement (RL) ?</h2>', unsafe_allow_html=True)
    
    st.markdown("""
    Le RL mod√©lise un probl√®me de d√©cision s√©quentielle sous forme de MDP (Markov Decision Process).
    
    - **√âtat**: $s_t \\in \\mathcal{S}$ (ici, observables li√©es √† la tension $\\psi_t$ et √† la m√©t√©o)
    - **Action**: $a_t \\in \\mathcal{A}$ (ici, dose d'irrigation $I_t$)
    - **Transition**: $p(s_{t+1}\\mid s_t, a_t)$ (dynamique du sol + m√©t√©o)
    - **R√©compense**: $r_t = r(s_t, a_t)$ (ex. stress hydrique, eau utilis√©e, drainage)
    - **Politique**: $\\pi_\\theta(a\\mid s)$, param√©tr√©e (r√©seau de neurones)
    - **Objectif**: maximiser le retour $J(\\theta)=\\mathbb{E}_\\pi\\!\\left[\\sum_{t=0}^{T}\\gamma^t r_t\\right]$
    """)
    
    st.markdown("### Ce que voit l'agent (exemple d'observation)")
    st.markdown("""
    L'observation $o_t$ que re√ßoit l'agent RL combine les variables d'√©tat physiques d√©crites pr√©c√©demment 
    avec des informations contextuelles :
    
    $$o_t = (\\psi_t,\\ t/T,\\ R_{t-k:t},\\ ET0_t,\\ \\hat{R}_{t+1:t+h},\\ \\widehat{ET0}_{t+1:t+h},\\ \\text{√©vent. } \\psi_{t-k:t-1})$$
    
    O√π :
    - **$\\psi_t$** : tension matricielle actuelle (variable mesur√©e, voir onglet "Variables d'√©tat")
    - **$t/T$** : progression temporelle dans la saison (normalis√©e entre 0 et 1)
    - **$R_{t-k:t}$** : historique de pluie sur les $k$ jours pr√©c√©dents
    - **$ET0_t$** : √©vapotranspiration de r√©f√©rence actuelle
    - **$\\hat{R}_{t+1:t+h}$** : pr√©visions de pluie pour les $h$ prochains jours
    - **$\\widehat{ET0}_{t+1:t+h}$** : pr√©visions d'ET0 pour les $h$ prochains jours
    - **$\\psi_{t-k:t-1}$** (optionnel) : historique de tension pour capturer les tendances
    
    **Normalisation** : Les observations sont g√©n√©ralement standardis√©es ou clipp√©es pour stabiliser l'apprentissage.
    """)
    
    st.markdown("### Espace d'actions")
    st.markdown("""
    L'action $a_t$ de l'agent correspond √† la dose d'irrigation $I_t$ √† appliquer :
    
    - **Continu** (Box): $I_t \\in [0, I_{\\max}]$ (mm) ‚Äî cas le plus r√©aliste
      - Permet des doses pr√©cises et adaptatives
      - $I_{\\max}$ est une contrainte op√©rationnelle (d√©bit maximal du syst√®me)
    
    - **Discret**: choix parmi des doses pr√©-d√©finies (ex. $I_t \\in \\{0, 5, 10, 15, 20\\}$ mm)
      - Plus simple √† impl√©menter mais moins flexible
    
    Le choix entre continu et discret est influenc√© par les contraintes op√©rationnelles et l'impl√©mentation RL.
    """)
    
    st.markdown("### Conception de la r√©compense (exemples)")
    st.markdown("""
    - **Stress hydrique**: p√©naliser les $\\psi_t$ hors zone de confort
      - $r^{stress}_t = -\\alpha\\ \\text{stress}(\\psi_t)$
    - **Eau utilis√©e**: p√©naliser la quantit√© d'irrigation
      - $r^{eau}_t = -\\beta\\, I_t$
    - **Drainage/pertes**: p√©naliser $D(S_t)$
      - $r^{drain}_t = -\\gamma\\, D(S_t)$
    - **Terminaison**: bonus de rendement ‚Äì p√©nalit√© eau
      - $R_{final} = Y - \\lambda \\sum_t I_t$, avec $Y = Y_{\\max} \\exp(-k_{CS}\\sum_t \\text{stress}(\\psi_t))$
    
    R√©compense totale typique: $r_t = r^{stress}_t + r^{eau}_t + r^{drain}_t$, puis ajout de $R_{final}$ en fin d'√©pisode.
    """)
    
    st.markdown("### PPO en bref (Proximal Policy Optimization)")
    st.markdown("""
    - **Type**: on-policy, gradient de politique
    - **Id√©e cl√©**: mise √† jour "proche" de la politique courante via un objectif avec **clipping**
    - **Avantage**: variance r√©duite via l'estimation d'**avantage** $\\hat{A}_t$
    - **Compatibilit√©**: actions continues (Box) et discr√®tes
    - **Stabilit√©**: bonnes propri√©t√©s empiriques, tuning mod√©r√©
    
    Objectif PPO (sch√©matique):
    - Maximiser $\\mathbb{E}_t\\left[\\min\\left(r_t(\\theta)\\hat{A}_t,\\ \\text{clip}(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_t\\right)\\right]$
    - Avec $r_t(\\theta)=\\frac{\\pi_\\theta(a_t\\mid s_t)}{\\pi_{\\theta_{old}}(a_t\\mid s_t)}$, et une perte valeur + entropie
    """)
    
    st.markdown("### Boucle d'apprentissage (sch√©ma)")
    st.markdown("""
    1. **Collecte**: rouler la politique $\\pi_\\theta$ dans l'environnement, stocker $(s_t,a_t,r_t,s_{t+1})$
    2. **Calcul**: retours, avantages (ex. GAE-$\\lambda$)
    3. **Update**: optimiser politique et critique (r√©seau valeur) via PPO
    4. **√âvaluer/valider**: sur seeds/√©pisodes non vus
    5. **R√©p√©ter** jusqu'√† convergence ou budget
    """)
    
    st.markdown("### Bonnes pratiques pour l'irrigation")
    st.markdown("""
    - **Exploration vs exploitation**: contr√¥ler la stochasticit√© (entropie), garder des doses plausibles
    - **Contraintes**: int√©grer $I_{\\max}$, quotas, fen√™tres temporelles (via clipping, masques, p√©nalit√©s)
    - **Robustesse**: randomiser m√©t√©o/sols (domain randomization), g√©rer donn√©es manquantes
    - **Observations**: inclure pr√©visions m√©t√©o, historiques, et indicateurs de confiance
    - **√âchelle temporelle**: choisir $\\Delta t$ (journalier vs infra-journalier), coh√©rent avec le mod√®le (CDE si irr√©gulier)
    - **√âvaluation**: multi-saisons, sc√©narios secs/humides, m√©triques eau, stress, rendement
    """)
    
    st.markdown("### Exemple d'√©tat et de politique (illustratifs)")
    st.markdown("""
    - √âtat: $s_t = (\\psi_t,\\ t/T,\\ R_{t-2:t},\\ ET0_t,\\ \\hat{R}_{t+1:t+2})$
    - Heuristiques utiles que PPO peut apprendre:
      - **Pluie pr√©vue** ‚Üí r√©duire $I_t$
      - **$\\psi_t$ √©lev√© (sol sec)** ‚Üí irriguer
      - **$\\psi_t$ mod√©r√© mais ET0 fort** ‚Üí irrigation pr√©ventive
    
    En r√©sum√©, le RL (et en particulier PPO) offre un cadre pour apprendre automatiquement une strat√©gie d'irrigation 
    tenant compte de la dynamique du sol, des pr√©visions et des co√ªts, tout en g√©rant des espaces d'actions continus 
    et des observations riches.
    """)

# ========================================================================
# ONGLET DOCUMENTATION 4 : SC√âNARIO 2 (RL SUR MOD√àLE PHYSIQUE)
# ========================================================================

def render_doc_scenario2():
    """
    Affiche le contenu de l'onglet de documentation : Sc√©nario 2 (RL sur mod√®le physique).
    """
    st.markdown('<h2 class="section-header">üéì RL sur mod√®le physique (avec tension observ√©e)</h2>', unsafe_allow_html=True)
    
    st.markdown("""
    ### ‚ùì Qu'est-ce que RL sur mod√®le physique ?
    
    Le **RL sur mod√®le physique** impl√©mente un agent d'apprentissage par renforcement (RL) qui apprend une politique d'irrigation optimale
    en interagissant directement avec un environnement simul√© par le mod√®le physique FAO. Contrairement au Sc√©nario 1 qui utilise
    des r√®gles fixes, ce Sc√©nario apprend automatiquement √† optimiser l'irrigation en minimisant le stress hydrique tout en
    √©conomisant l'eau.
    
    **Principe fondamental** : Un agent RL observe la tension matricielle $\\psi_t$ (et le contexte m√©t√©orologique) et choisit
    une dose d'irrigation $I_t$ dans un environnement simul√© par le mod√®le physique. L'agent apprend √† partir de ses interactions
    avec l'environnement pour am√©liorer progressivement sa strat√©gie.
    """)
    
    with st.expander("üî¨ Architecture MDP (Markov Decision Process)", expanded=False):
        st.markdown("""
        Le Sc√©nario 2 mod√©lise le probl√®me d'irrigation comme un MDP :
        
        **1. Espace d'observation** ($\\mathcal{S}$) :
        - Observation standard : $o_t = [\\psi_t, S_t, R_t, ET0_t] \\in \\mathbb{R}^4$
          - $\\psi_t$ : Tension matricielle actuelle (cbar) - variable cl√© mesur√©e par tensiom√®tre
          - $S_t$ : R√©serve en eau du sol (mm)
          - $R_t$ : Pluie du jour (mm)
          - $ET0_t$ : √âvapotranspiration de r√©f√©rence (mm/j)
        - Observation enrichie (optionnelle) : $o_t = (\\psi_t, t/T, R_{t-k:t}, ET0_t, \\hat{R}_{t+1:t+h}, \\widehat{ET0}_{t+1:t+h})$
          - $t/T$ : Progression temporelle dans la saison (normalis√©e)
          - $R_{t-k:t}$ : Historique de pluie sur $k$ jours pr√©c√©dents
          - $\\hat{R}_{t+1:t+h}$ : Pr√©visions de pluie pour les $h$ prochains jours
          - $\\widehat{ET0}_{t+1:t+h}$ : Pr√©visions d'ET0 pour les $h$ prochains jours
        
        **2. Espace d'actions** ($\\mathcal{A}$) :
        - Action continue : $a_t = I_t \\in [0, I_{\\max}]$ (mm)
          - $I_{\\max}$ : Irrigation maximale par jour (contrainte op√©rationnelle)
          - Permet des doses pr√©cises et adaptatives
        
        **3. Fonction de transition** ($p(s_{t+1} | s_t, a_t)$) :
        - Mod√®le physique FAO : $S_{t+1} = f_{\\text{FAO}}(S_t, I_t, R_t, ET0_t, Kc_t)$
        - Conversion : $\\psi_{t+1} = f_{\\text{retention}}(S_{t+1})$
        - Le mod√®le physique garantit la coh√©rence des pr√©dictions
        
        **4. Fonction de r√©compense** ($r_t = r(s_t, a_t)$) :
        - R√©compense journali√®re : $r_t = -\\alpha \\cdot \\text{stress}(\\psi_t) - \\beta \\cdot I_t - \\gamma \\cdot D(S_t)$
          - $\\alpha$ : Poids de la p√©nalit√© de stress hydrique
          - $\\beta$ : Poids de la p√©nalit√© d'irrigation (co√ªt de l'eau)
          - $\\gamma$ : Poids de la p√©nalit√© de drainage (pertes d'eau)
        - R√©compense terminale : $R_{\\text{final}} = Y(\\text{cum\_stress}) - \\lambda_{\\text{water}} \\cdot \\sum_t I_t$
          - $Y$ : Rendement de la culture (d√©cro√Æt avec le stress cumul√©)
          - $\\lambda_{\\text{water}}$ : Poids de la p√©nalit√© d'eau totale
        
        **5. Algorithme RL** :
        - **PPO (Proximal Policy Optimization)** : Algorithme on-policy, gradient de politique
        - Objectif : Maximiser $J(\\theta) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{T} \\gamma^t r_t\\right]$
        - $\\gamma$ : Discount factor (0.99 recommand√© pour planification long terme)
        """)
    
    with st.expander("üéØ Application dans notre projet : Pipeline d'entra√Ænement", expanded=False):
        st.markdown("""
        ### Processus d'entra√Ænement du Sc√©nario 2
        
        **1. G√©n√©ration de l'environnement** :
        - Cr√©ation de l'environnement Gymnasium avec le mod√®le physique FAO
        - Configuration des param√®tres (sol, m√©t√©o, r√©compenses)
        - G√©n√©ration de s√©ries m√©t√©orologiques avec seed pour reproductibilit√©
        
        **2. Initialisation de l'agent PPO** :
        - R√©seau de politique (policy network) : MLP qui mappe observation ‚Üí distribution d'actions
        - R√©seau de valeur (value network) : MLP qui estime $V(s_t)$ pour r√©duire la variance
        - Hyperparam√®tres PPO (learning rate, gamma, GAE lambda, etc.)
        
        **3. Boucle d'entra√Ænement** :
        - **Collecte de donn√©es** :
          - Rollout de la politique actuelle $\\pi_\\theta$ dans l'environnement
          - Stockage des trajectoires : $(s_t, a_t, r_t, s_{t+1})$
          - Calcul des retours et avantages (GAE-$\\lambda$)
        - **Mise √† jour de la politique** :
          - Optimisation de l'objectif PPO avec clipping
          - Mise √† jour du r√©seau de valeur
          - Contr√¥le de l'exploration via coefficient d'entropie
        - **√âvaluation** :
          - Test sur √©pisodes de validation (seeds diff√©rents)
          - Calcul de m√©triques (r√©compense moyenne, longueur d'√©pisode, etc.)
        
        **4. Utilisation du mod√®le entra√Æn√©** :
        - Chargement du mod√®le PPO sauvegard√©
        - √âvaluation sur nouvelles saisons
        - D√©ploiement pour prise de d√©cision en temps r√©el
        """)
    
    with st.expander("üìê Architecture de l'agent PPO", expanded=False):
        st.markdown("""
        ### R√©seau de politique (Policy Network)
        
        **Architecture** :
        - **Type** : MLP (Multi-Layer Perceptron)
        - **Entr√©e** : Observation $o_t \\in \\mathbb{R}^d$ o√π $d$ est la dimension de l'observation (4 par d√©faut)
        - **Couches cach√©es** : 2-3 couches avec 64-256 neurones chacune
        - **Activation** : Tanh ou ReLU
        - **Sortie** : Param√®tres d'une distribution d'actions
          - Pour actions continues : Moyenne $\\mu(o_t)$ et √©cart-type $\\sigma(o_t)$ d'une distribution normale
          - Action √©chantillonn√©e : $a_t \\sim \\mathcal{N}(\\mu(o_t), \\sigma(o_t))$
        
        **Formule** :
        $$
        \\begin{aligned}
        \\mathbf{h}_1 &= \\text{ReLU}(\\text{Linear}_1(o_t)) \\\\
        \\mathbf{h}_2 &= \\text{ReLU}(\\text{Linear}_2(\\mathbf{h}_1)) \\\\
        \\mu_t &= \\text{Linear}_\\mu(\\mathbf{h}_2) \\\\
        \\sigma_t &= \\text{softplus}(\\text{Linear}_\\sigma(\\mathbf{h}_2)) \\\\
        a_t &\\sim \\mathcal{N}(\\mu_t, \\sigma_t)
        \\end{aligned}
        $$
        
        ### R√©seau de valeur (Value Network)
        
        **Architecture** :
        - **Type** : MLP similaire au r√©seau de politique
        - **Entr√©e** : Observation $o_t$
        - **Sortie** : Estimation de la valeur $V(o_t) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{T-t} \\gamma^k r_{t+k} | o_t\\right]$
        
        **R√¥le** :
        - R√©duit la variance de l'estimation du gradient
        - Utilis√© dans le calcul de l'avantage : $\\hat{A}_t = \\delta_t + (\\gamma \\lambda) \\delta_{t+1} + \\ldots$
        - O√π $\\delta_t = r_t + \\gamma V(o_{t+1}) - V(o_t)$ est le TD-error
        
        ### Objectif PPO
        
        L'objectif PPO combine plusieurs termes :
        
        $$
        L^{\\text{PPO}}(\\theta) = \\mathbb{E}_t\\left[L^{\\text{CLIP}}(\\theta) - c_v L^V(\\theta) + c_e H[\\pi_\\theta](o_t)\\right]
        $$
        
        o√π :
        - $L^{\\text{CLIP}}(\\theta) = \\min\\left(r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t\\right)$
        - $r_t(\\theta) = \\frac{\\pi_\\theta(a_t | o_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | o_t)}$ est le ratio de probabilit√©
        - $L^V(\\theta) = (V_\\theta(o_t) - \\hat{V}_t)^2$ est la perte de valeur
        - $H[\\pi_\\theta](o_t)$ est l'entropie de la politique (encourage l'exploration)
        - $c_v$ et $c_e$ sont des coefficients de pond√©ration
        """)
    
    with st.expander("‚öñÔ∏è Avantages et limitations", expanded=False):
        st.markdown("""
        ### ‚úÖ Avantages du Sc√©nario 2
        
        **1. Apprentissage d'une politique optimale** :
        - ‚úÖ **Optimisation automatique** : L'agent RL apprend √† minimiser le stress hydrique tout en √©conomisant l'eau
        - ‚úÖ **Compromis optimal** : Trouve automatiquement le meilleur √©quilibre entre performance agronomique et co√ªt de l'eau
        - ‚úÖ **Strat√©gie adaptative** : S'ajuste selon les conditions m√©t√©orologiques et l'√©tat du sol
        
        **2. Adaptabilit√© aux conditions** :
        - ‚úÖ **Pr√©visions m√©t√©o** : Utilise les pr√©visions de pluie et d'ET0 pour anticiper et ajuster l'irrigation
        - ‚úÖ **Conditions variables** : S'adapte aux variations saisonni√®res et aux √©v√©nements m√©t√©orologiques
        - ‚úÖ **Historique contextuel** : Prend en compte l'historique r√©cent (pluie, tension) pour des d√©cisions inform√©es
        
        **3. Respect de la physique** :
        - ‚úÖ **Mod√®le physique fiable** : Utilise un mod√®le bucket valid√© pour simuler la dynamique du sol
        - ‚úÖ **Courbe de r√©tention** : Respecte la relation $S \\leftrightarrow \\psi$ bas√©e sur les propri√©t√©s p√©dophysiques
        - ‚úÖ **Bilan hydrique coh√©rent** : Les √©quations physiques garantissent la coh√©rence des pr√©dictions
        
        **4. Flexibilit√© des actions** :
        - ‚úÖ **Actions continues** : Permet des doses d'irrigation pr√©cises et gradu√©es (pas seulement 0 ou dose fixe)
        - ‚úÖ **Doses adaptatives** : Ajuste la quantit√© d'eau selon l'intensit√© du stress et les conditions
        - ‚úÖ **Strat√©gie pr√©ventive** : Peut irriguer pr√©ventivement avant que le stress ne devienne critique
        
        **5. Performance sup√©rieure** :
        - ‚úÖ **Efficacit√© de l'eau** : G√©n√©ralement meilleure que les r√®gles fixes en termes de consommation d'eau
        - ‚úÖ **R√©duction du stress** : Maintient mieux la tension dans la zone de confort
        - ‚úÖ **Minimisation du drainage** : Apprend √† √©viter les pertes d'eau par drainage excessif
        
        **6. R√©utilisabilit√©** :
        - ‚úÖ **Mod√®le entra√Æn√©** : Une fois entra√Æn√©, le mod√®le peut √™tre utilis√© sur diff√©rentes saisons
        - ‚úÖ **Transfert possible** : Peut √™tre adapt√© √† d'autres parcelles avec r√©-entra√Ænement
        - ‚úÖ **Am√©lioration continue** : Peut √™tre r√©-entra√Æn√© avec de nouvelles donn√©es pour s'am√©liorer
        
        ### ‚ö†Ô∏è Limitations du Sc√©nario 2
        
        **1. D√©pendance √† la qualit√© du mod√®le physique** :
        - ‚ö†Ô∏è **Biais du mod√®le** : Si le mod√®le bucket a des biais (param√®tres mal calibr√©s, processus n√©glig√©s), 
          la politique apprise sera biais√©e
        - ‚ö†Ô∏è **Erreurs de param√©trisation** : Des erreurs dans les param√®tres du sol ($S_{fc}$, $\\psi_{fc}$, $k_d$) 
          se propagent dans les d√©cisions
        - ‚ö†Ô∏è **Processus non mod√©lis√©s** : Ph√©nom√®nes non captur√©s par le mod√®le (h√©t√©rog√©n√©it√© spatiale, 
          interactions complexes) ne sont pas pris en compte
        
        **2. Phase d'entra√Ænement n√©cessaire** :
        - ‚ö†Ô∏è **Temps d'entra√Ænement** : N√©cessite une phase d'apprentissage (plusieurs milliers de timesteps) 
          avant d'√™tre utilisable
        - ‚ö†Ô∏è **Ressources computationnelles** : Entra√Ænement PPO n√©cessite des ressources CPU/GPU
        - ‚ö†Ô∏è **Expertise technique** : N√©cessite des comp√©tences en RL pour l'entra√Ænement et le r√©glage
        
        **3. Donn√©es d'entra√Ænement** :
        - ‚ö†Ô∏è **Simulation requise** : Besoin de g√©n√©rer des donn√©es de simulation pour l'entra√Ænement
        - ‚ö†Ô∏è **Qualit√© de la simulation** : La qualit√© de l'entra√Ænement d√©pend de la qualit√© de la simulation m√©t√©o
        - ‚ö†Ô∏è **Robustesse** : N√©cessite d'entra√Æner sur plusieurs saisons/sc√©narios pour √™tre robuste
        
        **4. Complexit√© de d√©ploiement** :
        - ‚ö†Ô∏è **Infrastructure** : N√©cessite une infrastructure pour ex√©cuter le mod√®le entra√Æn√©
        - ‚ö†Ô∏è **Maintenance** : Le mod√®le peut n√©cessiter un r√©-entra√Ænement p√©riodique
        - ‚ö†Ô∏è **Interpr√©tabilit√© r√©duite** : Moins interpr√©table que les r√®gles simples (bo√Æte noire)
        
        **5. Hyperparam√®tres √† r√©gler** :
        - ‚ö†Ô∏è **Tuning n√©cessaire** : Nombreux hyperparam√®tres √† ajuster (learning rate, gamma, GAE-$\\lambda$, etc.)
        - ‚ö†Ô∏è **Sensibilit√©** : La performance peut √™tre sensible aux choix d'hyperparam√®tres
        - ‚ö†Ô∏è **Expertise requise** : N√©cessite une compr√©hension du RL pour optimiser les hyperparam√®tres
        
        **6. Stabilit√© de l'apprentissage** :
        - ‚ö†Ô∏è **Convergence** : L'entra√Ænement peut ne pas converger ou converger vers un optimum local
        - ‚ö†Ô∏è **Variabilit√©** : La performance peut varier entre diff√©rentes ex√©cutions d'entra√Ænement
        - ‚ö†Ô∏è **Normalisation** : N√©cessite une normalisation soigneuse des observations et r√©compenses
        
        **7. Observations coh√©rentes** :
        - ‚ö†Ô∏è **Alignement temporel** : N√©cessite que les observations soient align√©es temporellement
        - ‚ö†Ô∏è **Donn√©es manquantes** : Doit g√©rer les cas de donn√©es manquantes ou irr√©guli√®res
        - ‚ö†Ô∏è **Pr√©visions m√©t√©o** : D√©pend de la qualit√© des pr√©visions m√©t√©orologiques disponibles
        """)
    
    with st.expander("üîß Param√®tres recommand√©s et tuning", expanded=False):
        st.markdown("""
        ### Hyperparam√®tres PPO
        
        **Learning rate** : $3 \\times 10^{-4}$ (recommand√©)
        - Trop √©lev√© (> $10^{-3}$) : Instabilit√©, oscillations
        - Trop faible (< $10^{-5}$) : Apprentissage trop lent
        - Tuning : R√©duire si loss oscille, augmenter si convergence lente
        
        **Gamma (discount factor)** : 0.99 (recommand√©)
        - Contr√¥le l'importance des r√©compenses futures
        - √âlev√© (0.99) : Planification √† long terme
        - Faible (0.95) : Focus sur court terme
        
        **GAE lambda** : 0.95 (recommand√©)
        - Contr√¥le le biais/variance de l'estimation de la valeur
        - √âlev√© (0.95-0.99) : Moins de variance, plus de biais
        - Faible (0.8-0.9) : Plus de variance, moins de biais
        
        **Entropy coefficient** : 0.01-0.05
        - Encourage l'exploration
        - √âlev√© : Plus d'exploration, convergence plus lente
        - Faible : Moins d'exploration, risque de sous-optimum local
        
        **Clip range** : 0.2 (standard PPO)
        - Limite les changements de politique
        - √âlev√© : Permet plus de changements, moins stable
        - Faible : Changements limit√©s, plus stable
        
        **Batch size** : 64-256
        - Plus grand : Gradients plus stables mais plus de m√©moire
        - Plus petit : Moins de m√©moire mais gradients plus variables
        
        **Number of steps per rollout** : 2048
        - Plus grand : Meilleure estimation mais plus de m√©moire
        - Plus petit : Moins de m√©moire mais estimation moins pr√©cise
        
        ### Hyperparam√®tres de l'environnement
        
        **Param√®tres de r√©compense** :
        - $\\alpha$ (p√©nalit√© stress) : 1.0 (recommand√©)
        - $\\beta$ (p√©nalit√© irrigation) : 0.05 (recommand√©)
        - $\\gamma$ (p√©nalit√© drainage) : 0.01 (recommand√©)
        - Tuning : Ajuster selon priorit√©s (eau vs stress)
        
        **Param√®tres du sol** :
        - Utiliser les valeurs par d√©faut sauf si donn√©es sp√©cifiques disponibles
        - Calibrer $S_{fc}$, $\\psi_{fc}$ selon mesures r√©elles si possible
        
        ### Strat√©gie de tuning
        
        **1. Commencer avec valeurs par d√©faut** :
        - Utiliser les valeurs recommand√©es ci-dessus
        - Entra√Æner sur 50,000-100,000 timesteps
        
        **2. Observer les m√©triques** :
        - R√©compense moyenne : Doit augmenter
        - Longueur d'√©pisode : Doit √™tre stable
        - Variance des actions : Ne doit pas exploser
        
        **3. Ajuster si n√©cessaire** :
        - Si instabilit√© : R√©duire learning rate, augmenter clip range
        - Si convergence lente : Augmenter learning rate, r√©duire entropy
        - Si sous-optimum : Augmenter entropy, r√©duire clip range
        """)
    
    with st.expander("üß≠ Quand utiliser le Sc√©nario 2 ?", expanded=False):
        st.markdown("""
        ### ‚úÖ Choisir le Sc√©nario 2 si :
        
        - **Optimisation recherch√©e** :
          - Besoin de minimiser la consommation d'eau
          - Recherche du compromis optimal stress/co√ªt
          - Performance sup√©rieure aux r√®gles simples
        
        - **Donn√©es disponibles** :
          - Possibilit√© de g√©n√©rer des simulations pour l'entra√Ænement
          - Mod√®le physique fiable et bien calibr√©
          - Conditions m√©t√©orologiques vari√©es pour robustesse
        
        - **Ressources computationnelles** :
          - Infrastructure disponible pour l'entra√Ænement PPO
          - Temps d'entra√Ænement acceptable (quelques heures)
          - Expertise en RL disponible
        
        - **Adaptabilit√© n√©cessaire** :
          - Conditions variables n√©cessitant adaptation
          - Besoin de strat√©gie pr√©ventive
          - Optimisation selon objectifs multiples
        
        - **Point de d√©part pour approches avanc√©es** :
          - Baseline pour comparer avec Sc√©narios 3-6
          - Validation de l'approche RL avant complexification
        
        ### ‚ùå Ne pas choisir le Sc√©nario 2 si :
        
        - **Simplicit√© prioritaire** :
          - Besoin de solution simple et rapide
          - Pas d'infrastructure d'entra√Ænement
          - R√®gles simples suffisent
        
        - **Mod√®le physique incertain** :
          - Param√®tres du sol mal connus
          - Mod√®le physique non valid√©
          - Donn√©es de simulation de mauvaise qualit√©
        
        - **Donn√©es limit√©es** :
          - Pas de possibilit√© de g√©n√©rer des simulations
          - Conditions trop sp√©cifiques pour g√©n√©raliser
        
        - **Besoin de correction physique** :
          - Mod√®le physique a des biais connus
          - N√©cessit√© de corriger les pr√©dictions physiques
          - ‚Üí Pr√©f√©rer Sc√©narios 3-4
        """)
    
    with st.expander("üõ†Ô∏è Conseils pratiques", expanded=False):
        st.markdown("""
        ### Workflow recommand√©
        
        **1. Pr√©paration** :
        - V√©rifier la coh√©rence m√©t√©o (m√™mes seeds/params que Sc√©nario 1)
        - Valider le mod√®le physique sur quelques √©pisodes
        - Configurer les hyperparam√®tres avec valeurs par d√©faut
        
        **2. Entra√Ænement initial** :
        - Commencer avec 50,000 timesteps
        - Observer les m√©triques (r√©compense, longueur d'√©pisode)
        - V√©rifier la convergence
        
        **3. Tuning it√©ratif** :
        - Ajuster les hyperparam√®tres si n√©cessaire
        - R√©-entra√Æner avec nouveaux param√®tres
        - Comparer les performances
        
        **4. √âvaluation** :
        - Tester sur nouvelles saisons (seeds diff√©rents)
        - Comparer avec Sc√©nario 1 (baseline)
        - Analyser les d√©cisions prises
        
        ### Troubleshooting
        
        **Probl√®me : Instabilit√© de l'entra√Ænement**
        - **Sympt√¥me** : Loss oscille, r√©compense ne converge pas
        - **Solutions** :
          - R√©duire learning rate (ex: $3 \\times 10^{-4} \\to 10^{-4}$)
          - Augmenter clip range (ex: 0.2 ‚Üí 0.3)
          - Normaliser les observations et r√©compenses
        
        **Probl√®me : Convergence lente**
        - **Sympt√¥me** : R√©compense augmente tr√®s lentement
        - **Solutions** :
          - Augmenter learning rate (avec prudence)
          - Augmenter entropy coefficient pour plus d'exploration
          - V√©rifier la normalisation des r√©compenses
        
        **Probl√®me : Sous-optimum local**
        - **Sympt√¥me** : Performance plafonne √† un niveau sous-optimal
        - **Solutions** :
          - Augmenter entropy coefficient
          - R√©duire clip range pour permettre plus de changements
          - Augmenter le nombre de timesteps d'entra√Ænement
        
        **Probl√®me : Politique trop conservatrice**
        - **Sympt√¥me** : Irrigation insuffisante, stress hydrique
        - **Solutions** :
          - Ajuster les poids de r√©compense ($\\alpha$ vs $\\beta$)
          - Augmenter la p√©nalit√© de stress ($\\alpha$)
          - R√©duire la p√©nalit√© d'irrigation ($\\beta$)
        
        ### M√©triques √† surveiller
        
        - **R√©compense moyenne** : Doit augmenter avec l'entra√Ænement
        - **Longueur d'√©pisode** : Doit √™tre stable (‚âà longueur de saison)
        - **Variance des actions** : Ne doit pas exploser (signe d'instabilit√©)
        - **Policy loss** : Doit d√©cro√Ætre et converger
        - **Value loss** : Doit d√©cro√Ætre (estimation de la valeur)
        """)
    
    with st.expander("üîó Comparaison avec les autres sc√©narios", expanded=False):
        st.markdown("""
        ### Sc√©nario 2 vs Sc√©nario 1 (R√®gles simples)
        
        **Sc√©nario 1** :
        - R√®gles fixes, pas d'apprentissage
        - Simple et rapide
        - Performance sous-optimale
        
        **Sc√©nario 2** :
        - Apprentissage automatique
        - Plus complexe mais meilleure performance
        - N√©cessite entra√Ænement
        
        **Quand choisir Sc√©nario 2** : Optimisation et adaptabilit√© recherch√©es
        
        ### Sc√©nario 2 vs Sc√©narios 3-4 (Neural ODE/CDE)
        
        **Sc√©nario 2** :
        - RL direct sur mod√®le physique
        - Pas de correction du mod√®le physique
        - Plus simple
        
        **Sc√©narios 3-4** :
        - Correction r√©siduelle du mod√®le physique
        - Am√©liore la pr√©diction physique
        - Plus complexe
        
        **Quand choisir Sc√©narios 3-4** : Mod√®le physique a des biais connus
        
        ### Sc√©nario 2 vs Sc√©nario 5 (PatchTST)
        
        **Sc√©nario 2** :
        - Observation standard (4 dimensions)
        - Pas de m√©moire temporelle explicite
        
        **Sc√©nario 5** :
        - Observation enrichie avec features temporelles
        - M√©moire longue via PatchTST
        
        **Quand choisir Sc√©nario 5** : Besoin de comprendre tendances et saisonnalit√©
        
        ### Sc√©nario 2 vs Sc√©nario 6 (World Model)
        
        **Sc√©nario 2** :
        - Model-free RL
        - Pas de planification explicite
        
        **Sc√©nario 6** :
        - Model-based RL avec planification
        - Rollouts d'imagination
        
        **Quand choisir Sc√©nario 6** : Besoin de planification et sample efficiency
        """)
    
    with st.expander("üìä Variables et notations", expanded=False):
        st.markdown("""
        ### Variables principales
        
        **Observations et √©tats** :
        - $o_t = [\\psi_t, S_t, R_t, ET0_t] \\in \\mathbb{R}^4$ : Observation au temps $t$
        - $s_t$ : √âtat du MDP (peut √™tre identique √† $o_t$ ou enrichi)
        - $a_t = I_t \\in [0, I_{\\max}]$ : Action (irrigation) au temps $t$
        - $r_t$ : R√©compense au temps $t$
        
        **Mod√®le physique** :
        - $S_t$ : R√©serve en eau du sol (mm)
        - $\\psi_t$ : Tension matricielle (cbar)
        - $R_t$ : Pluie (mm)
        - $ET0_t$ : √âvapotranspiration de r√©f√©rence (mm/j)
        - $Kc_t$ : Coefficient cultural
        - $ETc_t = Kc_t \\times ET0_t \\times f_{ET}(\\psi_t)$ : √âvapotranspiration culturelle
        - $D(S_t)$ : Drainage (mm)
        
        **Fonctions et mod√®les** :
        - $\\pi_\\theta(a_t | o_t)$ : Politique PPO (distribution d'actions)
        - $V_\\theta(o_t)$ : Fonction de valeur (estimation du retour futur)
        - $g(\\psi_t; \\theta_{\\text{r√®gle}})$ : R√®gle d'irrigation d√©terministe du Sc√©nario 1 (seuil, bande de confort, proportionnelle) qui renvoie $I_t$
        - $f_{\\text{FAO}}(\\cdot)$ : Mod√®le physique FAO
        - $f_{\\text{retention}}(\\cdot)$ : Courbe de r√©tention $S \\leftrightarrow \\psi$
        
        **Entra√Ænement** :
        - $\\theta$ : Param√®tres de la politique et de la fonction de valeur
        - $\\gamma$ : Discount factor (0.99)
        - $\\lambda$ : Param√®tre GAE (0.95)
        - $\\epsilon$ : Clip range (0.2)
        - $\\hat{A}_t$ : Estimation de l'avantage (GAE)
        """)

# ========================================================================
# ONGLET DOCUMENTATION 5 : NEURAL ODE
# ========================================================================

def render_doc_neural_ode():
    """
    Affiche le contenu de l'onglet de documentation : Neural ODE.
    """
    st.markdown('<h2 class="section-header">üß† Neural ODE : Mod√®le hybride physique-neuronal</h2>', unsafe_allow_html=True)
    
    st.markdown("""
    ### ‚ùì Qu'est-ce qu'un Neural ODE ?
    
    Un **Neural ODE** (Neural Ordinary Differential Equation) est un mod√®le qui combine des √©quations diff√©rentielles ordinaires (ODE) avec des r√©seaux de neurones. 
    Dans notre contexte, il s'agit d'un **mod√®le r√©siduel** qui apprend √† corriger les pr√©dictions d'un mod√®le physique.
    """)
    
    with st.expander("üî¨ Principe g√©n√©ral des Neural ODE", expanded=False):
        st.markdown("""
        Les Neural ODE mod√©lisent la dynamique d'un syst√®me par une √©quation diff√©rentielle o√π la d√©riv√©e est apprise par un r√©seau de neurones :
        
        $$
        \\frac{d\\mathbf{z}(t)}{dt} = f_\\theta(\\mathbf{z}(t), t)
        $$
        
        o√π :
        - $\\mathbf{z}(t)$ est l'√©tat du syst√®me au temps $t$
        - $f_\\theta$ est un r√©seau de neurones param√©tr√© par $\\theta$
        - La solution est obtenue par int√©gration num√©rique (m√©thode d'Euler, Runge-Kutta, etc.)
        
        **Avantages** :
        - **Continuit√©** : Mod√©lise des processus continus naturellement
        - **Efficacit√© m√©moire** : Pas besoin de stocker tous les √©tats interm√©diaires
        - **Flexibilit√©** : Le r√©seau apprend la dynamique √† partir des donn√©es
        """)
    
    with st.expander("üéØ Application dans notre projet : Mod√®le hybride", expanded=False):
        st.markdown("""
        Dans notre projet d'irrigation intelligente, le Neural ODE est utilis√© comme **correction r√©siduelle** sur le mod√®le physique FAO.
        L'id√©e est de combiner :
        
        - **Mod√®le physique** : Fournit une pr√©diction de base bas√©e sur les lois de la physique (bilan hydrique)
        - **Neural ODE** : Apprend les √©carts syst√©matiques et les ph√©nom√®nes non mod√©lis√©s par le mod√®le physique
        
        Cette approche hybride permet de :
        - ‚úÖ B√©n√©ficier de la robustesse et de l'interpr√©tabilit√© du mod√®le physique
        - ‚úÖ Capturer les biais syst√©matiques et les ph√©nom√®nes complexes non mod√©lis√©s
        - ‚úÖ S'adapter aux sp√©cificit√©s locales (type de sol, conditions m√©t√©o, etc.)
        
        ### üìã Param√®tres de configuration du pr√©-entra√Ænement
        
        **Nombre de trajectoires** :
        - **Signification** : Nombre de simulations ind√©pendantes utilis√©es pour g√©n√©rer les donn√©es d'entra√Ænement du Neural ODE
        - **Valeur usuelle** : 32 trajectoires (par d√©faut)
        - **Impact** : Plus de trajectoires = plus de diversit√© dans les donn√©es (conditions m√©t√©o vari√©es, strat√©gies d'irrigation diff√©rentes)
        - **Tuning** : Augmenter (50-100) si le mod√®le ne g√©n√©ralise pas bien, r√©duire (10-20) pour acc√©l√©rer l'entra√Ænement
        - **Note** : Chaque trajectoire simule une saison compl√®te (120 jours par d√©faut)
        
        **Nombre d'epochs** :
        - **Signification** : Nombre de passages complets sur l'ensemble des donn√©es d'entra√Ænement
        - **Valeur usuelle** : 10 epochs (par d√©faut)
        - **Impact** : Plus d'epochs = meilleur apprentissage mais risque de surapprentissage
        - **Tuning** : Augmenter (20-50) si la loss continue √† diminuer, r√©duire si la loss stagne ou augmente
        - **Note** : Surveiller la loss de validation pour d√©tecter le surapprentissage
        
        **Taille des batches** :
        - **Signification** : Nombre d'√©chantillons trait√©s simultan√©ment lors de chaque mise √† jour des param√®tres
        - **Valeur usuelle** : 256 (par d√©faut)
        - **Impact** : Batch plus grand = gradients plus stables mais plus de m√©moire requise
        - **Tuning** : R√©duire (32-128) si m√©moire limit√©e, augmenter (512+) si disponible et pour plus de stabilit√©
        - **Note** : Doit √™tre adapt√© √† la taille du dataset (nombre de trajectoires √ó longueur de saison)
        
        **Taux d'apprentissage (Learning Rate)** :
        - **Signification** : Vitesse √† laquelle le mod√®le ajuste ses param√®tres lors de l'optimisation
        - **Valeur usuelle** : $10^{-3}$ (0.001) (par d√©faut)
        - **Impact** : LR trop √©lev√© = instabilit√©, LR trop faible = apprentissage lent
        - **Tuning** : R√©duire (10^{-4} - 10^{-5}) si la loss oscille, augmenter (10^{-2}) si l'apprentissage est trop lent
        - **Note** : Utilise l'optimiseur Adam qui adapte le LR par param√®tre
        """)


def render_doc_neural_ode_cont():
    """
    Affiche le contenu de l'onglet de documentation : Neural ODE continu (Sc√©nario 3b).
    """
    st.markdown('<h2 class="section-header">üß† Neural ODE continu : correction r√©siduelle liss√©e</h2>', unsafe_allow_html=True)

    st.markdown("""
    ### ‚ùì Qu'est-ce qu'un Neural ODE continu ?

    Variante continue du Neural ODE : le r√©seau apprend directement la d√©riv√©e $d\\psi/dt$
    et l'int√®gre sur un pas (1 jour) avec `torchdiffeq` (Runge-Kutta) ou un Euler explicite fallback.
    R√©sultat : corrections $\\Delta \\psi$ plus lisses que la version discr√®te.
    """)

    with st.expander("üî¨ Principe g√©n√©ral", expanded=False):
        st.markdown("""
        $$
        \\frac{d\\psi}{dt} = f_\\theta(\\psi, I, R, ET0) \\quad\\Rightarrow\\quad
        \\Delta\\psi = \\int_{t}^{t+\\Delta t} f_\\theta(\\psi, I, R, ET0)\\, dt
        $$

        - **Int√©gration** : `torchdiffeq.odeint` (rk4) si dispo, sinon Euler multi-sous-pas.
        - **Avantage** : correction temporellement plus r√©guli√®re (moins d'oscillations sur œà et I).
        """)

    with st.expander("üéØ Application hybride (Sc√©nario 3b)", expanded=False):
        st.markdown("""
        - **Base physique** : m√™me bucket FAO que le sc√©nario 3.
        - **Correction continue** : $\\psi_{t+1} = \\psi_{t+1}^{phys} + \\Delta\\psi_{cont}$.
        - **Impact attendu** : transitions plus douces, politique PPO moins sujette aux √†-coups.
        """)

    with st.expander("üß∞ Hyperparam√®tres cl√©s", expanded=False):
        st.markdown("""
        - **Trajectoires (N_traj)** : 32 par d√©faut. Augmenter (50-100) si la loss continue de baisser.
        - **Epochs** : 10 par d√©faut. Monter √† 20-50 si sous-apprentissage.
        - **Batch size** : 256 par d√©faut. R√©duire (64-128) si m√©moire limit√©e.
        - **Learning rate** : $10^{-3}$ par d√©faut. Baisser (1e-4) si la loss oscille.
        - **Solver** : rk4 via `torchdiffeq` si install√© ; sinon Euler avec sous-pas (param√®tre `substeps`).
        """)

    with st.expander("üìê Architecture", expanded=False):
        st.markdown("""
        - **R√©seau f_Œ∏ (dœà/dt)** : MLP 4‚Üí64‚Üí64‚Üí1 avec Tanh.
        - **Entr√©e** : [œà, I, R, ET0].
        - **Sortie** : dœà/dt, int√©gr√© sur Œît=1 jour pour produire Œîœà.
        """)

    with st.expander("üöÄ Int√©gration RL", expanded=False):
        st.markdown("""
        - **Pr√©-entra√Æner** le mod√®le continu sur donn√©es simul√©es (√âtape 1 onglet 3b).
        - **Entra√Æner PPO** sur l'environnement hybride (√âtape 2 onglet 3b) : m√™mes r√©compenses que sc√©narios 2/3.
        - **√âvaluation** : onglet √âvaluation ‚Üí choisir "Sc√©nario 3b", puis Visualisation/Comparaison.
        """)
    
    with st.expander("üìê Architecture du mod√®le hybride", expanded=False):
        st.markdown("""
        ### Architecture du mod√®le hybride
    
    Le mod√®le hybride combine deux composantes :
    
    **a) Pr√©diction physique** :
    
    Le mod√®le physique calcule d'abord la r√©serve en eau $S_{t+1}^{\\text{phys}}$ selon le bilan hydrique :
    
    $$
    S_{t+1}^{\\text{phys}} = S_t + \\eta_I I_t + R_t - ETc_t - D(S_t)
    $$
    
    o√π :
    - $S_t$ : R√©serve en eau au jour $t$ (mm)
    - $\\eta_I$ : Efficacit√© d'irrigation (fraction de l'eau d'irrigation effectivement disponible)
    - $I_t$ : Dose d'irrigation appliqu√©e au jour $t$ (mm)
    - $R_t$ : Pluie au jour $t$ (mm)
    - $ETc_t$ : √âvapotranspiration culture au jour $t$ (mm), calcul√©e comme $ETc_t = Kc_t \\cdot ET0_t \\cdot f_{ET}(\\psi_t)$
    - $D(S_t)$ : Drainage (perte d'eau par percolation) au jour $t$ (mm)
    
    La tension matricielle pr√©dite par le mod√®le physique est ensuite obtenue via la courbe de r√©tention :
    
    $$
    \\psi_{t+1}^{\\text{phys}} = f_{\\text{retention}}(S_{t+1}^{\\text{phys}})
    $$
    
    o√π $f_{\\text{retention}}$ est la fonction de r√©tention d'eau du sol (relation $S \\leftrightarrow \\psi$).
    
    **b) Correction r√©siduelle par Neural ODE** :
    
    Le Neural ODE apprend une correction $\\Delta \\psi_t$ bas√©e sur l'√©tat actuel :
    
    $$
    \\Delta \\psi_t = f_\\theta(\\psi_t, I_t, R_t, ET0_t)
    $$
    
    o√π :
    - $\\psi_t$ : Tension matricielle actuelle (cbar)
    - $I_t$ : Irrigation appliqu√©e (mm)
    - $R_t$ : Pluie (mm)
    - $ET0_t$ : √âvapotranspiration de r√©f√©rence (mm/jour)
    - $f_\\theta$ : R√©seau de neurones (MLP) param√©tr√© par $\\theta$
    
    **c) Pr√©diction finale hybride** :
    
    La pr√©diction finale combine les deux composantes :
    
    $$
    \\psi_{t+1} = \\psi_{t+1}^{\\text{phys}} + \\Delta \\psi_t
    $$
    
    La r√©serve en eau corrig√©e est ensuite obtenue par inversion de la courbe de r√©tention :
    
    $$
    S_{t+1} = f_{\\text{retention}}^{-1}(\\psi_{t+1})
    $$
        """)
        
        with st.expander("üèóÔ∏è Architecture du r√©seau de neurones $f_\\theta$", expanded=False):
            st.markdown("""
            Le r√©seau de neurones $f_\\theta$ est un **MLP (Multi-Layer Perceptron)** avec :
            
            - **Couche d'entr√©e** : 4 neurones (pour $\\psi_t$, $I_t$, $R_t$, $ET0_t$)
            - **Couches cach√©es** : 2 couches de 64 neurones chacune avec activation $\\tanh$
            - **Couche de sortie** : 1 neurone (pour $\\Delta \\psi_t$)
            
            **√âquations du r√©seau** :
            
            $$
            \\mathbf{h}_1 = \\tanh(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1)
            $$
            
            $$
            \\mathbf{h}_2 = \\tanh(\\mathbf{W}_2 \\mathbf{h}_1 + \\mathbf{b}_2)
            $$
            
            $$
            \\Delta \\psi_t = \\mathbf{W}_3 \\mathbf{h}_2 + b_3
            $$
            
            o√π :
            - $\\mathbf{x} = [\\psi_t, I_t, R_t, ET0_t]^T$ est le vecteur d'entr√©e
            - $\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{W}_3$ sont les matrices de poids
            - $\\mathbf{b}_1, \\mathbf{b}_2, b_3$ sont les biais
            - $\\mathbf{h}_1, \\mathbf{h}_2$ sont les activations des couches cach√©es
            """)
    
    with st.expander("üìö Processus d'entra√Ænement", expanded=False):
        st.markdown("""
        ### Processus d'entra√Ænement
        
        Le Neural ODE est entra√Æn√© de mani√®re **supervis√©e** sur des donn√©es de simulation ou r√©elles :
        
        **a) G√©n√©ration des donn√©es d'entra√Ænement** :
        
        Pour chaque pas de temps $t$ d'une simulation, on collecte :
        - **Entr√©es** : $X_t = [\\psi_t, I_t, R_t, ET0_t]$
        - **Cible** : $y_t = \\psi_{t+1}^{\\text{r√©el}} - \\psi_{t+1}^{\\text{phys}}$
        
        o√π $\\psi_{t+1}^{\\text{r√©el}}$ peut √™tre :
        - Une mesure r√©elle de tension (si disponible)
        - Une simulation avec un mod√®le plus sophistiqu√© (HYDRUS, Aquacrop)
        - Une simulation physique avec biais artificiel pour tester la capacit√© de correction
        
        **b) Fonction de perte** :
        
        Le mod√®le est entra√Æn√© pour minimiser l'erreur quadratique moyenne :
        
        $$
        \\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\Delta \\psi_t^{(i)} - y_t^{(i)} \\right)^2
        $$
        
        o√π $N$ est le nombre d'√©chantillons d'entra√Ænement.
        
        **c) Optimisation** :
        
        Les param√®tres $\\theta$ sont optimis√©s via l'algorithme d'Adam avec un learning rate typiquement de $10^{-3}$ √† $10^{-4}$.
        """)
    
    with st.expander("üî¢ M√©thode d'int√©gration : Discr√©tisation temporelle", expanded=False):
        st.markdown("""
        ### M√©thode d'int√©gration : Approche discr√®te
        
        **Important** : Dans notre impl√©mentation, le Neural ODE utilise une **approche discr√®te** plut√¥t qu'une int√©gration continue.
        
        #### Principe de discr√©tisation
        
        Le mod√®le pr√©dit directement la correction r√©siduelle $\\Delta \\psi_t$ sur un **pas de temps fixe de 1 jour** :
        
        $$
        \\Delta \\psi_t = f_\\theta(\\psi_t, I_t, R_t, ET0_t)
        $$
        
        o√π $f_\\theta$ est un r√©seau de neurones (MLP) qui apprend directement la variation de tension sur un jour.
        
        #### Pourquoi une approche discr√®te ?
        
        - **Simplicit√©** : Pas besoin de solveurs d'ODE complexes (Euler, Runge-Kutta, etc.)
        - **Efficacit√©** : Un seul forward pass du r√©seau par pas de temps
        - **Ad√©quation au probl√®me** : Les donn√©es m√©t√©orologiques et les d√©cisions d'irrigation sont disponibles √† l'√©chelle journali√®re
        - **Stabilit√©** : √âvite les probl√®mes num√©riques li√©s aux solveurs d'ODE adaptatifs
        
        #### Comparaison avec une approche continue
        
        Dans un Neural ODE "classique" continu, on mod√©liserait :
        
        $$
        \\frac{d\\psi(t)}{dt} = f_\\theta(\\psi(t), I(t), R(t), ET0(t))
        $$
        
        et on int√©grerait cette √©quation diff√©rentielle avec un solveur (Euler, Runge-Kutta d'ordre 2 ou 4, etc.) :
        
        $$
        \\psi_{t+1} = \\psi_t + \\int_t^{t+1} f_\\theta(\\psi(\\tau), I(\\tau), R(\\tau), ET0(\\tau)) \\, d\\tau
        $$
        
        **Dans notre cas** : Le r√©seau apprend directement la solution discr√©tis√©e :
        
        $$
        \\Delta \\psi_t = \\psi_{t+1} - \\psi_t \\approx f_\\theta(\\psi_t, I_t, R_t, ET0_t)
        $$
        
        o√π l'int√©gration est implicite dans l'apprentissage du r√©seau.
        
        #### Impl√©mentation dans le code
        
        Dans l'environnement RL (`utils_env_modeles.py`), l'inf√©rence est tr√®s simple :
        
        ```python
        # Calcul direct de la correction (pas d'int√©gration)
        x = [psi_t, I_t, R_t, ET0_t]
        delta_psi = residual_ode(x)  # Un seul forward pass
        psi_next = psi_next_phys + delta_psi
        ```
        
        **Note** : Certaines variantes exp√©rimentales dans les notebooks utilisent une m√©thode du trap√®ze (Runge-Kutta d'ordre 2) avec deux √©valuations du r√©seau :
        
        $$
        k_1 = f_\\theta(\\psi_t, I_t, R_t, ET0_t)
        $$
        
        $$
        k_2 = f_\\theta(\\psi_t + 0.5 \\cdot k_1, I_t, R_t, ET0_t)
        $$
        
        $$
        \\Delta \\psi_t = \\frac{k_1 + k_2}{2}
        $$
        
        Cependant, l'impl√©mentation principale dans l'environnement RL utilise la version simple √† un seul pas.
        
        #### Avantages et limites
        
        **Avantages de l'approche discr√®te** :
        - ‚úÖ **Rapidit√©** : Calcul instantan√©, pas de solveur it√©ratif
        - ‚úÖ **Simplicit√©** : Facile √† impl√©menter et d√©boguer
        - ‚úÖ **Stabilit√©** : Pas de probl√®mes de convergence des solveurs
        - ‚úÖ **Ad√©quation** : Correspond √† la granularit√© temporelle des donn√©es disponibles
        
        **Limites** :
        - ‚ö†Ô∏è **Pas de r√©solution infra-journali√®re** : Ne peut pas mod√©liser des ph√©nom√®nes √† l'√©chelle horaire
        - ‚ö†Ô∏è **Pas de pas de temps adaptatif** : Le pas de temps est fixe (1 jour)
        - ‚ö†Ô∏è **Moins pr√©cis pour des dynamiques rapides** : Si des changements importants se produisent en moins d'un jour, ils peuvent √™tre mal captur√©s
        
        #### Alternative : Neural CDE (Sc√©nario 4)
        
        Pour des besoins de mod√©lisation plus sophistiqu√©s avec d√©pendances temporelles, le projet impl√©mente √©galement un **Neural CDE** (Controlled Differential Equation) qui utilise un sch√©ma d'Euler discretis√© sur une s√©quence d'√©tats pass√©s :
        
        $$
        Z_{k+1} = Z_k + f_\\theta(Z_k, X_k) \\cdot \\Delta X_k
        $$
        
        o√π $\\Delta X_k = X_{k+1} - X_k$ et $X_k = [\\psi_k, I_k, R_k, ET0_k]$.
        
        Cette approche capture mieux les d√©pendances temporelles longues mais n√©cessite de maintenir un historique des √©tats.
        """)
    
    with st.expander("ü§ñ Utilisation dans l'environnement RL", expanded=False):
        st.markdown("""
        ### Utilisation dans l'environnement RL
        
        Lors de l'ex√©cution dans l'environnement Gymnasium pour l'apprentissage par renforcement :
        
        **a) Inf√©rence** :
        
        √Ä chaque pas de temps $t$ :
        1. Le mod√®le physique calcule $\\psi_{t+1}^{\\text{phys}}$ √† partir de $S_t$, $I_t$, $R_t$, $ETc_t$, $D_t$
        2. Le Neural ODE calcule $\\Delta \\psi_t = f_\\theta(\\psi_t, I_t, R_t, ET0_t)$ (mode √©valuation, sans gradient)
        3. La pr√©diction finale est $\\psi_{t+1} = \\psi_{t+1}^{\\text{phys}} + \\Delta \\psi_t$
        4. La r√©serve en eau est mise √† jour : $S_{t+1} = f_{\\text{retention}}^{-1}(\\psi_{t+1})$
        
        **b) Avantages pour le RL** :
        
        - **Meilleure pr√©cision** : Le mod√®le hybride capture mieux la dynamique r√©elle du syst√®me
        - **Apprentissage plus efficace** : L'agent RL apprend sur un mod√®le plus fid√®le √† la r√©alit√©
        - **Robustesse** : Le mod√®le physique garantit des pr√©dictions dans des plages physiquement r√©alistes
        - **Adaptabilit√©** : Le Neural ODE peut √™tre r√©-entra√Æn√© avec de nouvelles donn√©es pour s'adapter aux conditions locales
        """)
    
    with st.expander("üìä Variables et notations compl√®tes", expanded=False):
        st.markdown("""
        **Variables d'√©tat** :
        - $S_t$ : R√©serve en eau du sol au jour $t$ (mm)
        - $\\psi_t$ : Tension matricielle de l'eau au jour $t$ (cbar)
        
        **Variables d'action** :
        - $I_t$ : Dose d'irrigation appliqu√©e au jour $t$ (mm)
        
        **Variables m√©t√©orologiques** :
        - $R_t$ : Pluie au jour $t$ (mm)
        - $ET0_t$ : √âvapotranspiration de r√©f√©rence au jour $t$ (mm/jour)
        - $Kc_t$ : Coefficient cultural au jour $t$ (dimensionless)
        - $ETc_t = Kc_t \\cdot ET0_t \\cdot f_{ET}(\\psi_t)$ : √âvapotranspiration culture (mm)
        
        **Variables de perte** :
        - $D(S_t)$ : Drainage (perte par percolation) au jour $t$ (mm)
        - $Q_t$ : Ruissellement au jour $t$ (mm, g√©n√©ralement n√©glig√© dans notre mod√®le)
        
        **Param√®tres du sol** :
        - $\\eta_I$ : Efficacit√© d'irrigation (fraction, typiquement 0.8)
        - $S_{\\max}$ : Capacit√© maximale de stockage (mm)
        - $S_{fc}$ : R√©serve √† la capacit√© au champ (mm)
        - $S_{wp}$ : R√©serve au point de fl√©trissement (mm)
        - $\\psi_{sat}$ : Tension √† saturation (cbar, typiquement ~10 cbar)
        - $\\psi_{wp}$ : Tension au point de fl√©trissement (cbar, typiquement ~150 cbar)
        
        **Fonctions** :
        - $f_{\\text{retention}}(S)$ : Courbe de r√©tention (relation $S \\to \\psi$)
        - $f_{\\text{retention}}^{-1}(\\psi)$ : Inversion de la courbe de r√©tention (relation $\\psi \\to S$)
        - $f_{ET}(\\psi)$ : Fonction de r√©duction de l'√©vapotranspiration selon la tension
        - $f_\\theta(\\psi, I, R, ET0)$ : R√©seau de neurones du Neural ODE
        
        **Corrections r√©siduelles** :
        - $\\Delta \\psi_t$ : Correction r√©siduelle apprise par le Neural ODE (cbar)
        - $\\psi_{t+1}^{\\text{phys}}$ : Pr√©diction du mod√®le physique (cbar)
        - $\\psi_{t+1}$ : Pr√©diction finale hybride (cbar)
        """)
    
    with st.expander("üîÑ Diff√©rence avec Neural CDE", expanded=False):
        st.markdown("""
        Le projet impl√©mente √©galement un **Neural CDE** (Controlled Differential Equation) qui est une extension plus sophistiqu√©e :
        
        - **Neural ODE** : Utilise uniquement l'√©tat actuel $[\\psi_t, I_t, R_t, ET0_t]$ pour pr√©dire $\\Delta \\psi_t$
        - **Neural CDE** : Utilise une s√©quence d'√©tats pass√©s $\\{X_{t-L+1}, \\ldots, X_t\\}$ o√π $X_k = [\\psi_k, I_k, R_k, ET0_k]$
        
        Le Neural CDE capture des d√©pendances temporelles plus longues mais n√©cessite de maintenir un historique des √©tats.
        """)
    
    with st.expander("‚öñÔ∏è Avantages et limitations", expanded=False):
        st.markdown("""
        ### ‚úÖ Avantages du mod√®le hybride Neural ODE
        
        - **Pr√©cision am√©lior√©e** : Capture les biais syst√©matiques du mod√®le physique
        - **Interpr√©tabilit√© pr√©serv√©e** : Le mod√®le physique reste la base, la correction est additive
        - **Efficacit√© computationnelle** : Inf√©rence rapide (un seul forward pass du r√©seau)
        - **Flexibilit√©** : Peut √™tre r√©-entra√Æn√© avec de nouvelles donn√©es
        - **Robustesse** : Le mod√®le physique garantit des pr√©dictions dans des plages r√©alistes
        
        ### ‚ö†Ô∏è Limitations
        
        - **Donn√©es d'entra√Ænement** : N√©cessite des donn√©es pour apprendre la correction
        - **G√©n√©ralisation** : Peut ne pas g√©n√©raliser √† des conditions tr√®s diff√©rentes de l'entra√Ænement
        - **Complexit√©** : Ajoute une couche de complexit√© par rapport au mod√®le physique pur
        """)
    
    with st.expander("üîß Param√®tres recommand√©s et tuning", expanded=False):
        st.markdown("""
        ### Hyperparam√®tres du Neural ODE
        
        **Architecture du r√©seau** :
        - **Nombre de couches cach√©es** : 2-3 (recommand√©)
        - **Dimension cach√©e** : 64-128
        - **Activation** : ReLU ou Tanh
        - **Dimension d'entr√©e** : 4 ($[\\psi_t, I_t, R_t, ET0_t]$)
        - **Dimension de sortie** : 1 ($\\Delta \\psi_t$)
        
        **Pr√©-entra√Ænement** :
        - **Nombre de trajectoires** : 32-50 (recommand√©)
        - **Nombre d'epochs** : 20-50 selon convergence
        - **Batch size** : 64-128
        - **Learning rate** : $10^{-3}$ (recommand√©)
        - **Optimiseur** : Adam
        
        **Hyperparam√®tres PPO** :
        - Identiques au Sc√©nario 2
        - Learning rate : $3 \\times 10^{-4}$
        - Gamma : 0.99
        - GAE lambda : 0.95
        
        ### Strat√©gie de tuning
        
        **1. Pr√©-entra√Ænement du Neural ODE** :
        - G√©n√©rer des trajectoires avec le mod√®le physique
        - Entra√Æner le Neural ODE √† pr√©dire $\\Delta \\psi$
        - V√©rifier que la loss converge (< 0.01 id√©alement)
        
        **2. Entra√Ænement PPO** :
        - Utiliser le Neural ODE pr√©-entra√Æn√©
        - Entra√Æner PPO comme Sc√©nario 2
        - Observer si la correction am√©liore les performances
        
        **3. Ajustements** :
        - Si correction trop faible : Augmenter la capacit√© du r√©seau
        - Si correction instable : R√©duire learning rate, ajouter r√©gularisation
        - Si pas d'am√©lioration : V√©rifier qualit√© des donn√©es d'entra√Ænement
        """)
    
    with st.expander("üß≠ Quand utiliser le Sc√©nario 3 ?", expanded=False):
        st.markdown("""
        ### ‚úÖ Choisir le Sc√©nario 3 si :
        
        - **Biais du mod√®le physique connu** :
          - Param√®tres du sol mal calibr√©s
          - Ph√©nom√®nes non mod√©lis√©s (h√©t√©rog√©n√©it√©, structure du sol)
          - Erreurs syst√©matiques dans les pr√©dictions
        
        - **Donn√©es r√©elles disponibles** :
          - Mesures de tension r√©elles pour entra√Æner la correction
          - Donn√©es historiques suffisantes
          - Qualit√© des donn√©es acceptable
        
        - **Besoin de pr√©cision am√©lior√©e** :
          - Sc√©nario 2 ne suffit pas
          - Besoin de corriger les biais du mod√®le physique
          - Performance sup√©rieure recherch√©e
        
        - **Interpr√©tabilit√© importante** :
          - Besoin de s√©parer physique et correction
          - Analyse des biais du mod√®le physique
          - Compr√©hension des ph√©nom√®nes non mod√©lis√©s
        
        ### ‚ùå Ne pas choisir le Sc√©nario 3 si :
        
        - **Mod√®le physique fiable** :
          - Pas de biais connus
          - Param√®tres bien calibr√©s
          - Sc√©nario 2 suffit
        
        - **Pas de donn√©es r√©elles** :
          - Impossible d'entra√Æner la correction
          - Seulement des simulations disponibles
          - ‚Üí Pr√©f√©rer Sc√©nario 2 ou 5
        
        - **Simplicit√© recherch√©e** :
          - Pas besoin de correction
          - Approche simple suffit
          - ‚Üí Pr√©f√©rer Sc√©nario 1 ou 2
        
        - **D√©pendances temporelles longues** :
          - Besoin de m√©moire temporelle
          - ‚Üí Pr√©f√©rer Sc√©nario 4 (Neural CDE)
        """)
    
    with st.expander("üõ†Ô∏è Conseils pratiques", expanded=False):
        st.markdown("""
        ### Workflow recommand√©
        
        **1. Validation du mod√®le physique** :
        - Comparer pr√©dictions physiques avec donn√©es r√©elles
        - Identifier les biais syst√©matiques
        - Quantifier l'erreur √† corriger
        
        **2. Pr√©-entra√Ænement Neural ODE** :
        - G√©n√©rer trajectoires avec mod√®le physique
        - Calculer $\\Delta \\psi = \\psi_{\\text{r√©el}} - \\psi_{\\text{physique}}$
        - Entra√Æner Neural ODE √† pr√©dire $\\Delta \\psi$
        - V√©rifier convergence (loss < 0.01)
        
        **3. Entra√Ænement PPO** :
        - Int√©grer Neural ODE dans l'environnement
        - Entra√Æner PPO comme Sc√©nario 2
        - Comparer performances avec Sc√©nario 2
        
        **4. Analyse** :
        - Analyser la correction apprise
        - Identifier quels ph√©nom√®nes sont corrig√©s
        - V√©rifier la coh√©rence physique
        
        ### Troubleshooting
        
        **Probl√®me : Correction trop faible**
        - **Sympt√¥me** : $\\Delta \\psi$ toujours proche de 0
        - **Solutions** :
          - Augmenter capacit√© du r√©seau (plus de couches/neurones)
          - V√©rifier qualit√© des donn√©es d'entra√Ænement
          - Augmenter nombre d'epochs
        
        **Probl√®me : Correction instable**
        - **Sympt√¥me** : $\\Delta \\psi$ oscille, pr√©dictions erratiques
        - **Solutions** :
          - R√©duire learning rate
          - Ajouter r√©gularisation (L2, dropout)
          - R√©duire capacit√© du r√©seau
        
        **Probl√®me : Pas d'am√©lioration vs Sc√©nario 2**
        - **Sympt√¥me** : Performance similaire ou pire
        - **Solutions** :
          - V√©rifier que le Neural ODE est bien pr√©-entra√Æn√©
          - V√©rifier qualit√© des donn√©es
          - Augmenter nombre de trajectoires d'entra√Ænement
        """)
    
    with st.expander("üîó Comparaison avec les autres sc√©narios", expanded=False):
        st.markdown("""
        ### Sc√©nario 3 vs Sc√©nario 2 (RL basique)
        
        **Sc√©nario 2** :
        - RL direct sur mod√®le physique
        - Pas de correction du mod√®le
        
        **Sc√©nario 3** :
        - Correction r√©siduelle du mod√®le physique
        - Am√©liore la pr√©diction
        
        **Quand choisir Sc√©nario 3** : Biais du mod√®le physique connus
        
        ### Sc√©nario 3 vs Sc√©nario 4 (Neural CDE)
        
        **Sc√©nario 3** :
        - Neural ODE : Pas de m√©moire temporelle
        - Correction locale bas√©e sur √©tat actuel
        
        **Sc√©nario 4** :
        - Neural CDE : M√©moire temporelle
        - Correction bas√©e sur historique
        
        **Quand choisir Sc√©nario 4** : Besoin de d√©pendances temporelles
        
        ### Sc√©nario 3 vs Sc√©narios 5-6
        
        **Sc√©nario 3** :
        - Correction du mod√®le physique
        - Am√©liore pr√©diction
        
        **Sc√©narios 5-6** :
        - Enrichissement observation (5) ou planification (6)
        - R√¥le diff√©rent (pas de correction physique)
        
        **Compl√©mentarit√©** : Peut √™tre combin√© avec Sc√©nario 5
        """)
    
    # ========================================================================
    # ONGLET DOCUMENTATION 5 : NEURAL CDE
    # ========================================================================

def render_doc_neural_cde():
    """
    Affiche le contenu de l'onglet de documentation : Neural CDE.
    """
    st.markdown('<h2 class="section-header">üåÄ Neural CDE : Mod√®le hybride avec d√©pendances temporelles</h2>', unsafe_allow_html=True)
    
    st.markdown("""
    ### ‚ùì Qu'est-ce qu'un Neural CDE ?
    
    Un **Neural CDE** (Neural Controlled Differential Equation) est une extension des Neural ODE qui utilise une s√©quence d'√©tats pass√©s pour capturer des d√©pendances temporelles plus longues. 
    Dans notre contexte, il s'agit d'un **mod√®le r√©siduel** qui apprend √† corriger les pr√©dictions d'un mod√®le physique en exploitant l'historique des √©tats.
    """)
    
    with st.expander("üî¨ Principe g√©n√©ral des Neural CDE", expanded=False):
        st.markdown("""
        Les Neural CDE mod√©lisent la dynamique d'un syst√®me par une √©quation diff√©rentielle contr√¥l√©e o√π la d√©riv√©e d√©pend d'une s√©quence d'√©tats pass√©s :
        
        $$
        dZ_t = f_\\theta(Z_t, X_t) \\, dX_t
        $$
        
        o√π :
        - $Z_t$ est l'√©tat latent du syst√®me au temps $t$
        - $X_t$ est le processus de contr√¥le (s√©quence d'observations)
        - $f_\\theta$ est un r√©seau de neurones param√©tr√© par $\\theta$
        - L'int√©gration se fait le long du chemin de $X_t$
        
        **Diff√©rence avec Neural ODE** :
        - **Neural ODE** : $\\frac{dZ(t)}{dt} = f_\\theta(Z(t), t)$ d√©pend uniquement de l'√©tat actuel
        - **Neural CDE** : $dZ_t = f_\\theta(Z_t, X_t) \\, dX_t$ d√©pend de la s√©quence compl√®te $\\{X_s : s \\leq t\\}$
        
        **Avantages** :
        - **D√©pendances temporelles** : Capture des effets √† long terme et des dynamiques complexes
        - **Robustesse** : G√®re mieux les donn√©es irr√©guli√®res ou manquantes
        - **Flexibilit√©** : Peut mod√©liser des processus avec m√©moire longue
        """)
    
    with st.expander("üéØ Application dans notre projet : Mod√®le hybride avec m√©moire", expanded=False):
        st.markdown("""
        Dans notre projet d'irrigation intelligente, le Neural CDE est utilis√© comme **correction r√©siduelle** sur le mod√®le physique FAO, 
        mais avec une **m√©moire temporelle** pour capturer des effets √† long terme :
        
        - **Mod√®le physique** : Fournit une pr√©diction de base bas√©e sur les lois de la physique (bilan hydrique)
        - **Neural CDE** : Apprend les √©carts syst√©matiques en utilisant une s√©quence d'√©tats pass√©s
        
        Cette approche hybride permet de :
        - ‚úÖ B√©n√©ficier de la robustesse et de l'interpr√©tabilit√© du mod√®le physique
        - ‚úÖ Capturer les biais syst√©matiques et les ph√©nom√®nes complexes non mod√©lis√©s
        - ‚úÖ Mod√©liser des d√©pendances temporelles longues (effets cumulatifs de l'irrigation, s√©cheresses prolong√©es, etc.)
        - ‚úÖ S'adapter aux sp√©cificit√©s locales avec une meilleure compr√©hension de l'historique
        
        ### üìã Param√®tres de configuration du pr√©-entra√Ænement
        
        **Nombre de trajectoires** :
        - **Signification** : Nombre de simulations ind√©pendantes utilis√©es pour g√©n√©rer les donn√©es d'entra√Ænement du Neural CDE
        - **Valeur usuelle** : 32 trajectoires (par d√©faut)
        - **Impact** : Plus de trajectoires = plus de diversit√© dans les s√©quences temporelles
        - **Tuning** : Augmenter (50-100) pour capturer plus de variabilit√©, r√©duire (10-20) pour acc√©l√©rer
        - **Note** : Important pour avoir suffisamment de s√©quences vari√©es pour l'apprentissage s√©quentiel
        
        **Nombre d'epochs** :
        - **Signification** : Nombre de passages complets sur l'ensemble des donn√©es d'entra√Ænement
        - **Valeur usuelle** : 10 epochs (par d√©faut)
        - **Impact** : Plus d'epochs = meilleur apprentissage des d√©pendances temporelles
        - **Tuning** : Augmenter (20-50) si le mod√®le continue √† apprendre, r√©duire si surapprentissage
        - **Note** : Les mod√®les s√©quentiels peuvent n√©cessiter plus d'epochs que les mod√®les sans m√©moire
        
        **Taille des batches** :
        - **Signification** : Nombre de s√©quences trait√©es simultan√©ment lors de chaque mise √† jour
        - **Valeur usuelle** : 256 (par d√©faut)
        - **Impact** : Batch plus grand = gradients plus stables pour les s√©quences
        - **Tuning** : R√©duire (32-128) si m√©moire limit√©e, augmenter (512+) pour plus de stabilit√©
        - **Note** : Doit tenir compte de la longueur de s√©quence (plus de m√©moire requise)
        
        **Taux d'apprentissage (Learning Rate)** :
        - **Signification** : Vitesse d'ajustement des param√®tres lors de l'optimisation
        - **Valeur usuelle** : $10^{-3}$ (0.001) (par d√©faut)
        - **Impact** : LR trop √©lev√© = instabilit√© dans l'apprentissage s√©quentiel
        - **Tuning** : R√©duire (10^{-4} - 10^{-5}) si la loss oscille, augmenter (10^{-2}) si trop lent
        - **Note** : Les mod√®les s√©quentiels sont souvent plus sensibles au LR que les mod√®les sans m√©moire
        
        **Longueur de s√©quence** :
        - **Signification** : Nombre de pas de temps dans l'historique utilis√© par le Neural CDE
        - **Valeur usuelle** : 5 pas de temps (par d√©faut)
        - **Impact** : S√©quence plus longue = capture de d√©pendances plus longues mais plus de complexit√©
        - **Tuning** : Augmenter (7-10) pour effets cumulatifs longs, r√©duire (3-4) pour d√©pendances courtes
        - **Note** : Doit √™tre adapt√© √† la nature des d√©pendances temporelles du probl√®me (effets d'irrigation sur plusieurs jours)
        """)
    
    with st.expander("üìê Architecture du mod√®le hybride Neural CDE", expanded=False):
        st.markdown("""
        ### Architecture du mod√®le hybride Neural CDE
    
    Le mod√®le hybride combine deux composantes, avec une approche s√©quentielle pour la correction :
    
    **a) Pr√©diction physique** :
    
    Le mod√®le physique calcule d'abord la r√©serve en eau $S_{t+1}^{\\text{phys}}$ selon le bilan hydrique :
    
    $$
    S_{t+1}^{\\text{phys}} = S_t + \\eta_I I_t + R_t - ETc_t - D(S_t)
    $$
    
    o√π :
    - $S_t$ : R√©serve en eau au jour $t$ (mm)
    - $\\eta_I$ : Efficacit√© d'irrigation (fraction de l'eau d'irrigation effectivement disponible)
    - $I_t$ : Dose d'irrigation appliqu√©e au jour $t$ (mm)
    - $R_t$ : Pluie au jour $t$ (mm)
    - $ETc_t$ : √âvapotranspiration culture au jour $t$ (mm), calcul√©e comme $ETc_t = Kc_t \\cdot ET0_t \\cdot f_{ET}(\\psi_t)$
    - $D(S_t)$ : Drainage (perte d'eau par percolation) au jour $t$ (mm)
    
    La tension matricielle pr√©dite par le mod√®le physique est ensuite obtenue via la courbe de r√©tention :
    
    $$
    \\psi_{t+1}^{\\text{phys}} = f_{\\text{retention}}(S_{t+1}^{\\text{phys}})
    $$
    
    o√π $f_{\\text{retention}}$ est la fonction de r√©tention d'eau du sol (relation $S \\leftrightarrow \\psi$).
    
    **b) Correction r√©siduelle par Neural CDE** :
    
    Le Neural CDE apprend une correction $\\Delta \\psi_t$ bas√©e sur une **s√©quence d'√©tats pass√©s** :
    
    $$
    X_k = [\\psi_k, I_k, R_k, ET0_k] \\quad \\text{pour } k \\in \\{t-L+1, \\ldots, t\\}
    $$
    
    o√π $L$ est la longueur de la s√©quence (typiquement $L = 5$ jours).
    
    Le mod√®le utilise un sch√©ma d'Euler discretis√© pour int√©grer l'√©quation diff√©rentielle contr√¥l√©e :
    
    $$
    Z_{k+1} = Z_k + f_\\theta(Z_k, X_k) \\cdot \\Delta X_k
    $$
    
    o√π :
    - $Z_k$ est l'√©tat latent au pas $k$
    - $\\Delta X_k = X_{k+1} - X_k$ est l'incr√©ment du processus de contr√¥le
    - $f_\\theta$ est un r√©seau de neurones (MLP) param√©tr√© par $\\theta$
    
    La correction finale est obtenue √† partir de l'√©tat latent final :
    
    $$
    \\Delta \\psi_t = g_\\phi(Z_t)
    $$
    
    o√π $g_\\phi$ est une couche de sortie qui mappe l'√©tat latent vers la correction r√©siduelle.
    
    **c) Pr√©diction finale hybride** :
    
    La pr√©diction finale combine les deux composantes :
    
    $$
    \\psi_{t+1} = \\psi_{t+1}^{\\text{phys}} + \\Delta \\psi_t
    $$
    
    La r√©serve en eau corrig√©e est ensuite obtenue par inversion de la courbe de r√©tention :
    
    $$
    S_{t+1} = f_{\\text{retention}}^{-1}(\\psi_{t+1})
    $$
        """)
        
        with st.expander("üèóÔ∏è Architecture du r√©seau de neurones $f_\\theta$ et $g_\\phi$", expanded=False):
            st.markdown("""
            Le r√©seau de neurones $f_\\theta$ est un **MLP (Multi-Layer Perceptron)** avec :
            
            - **Couche d'entr√©e** : $\\dim(Z_k) + 4$ neurones (pour l'√©tat latent $Z_k$ et $X_k = [\\psi_k, I_k, R_k, ET0_k]$)
            - **Couches cach√©es** : 2 couches de 64 neurones chacune avec activation $\\tanh$
            - **Couche de sortie** : $\\dim(Z_k)$ neurones (pour la mise √† jour de l'√©tat latent)
            
            La couche de sortie $g_\\phi$ est un MLP simple :
            
            - **Couche d'entr√©e** : $\\dim(Z_t)$ neurones (pour l'√©tat latent final $Z_t$)
            - **Couche cach√©e** : 1 couche de 32 neurones avec activation $\\tanh$
            - **Couche de sortie** : 1 neurone (pour $\\Delta \\psi_t$)
            
            **√âquations du r√©seau** :
            
            Pour $f_\\theta$ :
            $$
            \\mathbf{h}_1 = \\tanh(\\mathbf{W}_1 [Z_k, X_k] + \\mathbf{b}_1)
            $$
            
            $$
            \\mathbf{h}_2 = \\tanh(\\mathbf{W}_2 \\mathbf{h}_1 + \\mathbf{b}_2)
            $$
            
            $$
            \\Delta Z_k = \\mathbf{W}_3 \\mathbf{h}_2 + \\mathbf{b}_3
            $$
            
            Pour $g_\\phi$ :
            $$
            \\mathbf{h}_{out} = \\tanh(\\mathbf{W}_{out} Z_t + \\mathbf{b}_{out})
            $$
            
            $$
            \\Delta \\psi_t = \\mathbf{W}_{final} \\mathbf{h}_{out} + b_{final}
            $$
            
            o√π :
            - $[Z_k, X_k]$ est la concat√©nation de l'√©tat latent et de l'observation
            - $\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{W}_3, \\mathbf{W}_{out}, \\mathbf{W}_{final}$ sont les matrices de poids
            - $\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{b}_3, \\mathbf{b}_{out}, b_{final}$ sont les biais
            """)
    
    with st.expander("üìö Processus d'entra√Ænement", expanded=False):
        st.markdown("""
        ### Processus d'entra√Ænement
        
        Le Neural CDE est entra√Æn√© de mani√®re **supervis√©e** sur des s√©quences de donn√©es de simulation ou r√©elles :
        
        **a) G√©n√©ration des donn√©es d'entra√Ænement** :
        
        Pour chaque pas de temps $t$ d'une simulation, on collecte :
        - **Entr√©es** : S√©quence $\\{X_{t-L+1}, \\ldots, X_t\\}$ o√π $X_k = [\\psi_k, I_k, R_k, ET0_k]$
        - **Cible** : $y_t = \\psi_{t+1}^{\\text{r√©el}} - \\psi_{t+1}^{\\text{phys}}$
        
        o√π $\\psi_{t+1}^{\\text{r√©el}}$ peut √™tre :
        - Une mesure r√©elle de tension (si disponible)
        - Une simulation avec un mod√®le plus sophistiqu√© (HYDRUS, Aquacrop)
        - Une simulation physique avec biais artificiel pour tester la capacit√© de correction
        
        **b) Fonction de perte** :
        
        Le mod√®le est entra√Æn√© pour minimiser l'erreur quadratique moyenne sur les s√©quences :
        
        $$
        \\mathcal{L}(\\theta, \\phi) = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\Delta \\psi_t^{(i)} - y_t^{(i)} \\right)^2
        $$
        
        o√π $N$ est le nombre d'√©chantillons d'entra√Ænement (s√©quences).
        
        **c) Optimisation** :
        
        Les param√®tres $\\theta$ et $\\phi$ sont optimis√©s via l'algorithme d'Adam avec un learning rate typiquement de $10^{-3}$ √† $10^{-4}$.
        L'entra√Ænement se fait par mini-batches de s√©quences.
        """)
    
    with st.expander("üî¢ M√©thode d'int√©gration : Sch√©ma d'Euler discretis√©", expanded=False):
        st.markdown("""
        ### M√©thode d'int√©gration : Approche discr√®te avec s√©quence
        
        **Important** : Dans notre impl√©mentation, le Neural CDE utilise un **sch√©ma d'Euler discretis√©** sur une s√©quence d'√©tats pass√©s.
        
        #### Principe d'int√©gration s√©quentielle
        
        Le mod√®le int√®gre l'√©quation diff√©rentielle contr√¥l√©e sur une s√©quence de $L$ pas de temps :
        
        $$
        Z_{k+1} = Z_k + f_\\theta(Z_k, X_k) \\cdot \\Delta X_k \\quad \\text{pour } k \\in \\{t-L+1, \\ldots, t-1\\}
        $$
        
        o√π :
        - $Z_{t-L+1} = 0$ (√©tat latent initialis√© √† z√©ro)
        - $\\Delta X_k = X_{k+1} - X_k$ est l'incr√©ment du processus de contr√¥le
        - $f_\\theta(Z_k, X_k)$ est √©valu√© √† chaque pas $k$
        
        La correction finale est obtenue √† partir de l'√©tat latent final :
        
        $$
        \\Delta \\psi_t = g_\\phi(Z_t)
        $$
        
        #### Pourquoi une approche s√©quentielle ?
        
        - **D√©pendances temporelles** : Capture les effets cumulatifs et les dynamiques √† long terme
        - **M√©moire** : Maintient un historique des √©tats pour mieux pr√©dire les corrections
        - **Robustesse** : G√®re mieux les variations temporelles complexes
        - **Ad√©quation au probl√®me** : Les effets de l'irrigation et de la m√©t√©o peuvent avoir des impacts sur plusieurs jours
        
        #### Comparaison avec Neural ODE
        
        **Neural ODE** :
        - Utilise uniquement l'√©tat actuel : $\\Delta \\psi_t = f_\\theta(\\psi_t, I_t, R_t, ET0_t)$
        - Pas de m√©moire temporelle
        - Plus simple et plus rapide
        
        **Neural CDE** :
        - Utilise une s√©quence d'√©tats : $\\{X_{t-L+1}, \\ldots, X_t\\}$
        - Capture des d√©pendances temporelles longues
        - Plus complexe mais plus expressif
        
        #### Impl√©mentation dans le code
        
        Dans l'environnement RL (`utils_env_modeles.py`), l'inf√©rence se fait s√©quentiellement :
        
        ```python
        # Initialisation
        Z = torch.zeros(hidden_dim)
        X_prev = X_sequence[0]  # Premier √©tat de la s√©quence
        
        # Int√©gration s√©quentielle
        for k in range(1, seq_len):
            X_k = X_sequence[k]
            dX = X_k - X_prev
            dZ = f_theta(Z, X_k) * dX
            Z = Z + dZ
            X_prev = X_k
        
        # Correction finale
        delta_psi = g_phi(Z)
        psi_next = psi_next_phys + delta_psi
        ```
        
        #### Avantages et limites
        
        **Avantages de l'approche s√©quentielle** :
        - ‚úÖ **D√©pendances temporelles** : Capture des effets √† long terme
        - ‚úÖ **M√©moire** : Exploite l'historique des √©tats
        - ‚úÖ **Expressivit√©** : Mod√©lise des dynamiques complexes
        - ‚úÖ **Robustesse** : G√®re mieux les variations temporelles
        
        **Limites** :
        - ‚ö†Ô∏è **Complexit√© computationnelle** : N√©cessite $L$ √©valuations du r√©seau par pr√©diction
        - ‚ö†Ô∏è **M√©moire** : Doit maintenir un historique de $L$ √©tats
        - ‚ö†Ô∏è **Temps d'entra√Ænement** : Plus long que Neural ODE
        - ‚ö†Ô∏è **Hyperparam√®tres** : N√©cessite de choisir $L$ (longueur de s√©quence)
        """)
    
    with st.expander("ü§ñ Utilisation dans l'environnement RL", expanded=False):
        st.markdown("""
        ### Utilisation dans l'environnement RL
        
        Lors de l'ex√©cution dans l'environnement Gymnasium pour l'apprentissage par renforcement :
        
        **a) Inf√©rence** :
        
        √Ä chaque pas de temps $t$ :
        1. Le mod√®le physique calcule $\\psi_{t+1}^{\\text{phys}}$ √† partir de $S_t$, $I_t$, $R_t$, $ETc_t$, $D_t$
        2. Le Neural CDE utilise la s√©quence $\\{X_{t-L+1}, \\ldots, X_t\\}$ pour calculer $\\Delta \\psi_t$ (mode √©valuation, sans gradient)
        3. La pr√©diction finale est $\\psi_{t+1} = \\psi_{t+1}^{\\text{phys}} + \\Delta \\psi_t$
        4. La r√©serve en eau est mise √† jour : $S_{t+1} = f_{\\text{retention}}^{-1}(\\psi_{t+1})$
        5. L'historique est mis √† jour : $X_{t+1} = [\\psi_{t+1}, I_{t+1}, R_{t+1}, ET0_{t+1}]$
        
        **b) Avantages pour le RL** :
        
        - **Meilleure pr√©cision** : Le mod√®le hybride capture mieux la dynamique r√©elle avec m√©moire temporelle
        - **Apprentissage plus efficace** : L'agent RL apprend sur un mod√®le plus fid√®le √† la r√©alit√©
        - **Robustesse** : Le mod√®le physique garantit des pr√©dictions dans des plages physiquement r√©alistes
        - **Adaptabilit√©** : Le Neural CDE peut √™tre r√©-entra√Æn√© avec de nouvelles donn√©es pour s'adapter aux conditions locales
        - **D√©pendances temporelles** : Capture mieux les effets cumulatifs de l'irrigation et de la m√©t√©o
        """)
    
    with st.expander("üìä Variables et notations compl√®tes", expanded=False):
        st.markdown("""
        **Variables d'√©tat** :
        - $S_t$ : R√©serve en eau du sol au jour $t$ (mm)
        - $\\psi_t$ : Tension matricielle de l'eau au jour $t$ (cbar)
        - $Z_t$ : √âtat latent du Neural CDE au jour $t$ (vecteur de dimension $d$)
        
        **Variables d'action** :
        - $I_t$ : Dose d'irrigation appliqu√©e au jour $t$ (mm)
        
        **Variables m√©t√©orologiques** :
        - $R_t$ : Pluie au jour $t$ (mm)
        - $ET0_t$ : √âvapotranspiration de r√©f√©rence au jour $t$ (mm/jour)
        - $Kc_t$ : Coefficient cultural au jour $t$ (dimensionless)
        - $ETc_t = Kc_t \\cdot ET0_t \\cdot f_{ET}(\\psi_t)$ : √âvapotranspiration culture (mm)
        
        **Variables de perte** :
        - $D(S_t)$ : Drainage (perte par percolation) au jour $t$ (mm)
        - $Q_t$ : Ruissellement au jour $t$ (mm, g√©n√©ralement n√©glig√© dans notre mod√®le)
        
        **Param√®tres du sol** :
        - $\\eta_I$ : Efficacit√© d'irrigation (fraction, typiquement 0.8)
        - $S_{\\max}$ : Capacit√© maximale de stockage (mm)
        - $S_{fc}$ : R√©serve √† la capacit√© au champ (mm)
        - $S_{wp}$ : R√©serve au point de fl√©trissement (mm)
        - $\\psi_{sat}$ : Tension √† saturation (cbar, typiquement ~10 cbar)
        - $\\psi_{wp}$ : Tension au point de fl√©trissement (cbar, typiquement ~150 cbar)
        
        **Param√®tres du Neural CDE** :
        - $L$ : Longueur de la s√©quence d'√©tats (typiquement $L = 5$ jours)
        - $d$ : Dimension de l'√©tat latent $Z_t$ (typiquement $d = 32$)
        
        **Fonctions** :
        - $f_{\\text{retention}}(S)$ : Courbe de r√©tention (relation $S \\to \\psi$)
        - $f_{\\text{retention}}^{-1}(\\psi)$ : Inversion de la courbe de r√©tention (relation $\\psi \\to S$)
        - $f_{ET}(\\psi)$ : Fonction de r√©duction de l'√©vapotranspiration selon la tension
        - $f_\\theta(Z, X)$ : R√©seau de neurones du Neural CDE pour la dynamique de l'√©tat latent
        - $g_\\phi(Z)$ : R√©seau de neurones du Neural CDE pour la correction r√©siduelle
        
        **S√©quences** :
        - $X_k = [\\psi_k, I_k, R_k, ET0_k]$ : Vecteur d'observation au pas $k$
        - $\\{X_{t-L+1}, \\ldots, X_t\\}$ : S√©quence d'observations utilis√©e pour la pr√©diction
        - $\\Delta X_k = X_{k+1} - X_k$ : Incr√©ment du processus de contr√¥le
        
        **Corrections r√©siduelles** :
        - $\\Delta \\psi_t$ : Correction r√©siduelle apprise par le Neural CDE (cbar)
        - $\\psi_{t+1}^{\\text{phys}}$ : Pr√©diction du mod√®le physique (cbar)
        - $\\psi_{t+1}$ : Pr√©diction finale hybride (cbar)
        """)
    
    with st.expander("üîÑ Diff√©rence avec Neural ODE", expanded=False):
        st.markdown("""
        **Neural ODE** :
        - Utilise uniquement l'√©tat actuel : $\\Delta \\psi_t = f_\\theta(\\psi_t, I_t, R_t, ET0_t)$
        - Pas de m√©moire temporelle
        - Plus simple et plus rapide
        - Ad√©quat pour des dynamiques √† court terme
        
        **Neural CDE** :
        - Utilise une s√©quence d'√©tats : $\\{X_{t-L+1}, \\ldots, X_t\\}$ o√π $X_k = [\\psi_k, I_k, R_k, ET0_k]$
        - Capture des d√©pendances temporelles longues
        - Plus complexe mais plus expressif
        - Ad√©quat pour des dynamiques √† long terme et des effets cumulatifs
        
        **Quand utiliser Neural CDE ?**
        - Effets cumulatifs de l'irrigation sur plusieurs jours
        - S√©cheresses prolong√©es avec impacts retard√©s
        - Dynamiques complexes n√©cessitant une m√©moire temporelle
        - Donn√©es avec d√©pendances temporelles importantes
        """)
    
    with st.expander("üî¨ Relation avec les Physics-Informed Neural Networks (PINN)", expanded=False):
        st.markdown("""
        ### Neural CDE comme mod√®le Physics-Informed
        
        Le Neural CDE utilis√© dans ce projet peut √™tre consid√©r√© comme un **mod√®le hybride physics-informed avec m√©moire temporelle** :
        
        **D√©finition des PINN** :
        Les Physics-Informed Neural Networks (PINN) sont des r√©seaux de neurones qui int√®grent explicitement les lois de la physique dans leur architecture ou leur fonction de perte.
        
        **Notre approche hybride** :
        - **Mod√®le physique** : Fournit la structure et les contraintes physiques (bilan hydrique FAO)
        - **Neural CDE** : Apprend une correction r√©siduelle bas√©e sur une s√©quence d'√©tats pass√©s, respectant la structure physique
        
        **Caract√©ristiques physics-informed** :
        - ‚úÖ **Int√©gration explicite de la physique** : Le mod√®le physique FAO est int√©gr√© directement dans l'architecture
        - ‚úÖ **Respect des contraintes physiques** : La correction $\\Delta \\psi$ est appliqu√©e de mani√®re coh√©rente avec le mod√®le physique
        - ‚úÖ **M√©moire temporelle guid√©e par la physique** : Le Neural CDE utilise l'historique des √©tats physiques pour produire une correction coh√©rente
        - ‚úÖ **Apprentissage guid√© par la physique** : Le mod√®le apprend √† partir de donn√©es mais dans le contexte d'un mod√®le physique avec d√©pendances temporelles
        
        **Avantage par rapport aux PINN classiques** :
        - **PINN classiques** : G√©n√©ralement sans m√©moire, mod√©lisent des processus instantan√©s
        - **Notre Neural CDE** : Capture des d√©pendances temporelles longues tout en respectant la physique, permettant de mod√©liser des effets cumulatifs (s√©chage progressif, pluies r√©p√©t√©es, etc.)
        
        **Conclusion** :
        Notre mod√®le Neural CDE est un **mod√®le hybride physics-informed avec m√©moire temporelle** qui combine :
        - La robustesse et l'interpr√©tabilit√© du mod√®le physique
        - La flexibilit√© d'apprentissage des r√©seaux de neurones
        - La capacit√© de capturer des d√©pendances temporelles longues
        """)
    
    with st.expander("‚öñÔ∏è Avantages et limitations", expanded=False):
        st.markdown("""
        ### ‚úÖ Avantages du mod√®le hybride Neural CDE
        
        - **Pr√©cision am√©lior√©e** : Capture les biais syst√©matiques du mod√®le physique avec m√©moire temporelle
        - **D√©pendances temporelles** : Mod√©lise des effets √† long terme et des dynamiques complexes
        - **Interpr√©tabilit√© pr√©serv√©e** : Le mod√®le physique reste la base, la correction est additive
        - **Efficacit√© computationnelle** : Inf√©rence rapide (s√©quentielle mais parall√©lisable)
        - **Flexibilit√©** : Peut √™tre r√©-entra√Æn√© avec de nouvelles donn√©es
        - **Robustesse** : Le mod√®le physique garantit des pr√©dictions dans des plages r√©alistes
        - **M√©moire** : Exploite l'historique des √©tats pour de meilleures pr√©dictions
        
        ### ‚ö†Ô∏è Limitations
        
        - **Donn√©es d'entra√Ænement** : N√©cessite des donn√©es s√©quentielles pour apprendre la correction
        - **Complexit√© computationnelle** : N√©cessite $L$ √©valuations du r√©seau par pr√©diction
        - **M√©moire** : Doit maintenir un historique de $L$ √©tats
        - **Hyperparam√®tres** : N√©cessite de choisir $L$ (longueur de s√©quence) et $d$ (dimension de l'√©tat latent)
        - **G√©n√©ralisation** : Peut ne pas g√©n√©raliser √† des conditions tr√®s diff√©rentes de l'entra√Ænement
        - **Temps d'entra√Ænement** : Plus long que Neural ODE
        """)
    
    with st.expander("üîß Param√®tres recommand√©s et tuning", expanded=False):
        st.markdown("""
        ### Hyperparam√®tres du Neural CDE
        
        **Longueur de s√©quence** ($L$) : 5-10 (recommand√©)
        - Trop court (< 5) : Pas assez de m√©moire temporelle
        - Trop long (> 15) : Complexit√© computationnelle √©lev√©e, risque de surapprentissage
        - Tuning : Commencer √† 8, ajuster selon r√©sultats
        
        **Dimension de l'√©tat latent** ($d$) : 16-32 (recommand√©)
        - Plus grande : Plus de capacit√© mais plus complexe
        - Plus petite : Plus simple mais moins de capacit√©
        - Tuning : Commencer √† 16, augmenter si n√©cessaire
        
        **Architecture du r√©seau** :
        - **Nombre de couches cach√©es** : 2-3
        - **Dimension cach√©e** : 64-128
        - **Activation** : ReLU ou Tanh
        
        **Pr√©-entra√Ænement** :
        - **Nombre de trajectoires** : 50-100 (plus que Neural ODE)
        - **Nombre d'epochs** : 30-60 selon convergence
        - **Batch size** : 32-64 (plus petit que Neural ODE)
        - **Learning rate** : $10^{-3}$ (recommand√©)
        
        **Hyperparam√®tres PPO** :
        - Identiques au Sc√©nario 2
        - Learning rate : $3 \\times 10^{-4}$
        - Gamma : 0.99
        
        ### Strat√©gie de tuning
        
        **1. Choix de $L$** :
        - Commencer avec $L = 8$
        - Augmenter si besoin de m√©moire plus longue
        - R√©duire si complexit√© trop √©lev√©e
        
        **2. Pr√©-entra√Ænement** :
        - Plus de trajectoires que Neural ODE (m√©moire n√©cessite plus de donn√©es)
        - V√©rifier convergence de la loss
        - Analyser la qualit√© de la correction
        
        **3. Entra√Ænement PPO** :
        - Comme Sc√©nario 3
        - Comparer avec Sc√©nario 3 pour √©valuer gain
        """)
    
    with st.expander("üß≠ Quand utiliser le Sc√©nario 4 ?", expanded=False):
        st.markdown("""
        ### ‚úÖ Choisir le Sc√©nario 4 si :
        
        - **D√©pendances temporelles importantes** :
          - Effets √† long terme des d√©cisions d'irrigation
          - Dynamiques complexes n√©cessitant m√©moire
          - Sc√©nario 3 (Neural ODE) insuffisant
        
        - **Donn√©es s√©quentielles disponibles** :
          - Historique de mesures disponible
          - Donn√©es temporelles de qualit√©
          - S√©ries temporelles compl√®tes
        
        - **Biais temporels du mod√®le** :
          - Mod√®le physique ne capture pas bien les effets retard√©s
          - N√©cessit√© de corriger avec m√©moire temporelle
          - Ph√©nom√®nes avec inertie (drainage, remont√©e capillaire)
        
        - **Performance maximale recherch√©e** :
          - Sc√©nario 3 ne suffit pas
          - Besoin de meilleure pr√©cision
          - Ressources computationnelles disponibles
        
        ### ‚ùå Ne pas choisir le Sc√©nario 4 si :
        
        - **Pas de d√©pendances temporelles** :
          - Effets locaux uniquement
          - Sc√©nario 3 (Neural ODE) suffit
          - Pas besoin de m√©moire
        
        - **Ressources limit√©es** :
          - Complexit√© computationnelle trop √©lev√©e
          - Temps d'entra√Ænement trop long
          - M√©moire insuffisante
        
        - **Donn√©es insuffisantes** :
          - Pas assez de donn√©es s√©quentielles
          - Qualit√© des donn√©es insuffisante
          - ‚Üí Pr√©f√©rer Sc√©nario 3
        
        - **Simplicit√© recherch√©e** :
          - Approche simple suffit
          - ‚Üí Pr√©f√©rer Sc√©narios 1-3
        """)
    
    with st.expander("üõ†Ô∏è Conseils pratiques", expanded=False):
        st.markdown("""
        ### Workflow recommand√©
        
        **1. √âvaluer besoin de m√©moire** :
        - Analyser si Sc√©nario 3 suffit
        - Identifier d√©pendances temporelles
        - Quantifier gain potentiel
        
        **2. Choix de $L$** :
        - Commencer avec $L = 8$
        - Tester avec $L = 5, 10, 12$
        - Choisir selon performance/complexit√©
        
        **3. Pr√©-entra√Ænement** :
        - G√©n√©rer plus de trajectoires que Neural ODE
        - Entra√Æner Neural CDE
        - V√©rifier qualit√© de correction
        
        **4. Entra√Ænement PPO** :
        - Comme Sc√©nario 3
        - Comparer performances
        
        ### Troubleshooting
        
        **Probl√®me : Complexit√© computationnelle trop √©lev√©e**
        - **Sympt√¥me** : Entra√Ænement tr√®s lent
        - **Solutions** :
          - R√©duire $L$ (ex: 8 ‚Üí 5)
          - R√©duire dimension latente $d$
          - R√©duire batch size
        
        **Probl√®me : Pas d'am√©lioration vs Sc√©nario 3**
        - **Sympt√¥me** : Performance similaire
        - **Solutions** :
          - V√©rifier que $L$ est adapt√©
          - Augmenter nombre de trajectoires
          - V√©rifier qualit√© des donn√©es s√©quentielles
        
        **Probl√®me : Surapprentissage**
        - **Sympt√¥me** : Bonne performance entra√Ænement, mauvaise g√©n√©ralisation
        - **Solutions** :
          - R√©duire $L$
          - Ajouter r√©gularisation
          - Augmenter nombre de trajectoires d'entra√Ænement
        """)
    
    with st.expander("üîó Comparaison avec les autres sc√©narios", expanded=False):
        st.markdown("""
        ### Sc√©nario 4 vs Sc√©nario 3 (Neural ODE)
        
        **Sc√©nario 3** :
        - Neural ODE : Pas de m√©moire
        - Correction locale
        
        **Sc√©nario 4** :
        - Neural CDE : M√©moire temporelle
        - Correction avec historique
        
        **Quand choisir Sc√©nario 4** : Besoin de d√©pendances temporelles
        
        ### Sc√©nario 4 vs Sc√©nario 5 (PatchTST)
        
        **Sc√©nario 4** :
        - Correction du mod√®le physique
        - M√©moire courte (5-10 pas)
        
        **Sc√©nario 5** :
        - Enrichissement observation
        - M√©moire longue (30+ pas)
        
        **Diff√©rence** : Sc√©nario 4 corrige physique, Sc√©nario 5 enrichit observation
        
        ### Sc√©nario 4 vs Sc√©narios 1-2
        
        **Sc√©nario 4** :
        - Correction r√©siduelle avec m√©moire
        - Plus complexe
        
        **Sc√©narios 1-2** :
        - Pas de correction
        - Plus simple
        
        **Quand choisir Sc√©nario 4** : Biais temporels du mod√®le physique
        """)
    
    # ========================================================================
    # ONGLET DOCUMENTATION 6 : PATCHTST
    # ========================================================================

def render_doc_patchtst():
    """
    Affiche le contenu de l'onglet de documentation : PatchTST.
    """
    st.markdown('<h2 class="section-header">üîÆ PatchTST : Extracteur de features temporelles pour l\'apprentissage par renforcement</h2>', unsafe_allow_html=True)
    
    st.markdown("""
    ### ‚ùì Qu'est-ce que PatchTST ?
    
    **PatchTST** (Patch-based Time Series Transformer) est un mod√®le Transformer sp√©cialement con√ßu pour la pr√©vision de s√©ries temporelles. 
    Contrairement aux approches traditionnelles qui traitent chaque point de temps individuellement, PatchTST segmente la s√©rie temporelle en **"patches"** 
    (segments temporels) pour capturer des motifs √† diff√©rentes √©chelles temporelles.
    
    Dans notre contexte d'irrigation intelligente, PatchTST est utilis√© comme **extracteur de features temporelles** pour enrichir l'observation de l'agent RL, 
    lui permettant de mieux comprendre les dynamiques temporelles longues (tendances, saisonnalit√©, motifs r√©currents).
    """)
    
    with st.expander("üî¨ Principe g√©n√©ral de PatchTST", expanded=False):
        st.markdown("""
        PatchTST transforme une s√©rie temporelle en une s√©quence de patches qui sont ensuite trait√©s par un Transformer :
        
        **1. Patchification** :
        - La s√©rie temporelle $X = [x_1, x_2, \\ldots, x_T]$ est divis√©e en patches de longueur $P$ avec un stride $S$
        - Chaque patch $\\mathbf{p}_i = [x_{iS}, x_{iS+1}, \\ldots, x_{iS+P-1}]$ capture un segment temporel local
        
        **2. Embedding des patches** :
        - Chaque patch est projet√© dans un espace de dimension $d_{\\text{model}}$ via une couche lin√©aire :
        $$
        \\mathbf{e}_i = \\text{Linear}(\\mathbf{p}_i) \\in \\mathbb{R}^{d_{\\text{model}}}
        $$
        
        **3. Positional Encoding** :
        - Un encodage positionnel est ajout√© pour pr√©server l'ordre temporel :
        $$
        \\mathbf{e}_i^{\\text{pos}} = \\mathbf{e}_i + \\text{PE}(i)
        $$
        
        **4. Transformer Encoder** :
        - Les patches encod√©s passent √† travers un Transformer Encoder (attention multi-t√™tes) :
        $$
        \\mathbf{h}_i = \\text{TransformerEncoder}(\\mathbf{e}_i^{\\text{pos}})
        $$
        
        **5. Extraction de features** :
        - Les repr√©sentations des patches sont concat√©n√©es et projet√©es pour produire des features finales :
        $$
        \\mathbf{f} = \\text{Linear}([\\mathbf{h}_1, \\mathbf{h}_2, \\ldots, \\mathbf{h}_N]) \\in \\mathbb{R}^{d_{\\text{features}}}
        $$
        
        **Avantages de PatchTST** :
        - **Efficacit√©** : R√©duit la complexit√© computationnelle en traitant des patches plut√¥t que des points individuels
        - **Captures multi-√©chelles** : Les patches de diff√©rentes tailles capturent des motifs √† diff√©rentes √©chelles temporelles
        - **Longue port√©e** : Le m√©canisme d'attention permet de capturer des d√©pendances √† long terme
        - **Robustesse** : Moins sensible au bruit gr√¢ce √† l'agr√©gation dans les patches
        """)
    
    with st.expander("üéØ Application dans notre projet : Enrichissement de l'observation RL", expanded=False):
        st.markdown("""
        Dans notre projet d'irrigation intelligente, PatchTST est utilis√© comme **extracteur de features temporelles** pour enrichir l'observation de l'agent RL :
        
        **Probl√®me** :
        - L'observation standard de l'agent RL est $\\mathbf{o}_t = [\\psi_t, S_t, R_t, ET0_t]$ (4 dimensions)
        - Cette observation ne contient que l'√©tat actuel, sans information sur les tendances ou les motifs temporels pass√©s
        - L'agent a du mal √† anticiper les effets √† long terme de ses d√©cisions d'irrigation
        
        **Solution avec PatchTST** :
        - PatchTST analyse un historique de $L$ pas de temps : $\\mathbf{X}_{t-L:t} = [\\mathbf{o}_{t-L}, \\mathbf{o}_{t-L+1}, \\ldots, \\mathbf{o}_t]$
        - Il extrait des features temporelles $\\mathbf{f}_t \\in \\mathbb{R}^{d_{\\text{features}}}$ qui capturent :
          - **Tendances** : √âvolution √† long terme de la tension, de l'humidit√© du sol
          - **Saisonnalit√©** : Patterns r√©currents li√©s aux conditions m√©t√©orologiques
          - **Motifs** : Relations complexes entre irrigation, pluie, √©vapotranspiration
        - L'observation enrichie devient : $\\mathbf{o}_t^{\\text{enrichi}} = [\\psi_t, S_t, R_t, ET0_t, \\mathbf{f}_t]$
        
        **Avantages** :
        - ‚úÖ L'agent RL re√ßoit des informations sur les dynamiques temporelles longues
        - ‚úÖ Meilleure anticipation des effets cumulatifs de l'irrigation
        - ‚úÖ Compr√©hension des tendances et des patterns saisonniers
        - ‚úÖ D√©cisions plus inform√©es bas√©es sur l'historique complet
        
        ### üìã Param√®tres de configuration du pr√©-entra√Ænement
        
        **Nombre de trajectoires** :
        - **Signification** : Nombre de simulations ind√©pendantes utilis√©es pour g√©n√©rer les s√©quences d'entra√Ænement
        - **Valeur usuelle** : 32 trajectoires (par d√©faut)
        - **Impact** : Plus de trajectoires = plus de diversit√© dans les patterns temporels appris
        - **Tuning** : Augmenter (50-100) pour capturer plus de variabilit√© m√©t√©orologique, r√©duire (10-20) pour acc√©l√©rer
        - **Note** : Chaque trajectoire g√©n√®re de nombreuses s√©quences (une par pas de temps avec historique)
        
        **Nombre d'epochs** :
        - **Signification** : Nombre de passages complets sur l'ensemble des s√©quences d'entra√Ænement
        - **Valeur usuelle** : 10 epochs (par d√©faut)
        - **Impact** : Plus d'epochs = meilleur apprentissage des patterns temporels complexes
        - **Tuning** : Augmenter (20-50) si la reconstruction/pr√©diction continue √† s'am√©liorer, r√©duire si surapprentissage
        - **Note** : Les Transformers peuvent n√©cessiter plus d'epochs pour converger que les MLP simples
        
        **Taille des batches** :
        - **Signification** : Nombre de s√©quences trait√©es simultan√©ment lors de chaque mise √† jour
        - **Valeur usuelle** : 64 (par d√©faut, plus petit que Neural ODE/CDE)
        - **Impact** : Batch plus grand = gradients plus stables mais plus de m√©moire (surtout avec s√©quences longues)
        - **Tuning** : R√©duire (16-32) si m√©moire limit√©e ou s√©quences tr√®s longues, augmenter (128+) si disponible
        - **Note** : La m√©moire requise augmente quadratiquement avec la longueur de s√©quence (attention)
        
        **Taux d'apprentissage (Learning Rate)** :
        - **Signification** : Vitesse d'ajustement des param√®tres du Transformer
        - **Valeur usuelle** : $10^{-3}$ (0.001) (par d√©faut)
        - **Impact** : LR trop √©lev√© = instabilit√© dans l'apprentissage du Transformer
        - **Tuning** : R√©duire (10^{-4} - 10^{-5}) si la loss oscille, augmenter (10^{-2}) si apprentissage trop lent
        - **Note** : Les Transformers b√©n√©ficient souvent d'un warmup du LR au d√©but de l'entra√Ænement
        
        **Longueur de s√©quence** :
        - **Signification** : Nombre de pas de temps dans l'historique analys√© par PatchTST
        - **Valeur usuelle** : 30 pas de temps (par d√©faut, ~1 mois)
        - **Impact** : S√©quence plus longue = capture de patterns √† plus long terme mais plus de complexit√©
        - **Tuning** : Augmenter (40-60) pour patterns saisonniers longs, r√©duire (10-20) pour tendances courtes
        - **Note** : Doit √™tre adapt√© √† l'horizon de planification souhait√© (anticipation sur plusieurs semaines)
        
        **Dimension des features** :
        - **Signification** : Taille du vecteur de features temporelles extrait par PatchTST
        - **Valeur usuelle** : 16 dimensions (par d√©faut)
        - **Impact** : Dimension plus grande = plus d'information mais observation plus grande pour l'agent RL
        - **Tuning** : Augmenter (24-32) pour patterns complexes, r√©duire (8-12) pour simplicit√©
        - **Note** : Doit √™tre √©quilibr√© avec la capacit√© de l'agent RL √† utiliser ces features
        """)
    
    with st.expander("üìê Architecture du mod√®le PatchTST dans le projet", expanded=False):
        st.markdown("""
        ### Architecture du PatchTST Feature Extractor
        
        Le mod√®le PatchTST utilis√© dans ce projet est un **extracteur de features** qui prend en entr√©e une s√©quence d'observations et produit des features temporelles :
        
        **Entr√©e** :
        - S√©quence d'observations : $\\mathbf{X} \\in \\mathbb{R}^{L \\times 4}$ o√π $L$ est la longueur de la s√©quence
        - Chaque observation : $[\\psi, I, R, ET0]$
        
        **Patchification** :
        - Patch length : $P = 5$ (chaque patch couvre 5 pas de temps)
        - Stride : $S = 1$ (patches se chevauchent)
        - Nombre de patches : $N = \\lfloor (L - P) / S \\rfloor + 1$
        
        **Embedding** :
        - Chaque patch $\\mathbf{p}_i \\in \\mathbb{R}^{P \\times 4}$ est aplati en $\\mathbb{R}^{P \\times 4}$
        - Projection lin√©aire : $\\mathbf{e}_i = \\text{Linear}(\\text{flatten}(\\mathbf{p}_i)) \\in \\mathbb{R}^{d_{\\text{model}}}$
        - $d_{\\text{model}} = 64$ (dimension du mod√®le)
        
        **Transformer Encoder** :
        - Nombre de couches : $n_{\\text{layers}} = 2$
        - Nombre de t√™tes d'attention : $n_{\\text{heads}} = 4$
        - Dimension du feed-forward : $d_{\\text{ff}} = 4 \\times d_{\\text{model}} = 256$
        - Positional encoding : Ajout√© pour pr√©server l'ordre temporel
        
        **Extraction de features** :
        - Les repr√©sentations des patches sont concat√©n√©es : $[\\mathbf{h}_1, \\mathbf{h}_2, \\ldots, \\mathbf{h}_N]$
        - Projection finale : $\\mathbf{f} = \\text{Linear}(\\text{concat}(\\mathbf{h}_1, \\ldots, \\mathbf{h}_N)) \\in \\mathbb{R}^{d_{\\text{features}}}$
        - $d_{\\text{features}} = 16$ (dimension des features extraites)
        
        **Sortie** :
        - Features temporelles : $\\mathbf{f} \\in \\mathbb{R}^{16}$
        - Ces features sont concat√©n√©es √† l'observation standard pour former l'observation enrichie
        """)
    
    with st.expander("üéì Processus d'entra√Ænement de PatchTST", expanded=False):
        st.markdown("""
        ### Pr√©-entra√Ænement de PatchTST
        
        PatchTST est pr√©-entra√Æn√© sur des donn√©es simul√©es avant d'√™tre utilis√© comme extracteur de features pour l'agent RL :
        
        **1. G√©n√©ration de donn√©es** :
        - Simulation de $N_{\\text{traj}}$ trajectoires avec le mod√®le physique
        - Pour chaque trajectoire, extraction de s√©quences de longueur $L$ : $\\mathbf{X}_{t-L:t}$
        - Les s√©quences capturent diff√©rentes conditions m√©t√©orologiques et strat√©gies d'irrigation
        
        **2. T√¢che de pr√©-entra√Ænement** :
        - **Auto-supervis√© (reconstruction)** : PatchTST apprend √† reconstruire la s√©quence d'entr√©e √† partir des features extraites
        - **Supervis√© (pr√©diction)** : PatchTST apprend √† pr√©dire des statistiques de la s√©quence (tendance, variance, moyenne)
        
        **3. Fonction de perte** :
        - Pour la reconstruction : $\\mathcal{L} = \\|\\mathbf{X}_{\\text{recon}} - \\mathbf{X}_{\\text{original}}\\|_2^2$
        - Pour la pr√©diction : $\\mathcal{L} = \\|\\mathbf{y}_{\\text{pred}} - \\mathbf{y}_{\\text{target}}\\|_2^2$
        
        **4. Optimisation** :
        - Optimiseur : Adam
        - Learning rate : $10^{-3}$
        - Nombre d'epochs : 10-20
        - Batch size : 64
        
        **5. Utilisation dans l'environnement RL** :
        - Apr√®s pr√©-entra√Ænement, PatchTST est fig√© (frozen)
        - √Ä chaque pas de temps, l'historique des observations est pass√© √† PatchTST
        - Les features extraites sont ajout√©es √† l'observation de l'agent RL
        """)
    
    with st.expander("üîÑ Int√©gration dans l'environnement RL", expanded=False):
        st.markdown("""
        ### Wrapper d'environnement PatchTST
        
        Un wrapper `PatchTSTEnvWrapper` enrichit l'observation de l'environnement RL :
        
        **1. Historique des observations** :
        - Le wrapper maintient un historique des $L$ derni√®res observations : $\\{\\mathbf{o}_{t-L}, \\ldots, \\mathbf{o}_t\\}$
        - √Ä chaque pas de temps, la nouvelle observation est ajout√©e √† l'historique
        
        **2. Extraction de features** :
        - L'historique est pass√© √† PatchTST : $\\mathbf{f}_t = \\text{PatchTST}([\\mathbf{o}_{t-L}, \\ldots, \\mathbf{o}_t])$
        - PatchTST est en mode √©valuation (pas de gradient)
        
        **3. Observation enrichie** :
        - L'observation originale : $\\mathbf{o}_t = [\\psi_t, S_t, R_t, ET0_t] \\in \\mathbb{R}^4$
        - L'observation enrichie : $\\mathbf{o}_t^{\\text{enrichi}} = [\\psi_t, S_t, R_t, ET0_t, \\mathbf{f}_t] \\in \\mathbb{R}^{4 + d_{\\text{features}}}$
        - L'espace d'observation est mis √† jour pour refl√©ter la nouvelle dimension
        
        **4. Entra√Ænement PPO** :
        - L'agent PPO re√ßoit l'observation enrichie
        - La politique apprend √† utiliser les features temporelles pour prendre de meilleures d√©cisions
        - Les features permettent √† l'agent de comprendre les tendances et d'anticiper les effets futurs
        """)
    
    with st.expander("üìä Variables et notations", expanded=False):
        st.markdown("""
        ### Variables et notations utilis√©es
        
        **S√©rie temporelle** :
        - $X = [x_1, x_2, \\ldots, x_T]$ : S√©rie temporelle de longueur $T$
        - $L$ : Longueur de la s√©quence d'historique utilis√©e par PatchTST
        - $P$ : Longueur d'un patch (nombre de pas de temps par patch)
        - $S$ : Stride (d√©calage entre patches cons√©cutifs)
        - $N = \\lfloor (L - P) / S \\rfloor + 1$ : Nombre de patches
        
        **Architecture** :
        - $d_{\\text{model}}$ : Dimension du mod√®le Transformer (64)
        - $d_{\\text{features}}$ : Dimension des features extraites (16)
        - $n_{\\text{layers}}$ : Nombre de couches Transformer (2)
        - $n_{\\text{heads}}$ : Nombre de t√™tes d'attention (4)
        - $d_{\\text{ff}}$ : Dimension du feed-forward network (256)
        
        **Observations** :
        - $\\mathbf{o}_t = [\\psi_t, S_t, R_t, ET0_t]$ : Observation standard au temps $t$
        - $\\mathbf{X}_{t-L:t} = [\\mathbf{o}_{t-L}, \\ldots, \\mathbf{o}_t]$ : Historique de $L$ observations
        - $\\mathbf{f}_t$ : Features temporelles extraites par PatchTST
        - $\\mathbf{o}_t^{\\text{enrichi}} = [\\mathbf{o}_t, \\mathbf{f}_t]$ : Observation enrichie
        
        **Entra√Ænement** :
        - $N_{\\text{traj}}$ : Nombre de trajectoires simul√©es pour le pr√©-entra√Ænement
        - $\\mathcal{L}$ : Fonction de perte (MSE pour reconstruction ou pr√©diction)
        - $\\theta$ : Param√®tres du mod√®le PatchTST
        """)
    
    with st.expander("üî¨ Relation avec les Physics-Informed Neural Networks (PINN)", expanded=False):
        st.markdown("""
        ### Neural ODE comme mod√®le Physics-Informed
        
        Le Neural ODE utilis√© dans ce projet peut √™tre consid√©r√© comme un **mod√®le hybride physics-informed** :
        
        **D√©finition des PINN** :
        Les Physics-Informed Neural Networks (PINN) sont des r√©seaux de neurones qui int√®grent explicitement les lois de la physique dans leur architecture ou leur fonction de perte. 
        Ils combinent g√©n√©ralement :
        - Des √©quations diff√©rentielles comme contraintes
        - Des termes de r√©gularisation bas√©s sur la physique
        - Une combinaison de donn√©es et de connaissances physiques
        
        **Notre approche hybride** :
        - **Mod√®le physique** : Fournit la structure et les contraintes physiques (bilan hydrique FAO)
        - **Neural ODE** : Apprend une correction r√©siduelle qui respecte implicitement la structure physique
        
        **Caract√©ristiques physics-informed** :
        - ‚úÖ **Int√©gration explicite de la physique** : Le mod√®le physique FAO est int√©gr√© directement dans l'architecture
        - ‚úÖ **Respect des contraintes physiques** : La correction $\\Delta \\psi$ est appliqu√©e de mani√®re coh√©rente avec le mod√®le physique
        - ‚úÖ **Apprentissage guid√© par la physique** : Le Neural ODE apprend √† partir de donn√©es mais dans le contexte d'un mod√®le physique
        - ‚úÖ **Interpr√©tabilit√©** : La s√©paration entre physique et correction permet de comprendre les √©carts
        
        **Diff√©rence avec les PINN classiques** :
        - **PINN classiques** : Int√®grent les √©quations diff√©rentielles directement dans la fonction de perte (ex: $\\mathcal{L} = \\mathcal{L}_{\\text{data}} + \\lambda \\mathcal{L}_{\\text{physics}}$)
        - **Notre approche** : Utilise un mod√®le physique explicite comme base et apprend une correction r√©siduelle
        
        **Conclusion** :
        Notre mod√®le Neural ODE est un **mod√®le hybride physics-informed** qui combine le meilleur des deux mondes : 
        la robustesse et l'interpr√©tabilit√© du mod√®le physique avec la flexibilit√© d'apprentissage des r√©seaux de neurones.
        """)
    
    with st.expander("‚öñÔ∏è Avantages et limitations", expanded=False):
        st.markdown("""
        ### Avantages de PatchTST comme extracteur de features
        
        **‚úÖ Captures temporelles longues** :
        - PatchTST peut capturer des d√©pendances √† long terme gr√¢ce au m√©canisme d'attention
        - Les patches permettent de capturer des motifs √† diff√©rentes √©chelles temporelles
        
        **‚úÖ Efficacit√© computationnelle** :
        - Le traitement par patches r√©duit la complexit√© par rapport √† un traitement point par point
        - Le pr√©-entra√Ænement permet de r√©utiliser les features sans recalculer √† chaque pas
        
        **‚úÖ Robustesse** :
        - L'agr√©gation dans les patches r√©duit la sensibilit√© au bruit
        - Les features extraites sont plus stables que les observations brutes
        
        **‚úÖ Flexibilit√©** :
        - Peut √™tre pr√©-entra√Æn√© sur diff√©rentes t√¢ches (reconstruction, pr√©diction)
        - Les features peuvent √™tre utilis√©es pour diff√©rents types d'agents RL
        
        ### Limitations
        
        **‚ö†Ô∏è Complexit√©** :
        - Ajoute une couche de complexit√© √† l'architecture
        - N√©cessite un pr√©-entra√Ænement suppl√©mentaire
        
        **‚ö†Ô∏è Hyperparam√®tres** :
        - Sensible aux hyperparam√®tres (longueur de patch, stride, dimension des features)
        - N√©cessite un tuning pour chaque application
        
        **‚ö†Ô∏è M√©moire** :
        - N√©cessite de maintenir un historique des observations
        - Augmente l√©g√®rement la consommation m√©moire
        
        **‚ö†Ô∏è Interpr√©tabilit√©** :
        - Les features extraites sont moins interpr√©tables que les observations brutes
        - Difficile de comprendre exactement ce que chaque feature repr√©sente
        """)
    
    with st.expander("üîß Param√®tres recommand√©s et tuning", expanded=False):
        st.markdown("""
        ### Hyperparam√®tres du pr√©-entra√Ænement PatchTST
        
        **Nombre de trajectoires** : 32-50 (recommand√©)
        - Plus de trajectoires = plus de diversit√© dans les patterns
        - Tuning : Augmenter (50-100) pour variabilit√© m√©t√©o, r√©duire (10-20) pour acc√©l√©rer
        
        **Nombre d'epochs** : 10-20 (recommand√©)
        - Plus d'epochs = meilleur apprentissage
        - Tuning : Augmenter (20-50) si loss continue √† d√©cro√Ætre, r√©duire si surapprentissage
        
        **Taille des batches** : 64 (recommand√©)
        - Plus grand = gradients plus stables mais plus de m√©moire
        - Tuning : R√©duire (16-32) si m√©moire limit√©e, augmenter (128+) si disponible
        
        **Taux d'apprentissage** : $10^{-3}$ (recommand√©)
        - Trop √©lev√© = instabilit√©
        - Tuning : R√©duire ($10^{-4}$-$10^{-5}$) si loss oscille, augmenter ($10^{-2}$) si trop lent
        
        **Longueur de s√©quence** ($L$) : 30 (recommand√©, ~1 mois)
        - Plus long = capture patterns plus longs mais plus complexe
        - Tuning : Augmenter (40-60) pour saisonnalit√©, r√©duire (10-20) pour tendances courtes
        
        **Dimension des features** ($d_{\\text{features}}$) : 16 (recommand√©)
        - Plus grande = plus d'information mais observation plus grande
        - Tuning : Augmenter (24-32) pour patterns complexes, r√©duire (8-12) pour simplicit√©
        
        **Type de t√¢che** :
        - **Auto-supervis√© (reconstruction)** : Recommand√© pour d√©but
        - **Supervis√© (pr√©diction)** : Si objectif sp√©cifique
        
        ### Hyperparam√®tres PPO
        
        - Identiques au Sc√©nario 2
        - Learning rate : $3 \\times 10^{-4}$
        - Gamma : 0.99
        - Observation space : 4 + $d_{\\text{features}}$ dimensions
        
        ### Strat√©gie de tuning
        
        **1. Pr√©-entra√Ænement PatchTST** :
        - Commencer avec valeurs par d√©faut
        - Observer convergence de la loss
        - Ajuster selon r√©sultats
        
        **2. Entra√Ænement PPO** :
        - Comme Sc√©nario 2
        - Observer si features am√©liorent performance
        - Comparer avec Sc√©nario 2
        """)
    
    with st.expander("üß≠ Quand utiliser le Sc√©nario 5 ?", expanded=False):
        st.markdown("""
        ### ‚úÖ Choisir le Sc√©nario 5 si :
        
        - **Besoin de m√©moire temporelle longue** :
          - Comprendre tendances et saisonnalit√©
          - Capturer patterns √† long terme (30+ jours)
          - Sc√©nario 2 insuffisant pour contexte temporel
        
        - **Enrichissement d'observation** :
          - Observation standard (4D) insuffisante
          - Besoin de features temporelles avanc√©es
          - Am√©liorer compr√©hension de l'agent RL
        
        - **Pas de correction physique n√©cessaire** :
          - Mod√®le physique fiable
          - Pas de biais √† corriger
          - Focus sur am√©lioration observation
        
        - **Donn√©es de simulation disponibles** :
          - Possibilit√© de g√©n√©rer trajectoires pour pr√©-entra√Ænement
          - Pas besoin de donn√©es r√©elles (pr√©-entra√Ænement auto-supervis√©)
          - Qualit√© de simulation acceptable
        
        - **Performance am√©lior√©e recherch√©e** :
          - Sc√©nario 2 ne suffit pas
          - Besoin de meilleure compr√©hension temporelle
          - Ressources computationnelles disponibles
        
        ### ‚ùå Ne pas choisir le Sc√©nario 5 si :
        
        - **Pas de d√©pendances temporelles longues** :
          - Effets locaux uniquement
          - Pas besoin de tendances/saisonnalit√©
          - Sc√©nario 2 suffit
        
        - **Biais du mod√®le physique** :
          - Mod√®le physique a des biais connus
          - N√©cessit√© de corriger les pr√©dictions
          - ‚Üí Pr√©f√©rer Sc√©narios 3-4
        
        - **Simplicit√© recherch√©e** :
          - Approche simple suffit
          - Pas de ressources pour pr√©-entra√Ænement
          - ‚Üí Pr√©f√©rer Sc√©narios 1-2
        
        - **Besoin de planification** :
          - Besoin de planification explicite
          - Rollouts d'imagination n√©cessaires
          - ‚Üí Pr√©f√©rer Sc√©nario 6
        """)
    
    with st.expander("üõ†Ô∏è Conseils pratiques", expanded=False):
        st.markdown("""
        ### Workflow recommand√©
        
        **1. Pr√©-entra√Ænement PatchTST** :
        - G√©n√©rer trajectoires avec mod√®le physique
        - Extraire s√©quences de longueur $L$
        - Pr√©-entra√Æner PatchTST (reconstruction ou pr√©diction)
        - V√©rifier convergence (loss d√©cro√Æt)
        
        **2. Int√©gration dans environnement** :
        - Cr√©er wrapper qui enrichit observation
        - Maintenir historique de $L$ observations
        - Extraire features √† chaque pas
        
        **3. Entra√Ænement PPO** :
        - Comme Sc√©nario 2
        - Observer si features am√©liorent performance
        - Comparer avec Sc√©nario 2
        
        **4. Analyse** :
        - Analyser quelles features sont utilis√©es
        - V√©rifier am√©lioration vs Sc√©nario 2
        - Ajuster hyperparam√®tres si n√©cessaire
        
        ### Troubleshooting
        
        **Probl√®me : Features non informatives**
        - **Sympt√¥me** : Pas d'am√©lioration vs Sc√©nario 2
        - **Solutions** :
          - Augmenter longueur de s√©quence $L$
          - Augmenter dimension des features
          - V√©rifier qualit√© du pr√©-entra√Ænement
        
        **Probl√®me : Pr√©-entra√Ænement ne converge pas**
        - **Sympt√¥me** : Loss ne d√©cro√Æt pas
        - **Solutions** :
          - R√©duire learning rate
          - Augmenter nombre de trajectoires
          - V√©rifier qualit√© des donn√©es
        
        **Probl√®me : M√©moire insuffisante**
        - **Sympt√¥me** : Erreur m√©moire lors pr√©-entra√Ænement
        - **Solutions** :
          - R√©duire batch size
          - R√©duire longueur de s√©quence $L$
          - R√©duire dimension des features
        """)
    
    with st.expander("üî¨ Relation avec les Physics-Informed Neural Networks (PINN)", expanded=False):
        st.markdown("""
        ### PatchTST : Mod√®le data-driven, pas un PINN
        
        **D√©finition des PINN** :
        Les Physics-Informed Neural Networks (PINN) sont des r√©seaux de neurones qui int√®grent explicitement les lois de la physique dans leur architecture ou leur fonction de perte.
        
        **PatchTST dans notre projet** :
        - **R√¥le** : Extracteur de features temporelles purement data-driven
        - **Apprentissage** : Bas√© uniquement sur des patterns temporels dans les donn√©es, sans int√©gration explicite de connaissances physiques
        - **Objectif** : Capturer des tendances, saisonnalit√©s et motifs temporels pour enrichir l'observation de l'agent RL
        
        **Pourquoi PatchTST n'est pas un PINN** :
        - ‚ùå **Pas d'int√©gration de physique** : PatchTST n'int√®gre pas explicitement les √©quations physiques (bilan hydrique, courbe de r√©tention, etc.)
        - ‚ùå **Apprentissage purement data-driven** : Le mod√®le apprend uniquement √† partir de patterns dans les donn√©es, sans contraintes physiques
        - ‚ùå **Pas de r√©gularisation physique** : La fonction de perte ne contient pas de termes bas√©s sur les lois de la physique
        
        **Relation indirecte avec la physique** :
        - ‚úÖ **Donn√©es g√©n√©r√©es par un mod√®le physique** : PatchTST est pr√©-entra√Æn√© sur des donn√©es simul√©es par le mod√®le physique FAO
        - ‚úÖ **Features informatives** : Les features extraites capturent indirectement des patterns li√©s √† la physique (tendances de tension, cycles d'irrigation, etc.)
        - ‚úÖ **Compl√©mentarit√©** : PatchTST enrichit l'observation de l'agent RL qui √©volue dans un environnement bas√© sur un mod√®le physique
        
        **Conclusion** :
        PatchTST n'est **pas un PINN** mais un mod√®le **data-driven** qui compl√®te l'approche physics-informed en fournissant des features temporelles avanc√©es √† l'agent RL. 
        L'approche globale du projet combine :
        - **Mod√®les physics-informed** (Neural ODE, Neural CDE) : Pour am√©liorer la pr√©diction du mod√®le physique
        - **Mod√®les data-driven** (PatchTST) : Pour enrichir la compr√©hension temporelle de l'agent RL
        """)
    
    with st.expander("üîó Comparaison avec Neural ODE et Neural CDE", expanded=False):
        st.markdown("""
        ### Diff√©rences avec Neural ODE et Neural CDE
        
        **Neural ODE** :
        - **R√¥le** : Correction r√©siduelle du mod√®le physique
        - **Entr√©e** : √âtat actuel $[\\psi_t, I_t, R_t, ET0_t]$
        - **Sortie** : Correction $\\Delta \\psi_t$ ajout√©e √† la pr√©diction physique
        - **M√©moire** : Aucune (d√©pend uniquement de l'√©tat actuel)
        
        **Neural CDE** :
        - **R√¥le** : Correction r√©siduelle avec m√©moire temporelle
        - **Entr√©e** : S√©quence d'√©tats pass√©s $[\\psi_{t-k}, \\ldots, \\psi_t]$
        - **Sortie** : Correction $\\Delta \\psi_t$ bas√©e sur l'historique
        - **M√©moire** : Court terme (5-10 pas de temps)
        
        **PatchTST** :
        - **R√¥le** : Extracteur de features temporelles pour l'agent RL
        - **Entr√©e** : Historique d'observations $[\\mathbf{o}_{t-L}, \\ldots, \\mathbf{o}_t]$
        - **Sortie** : Features temporelles $\\mathbf{f}_t$ enrichissant l'observation
        - **M√©moire** : Long terme (30+ pas de temps)
        - **Utilisation** : Enrichit l'observation de l'agent, ne modifie pas le mod√®le physique
        
        **Compl√©mentarit√©** :
        - Neural ODE/CDE am√©liorent la **pr√©diction du mod√®le physique**
        - PatchTST am√©liore la **compr√©hension de l'agent RL** des dynamiques temporelles
        - Les deux approches peuvent √™tre combin√©es pour un syst√®me encore plus performant
        """)
    
    # ========================================================================
    # ONGLET DOCUMENTATION 7 : SC√âNARIOS
    # ========================================================================


def render_doc_scenario6_world_model():
    """
    Affiche le contenu de l'onglet de documentation : Sc√©nario 6 (World Model).
    """
    st.markdown('<h2 class="section-header">üåç Sc√©nario 6 ‚Äî Model-Based RL avec World Model</h2>', unsafe_allow_html=True)
    
    st.markdown("""
    ### ‚ùì Qu'est-ce qu'un World Model ?
    
    Un **World Model** est un mod√®le interne de l'environnement appris par l'agent RL. Contrairement aux approches model-free 
    (comme PPO dans les Sc√©narios 2-5) qui apprennent directement une politique √† partir des interactions avec l'environnement, 
    un model-based RL utilise un mod√®le du monde pour **planifier** et **simuler** les cons√©quences des actions avant de les ex√©cuter.
    
    **Objectif principal** : Tirer parti d'un mod√®le du monde pour planifier et am√©liorer l'efficacit√© sample tout en gardant 
    les contraintes physiques via le mod√®le FAO. Cette approche permet une meilleure **sample efficiency** et une **planification √† long terme**.
    
    **Pourquoi utiliser un World Model ?**
    - ‚úÖ **Sample efficiency** : R√©duit le nombre d'interactions r√©elles avec l'environnement n√©cessaires pour apprendre
    - ‚úÖ **Planification** : Permet d'anticiper les effets des actions sur plusieurs pas de temps
    - ‚úÖ **Rollouts d'imagination** : Simulation de trajectoires futures pour am√©liorer l'apprentissage
    - ‚úÖ **Robustesse** : Compr√©hension plus profonde de la dynamique de l'environnement
    """)
    
    with st.expander("üî¨ Principe g√©n√©ral des World Models", expanded=False):
        st.markdown("""
        ### Architecture d'un World Model
        
        Un World Model typique est compos√© de trois composants principaux :
        
        **1. Encodeur (Representation Model)** :
        - **R√¥le** : Comprime les observations brutes en un √©tat latent compact
        - **Entr√©e** : Observations $\\mathbf{o}_t = [\\psi_t, S_t, R_t, ET0_t] \\in \\mathbb{R}^4$
        - **Sortie** : √âtat latent $\\mathbf{z}_t \\in \\mathbb{R}^{d_z}$ o√π $d_z$ est la dimension de l'espace latent
        - **Formule** : $\\mathbf{z}_t = \\text{Encoder}(\\mathbf{o}_t)$
        - **Dans notre projet** : PatchTST encode un historique de $L$ observations en un √©tat latent
        
        **2. Mod√®le de Transition (Dynamics Model)** :
        - **R√¥le** : Pr√©dit l'√©volution de l'√©tat latent apr√®s une action
        - **Entr√©e** : √âtat latent actuel $\\mathbf{z}_t$ et action $a_t$
        - **Sortie** : √âtat latent futur $\\hat{\\mathbf{z}}_{t+1}$
        - **Formule** : $\\hat{\\mathbf{z}}_{t+1} = f_{\\text{transition}}(\\mathbf{z}_t, a_t)$
        - **Dans notre projet** : Neural ODE ou Neural CDE pour mod√©liser la dynamique temporelle
        
        **3. D√©codeur (Observation Model, optionnel)** :
        - **R√¥le** : Reconstruit les observations depuis l'√©tat latent (pour rollouts longs)
        - **Entr√©e** : √âtat latent $\\mathbf{z}_t$
        - **Sortie** : Observations reconstruites $\\hat{\\mathbf{o}}_t$
        - **Formule** : $\\hat{\\mathbf{o}}_t = \\text{Decoder}(\\mathbf{z}_t)$
        - **Dans notre projet** : MLP qui reconstruit $[\\psi, S, R, ET0]$ depuis $\\mathbf{z}_t$
        
        ### Rollouts d'Imagination
        
        Le World Model permet de g√©n√©rer des **rollouts d'imagination** : des trajectoires simul√©es dans l'espace latent :
        
        $$
        \\begin{aligned}
        \\mathbf{z}_t &= \\text{Encoder}(\\mathbf{o}_t) \\\\
        \\mathbf{z}_{t+1} &= f_{\\text{transition}}(\\mathbf{z}_t, a_t) \\\\
        \\mathbf{z}_{t+2} &= f_{\\text{transition}}(\\mathbf{z}_{t+1}, a_{t+1}) \\\\
        &\\vdots \\\\
        \\mathbf{z}_{t+H} &= f_{\\text{transition}}(\\mathbf{z}_{t+H-1}, a_{t+H-1})
        \\end{aligned}
        $$
        
        o√π $H$ est l'horizon d'imagination. Ces rollouts permettent √† l'agent de planifier et d'anticiper les cons√©quences de ses actions.
        
        ### Avantages des World Models
        
        - **Sample Efficiency** : Moins d'interactions r√©elles n√©cessaires gr√¢ce √† la simulation interne
        - **Planification** : Capacit√© √† explorer diff√©rentes strat√©gies sans co√ªt r√©el
        - **Compression** : L'espace latent capture l'information essentielle de mani√®re compacte
        - **G√©n√©ralisation** : Compr√©hension plus profonde des dynamiques permet de mieux g√©n√©raliser
        """)
    
    with st.expander("üéØ Application dans notre projet : Architecture du Sc√©nario 6", expanded=False):
        st.markdown("""
        ### Architecture compl√®te du World Model
        
        Notre World Model combine plusieurs composants pour cr√©er un syst√®me de planification performant :
        
        **1. Encodeur PatchTST** :
        - **R√¥le** : Transforme l'historique d'observations en repr√©sentation latente riche
        - **Entr√©e** : Historique $\\mathbf{X}_{t-L:t} = [\\mathbf{o}_{t-L}, \\ldots, \\mathbf{o}_t] \\in \\mathbb{R}^{L \\times 4}$
        - **Sortie** : √âtat latent $\\mathbf{z}_t \\in \\mathbb{R}^{d_z}$ o√π $d_z$ est la dimension des features PatchTST (16 par d√©faut)
        - **R√©utilisation** : Utilise le mod√®le PatchTST pr√©-entra√Æn√© du Sc√©nario 5
        - **Formule** : $\\mathbf{z}_t = \\text{PatchTST}(\\mathbf{X}_{t-L:t})$
        
        **2. Mod√®le de Transition** :
        
        **Phase 1 - Neural ODE** :
        - Mod√©lise la dynamique comme une √©quation diff√©rentielle ordinaire continue
        - **Formule** : $\\frac{d\\mathbf{z}(t)}{dt} = f_{\\theta}(\\mathbf{z}(t), a(t))$
        - Int√©gration num√©rique (Euler ou Runge-Kutta) pour obtenir $\\mathbf{z}_{t+1}$
        - **Avantage** : Simple et efficace pour transitions courtes
        
        **Phase 2 - Neural CDE** :
        - Mod√©lise la dynamique avec m√©moire temporelle via √©quations diff√©rentielles contr√¥l√©es
        - Prend en compte l'historique des √©tats pour mieux pr√©dire
        - **Formule** : $d\\mathbf{z}_t = f_{\\theta}(\\mathbf{z}_t, a_t, \\mathbf{h}_t) dt$
        - **Avantage** : Capture mieux les d√©pendances temporelles longues
        
        **3. D√©codeur (Phase 2 uniquement)** :
        - Reconstruit les observations depuis l'√©tat latent pour rollouts longs
        - **Formule** : $\\hat{\\mathbf{o}}_t = \\text{Decoder}(\\mathbf{z}_t)$
        - Permet de simuler des trajectoires compl√®tes avec observables
        
        **4. Wrapper Physics-Informed (Phase 3)** :
        - Combine les pr√©dictions du World Model avec le mod√®le physique FAO
        - **Formule** : $\\mathbf{o}_{t+1} = \\alpha \\cdot \\mathbf{o}_{t+1}^{\\text{physique}} + (1-\\alpha) \\cdot \\hat{\\mathbf{o}}_{t+1}^{\\text{world model}}$
        - O√π $\\alpha$ est un hyperparam√®tre de blend (0.3-0.6 recommand√©)
        - **Avantage** : Garantit la coh√©rence physique m√™me si le World Model d√©rive
        """)
    
    with st.expander("‚öôÔ∏è Les trois phases du Sc√©nario 6", expanded=False):
        st.markdown("""
        ### Phase 1 : World Model Simple (sans d√©codeur)
        
        **Architecture** :
        - Encodeur : PatchTST pr√©-entra√Æn√©
        - Transition : Neural ODE simple
        - D√©codeur : ‚ùå Aucun (agent travaille directement dans l'espace latent)
        
        **Rollouts d'imagination** :
        - Horizon court : 5-10 pas de temps
        - Trajectoires simul√©es uniquement dans l'espace latent
        - R√©compenses estim√©es directement depuis $\\mathbf{z}_t$ (via une fonction de r√©compense latente)
        
        **Objectif** :
        - Valider le concept avec une impl√©mentation minimale
        - Acc√©l√©rer l'apprentissage PPO avec rollouts courts
        - √âvaluer la faisabilit√© du World Model
        
        **Avantages** :
        - ‚úÖ Simple et rapide √† entra√Æner
        - ‚úÖ Moins de param√®tres √† optimiser
        - ‚úÖ Bon point de d√©part pour validation
        
        **Limites** :
        - ‚ö†Ô∏è Pas de reconstruction des observables
        - ‚ö†Ô∏è Rollouts limit√©s √† horizon court
        - ‚ö†Ô∏è Difficile de v√©rifier la coh√©rence physique
        
        ### Phase 2 : World Model Complet (avec d√©codeur)
        
        **Architecture** :
        - Encodeur : PatchTST pr√©-entra√Æn√©
        - Transition : Neural CDE (m√©moire temporelle am√©lior√©e)
        - D√©codeur : ‚úÖ MLP qui reconstruit les observables
        
        **Rollouts d'imagination** :
        - Horizon long : 20-30 pas de temps
        - Trajectoires compl√®tes avec observables reconstruits
        - Planification √† long terme possible
        
        **Objectif** :
        - Maximiser les b√©n√©fices du model-based RL
        - Permettre une planification √† long terme
        - Capturer des strat√©gies complexes
        
        **Avantages** :
        - ‚úÖ Rollouts longs pour planification
        - ‚úÖ Observables reconstruits permettent v√©rification
        - ‚úÖ Neural CDE capture mieux les d√©pendances temporelles
        
        **Limites** :
        - ‚ö†Ô∏è Plus complexe √† entra√Æner (3 composants)
        - ‚ö†Ô∏è Risque de d√©rive si pr√©-entra√Ænement insuffisant
        - ‚ö†Ô∏è Co√ªt computationnel plus √©lev√©
        
        ### Phase 3 : Hybridation Physics-Informed
        
        **Architecture** :
        - Tous les composants de Phase 2
        - **Plus** : Wrapper qui combine World Model et mod√®le physique FAO
        
        **Principe de blend** :
        - √Ä chaque pas de temps, on combine :
          - Pr√©diction du mod√®le physique : $\\mathbf{o}_{t+1}^{\\text{physique}} = f_{\\text{FAO}}(\\mathbf{o}_t, a_t)$
          - Pr√©diction du World Model : $\\hat{\\mathbf{o}}_{t+1}^{\\text{world model}} = \\text{Decoder}(f_{\\text{CDE}}(\\mathbf{z}_t, a_t))$
        - Pr√©diction finale : $\\mathbf{o}_{t+1} = \\alpha \\cdot \\mathbf{o}_{t+1}^{\\text{physique}} + (1-\\alpha) \\cdot \\hat{\\mathbf{o}}_{t+1}^{\\text{world model}}$
        
        **Objectif** :
        - Garantir la coh√©rence physique
        - Limiter la d√©rive du World Model
        - Combiner efficacit√© du World Model et robustesse du mod√®le physique
        
        **Avantages** :
        - ‚úÖ Robuste physiquement (le mod√®le physique corrige les d√©rives)
        - ‚úÖ Meilleur des deux mondes (efficacit√© + robustesse)
        - ‚úÖ Permet de r√©gler le compromis via $\\alpha$
        
        **Recommandation pour $\\alpha$** :
        - $\\alpha = 0.5$ : √âquilibre entre physique et World Model
        - $\\alpha = 0.6-0.7$ : Privil√©gie la physique (recommand√© si World Model d√©rive)
        - $\\alpha = 0.3-0.4$ : Privil√©gie le World Model (si bien entra√Æn√©)
        """)
    
    with st.expander("üöÄ Processus d'entra√Ænement d√©taill√©", expanded=False):
        st.markdown("""
        ### Pipeline complet d'entra√Ænement
        
        Le Sc√©nario 6 suit un pipeline progressif en plusieurs √©tapes :
        
        **√âtape 0 : Pr√©-entra√Ænement PatchTST (Sc√©nario 5)**
        - **Objectif** : Obtenir un encodeur capable de comprimer l'historique d'observations
        - **Processus** :
          1. G√©n√©ration de $N_{\\text{traj}}$ trajectoires avec le mod√®le physique
          2. Extraction de s√©quences de longueur $L$ pour chaque trajectoire
          3. Pr√©-entra√Ænement de PatchTST sur reconstruction/pr√©diction
          4. Sauvegarde du mod√®le PatchTST comme encodeur
        - **R√©sultat** : Encodeur PatchTST pr√™t avec features de dimension $d_z$
        
        **√âtape 1 : Pr√©-entra√Ænement du Mod√®le de Transition (Phase 1 ou 2)**
        - **Objectif** : Apprendre la dynamique de transition dans l'espace latent
        - **Processus** :
          1. G√©n√©ration de $N_{\\text{traj}}$ nouvelles trajectoires
          2. Encodage avec PatchTST : $\\mathbf{z}_t = \\text{PatchTST}(\\mathbf{X}_{t-L:t})$
          3. Construction de paires $(\\mathbf{z}_t, a_t, \\mathbf{z}_{t+1})$ pour l'entra√Ænement
          4. Minimisation de la perte de pr√©diction :
             $$
             \\mathcal{L}_{\\text{transition}} = \\frac{1}{N} \\sum_{i=1}^{N} \\|f_{\\theta}(\\mathbf{z}_t^{(i)}, a_t^{(i)}) - \\mathbf{z}_{t+1}^{(i)}\\|_2^2
             $$
          5. (Phase 2 uniquement) Pr√©-entra√Ænement du d√©codeur :
             $$
             \\mathcal{L}_{\\text{decodeur}} = \\frac{1}{N} \\sum_{i=1}^{N} \\|\\text{Decoder}(\\mathbf{z}_t^{(i)}) - \\mathbf{o}_t^{(i)}\\|_2^2
             $$
        - **Hyperparam√®tres** :
          - Nombre de trajectoires : 32-50
          - Nombre d'epochs : 20-50
          - Batch size : 64
          - Learning rate : $10^{-3}$
        
        **√âtape 2 : Entra√Ænement PPO avec World Model**
        - **Objectif** : Apprendre une politique optimale en utilisant les rollouts d'imagination
        - **Processus** :
          1. Pour chaque interaction avec l'environnement r√©el :
             - Observation $\\mathbf{o}_t$ ‚Üí Encodage $\\mathbf{z}_t = \\text{Encoder}(\\mathbf{o}_t)$
             - Action $a_t \\sim \\pi_\\theta(\\cdot | \\mathbf{z}_t)$
             - Ex√©cution dans l'environnement r√©el ‚Üí $\\mathbf{o}_{t+1}, r_t$
          2. Pour chaque rollout d'imagination (horizon $H$) :
             $$
             \\begin{aligned}
             \\mathbf{z}_t &= \\text{Encoder}(\\mathbf{o}_t) \\\\
             \\mathbf{z}_{t+1} &= f_{\\text{transition}}(\\mathbf{z}_t, a_t) \\\\
             \\hat{r}_{t+1} &= r_{\\text{latent}}(\\mathbf{z}_{t+1}) \\quad \\text{(Phase 1)} \\\\
             \\text{ou} \\quad \\hat{\\mathbf{o}}_{t+1} &= \\text{Decoder}(\\mathbf{z}_{t+1}), \\quad \\hat{r}_{t+1} = r(\\hat{\\mathbf{o}}_{t+1}) \\quad \\text{(Phase 2)} \\\\
             &\\vdots
             \\end{aligned}
             $$
          3. Utilisation des rollouts pour enrichir l'apprentissage PPO :
             - Estimation de la fonction de valeur avec trajectoires r√©elles + imaginaires
             - Calcul du gradient de la politique avec donn√©es augment√©es
          4. Mise √† jour des param√®tres PPO : $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$
        - **Hyperparam√®tres PPO** :
          - Total timesteps : 10,000-100,000
          - Horizon d'imagination : 5-10 (Phase 1), 20-30 (Phase 2)
          - Learning rate : $3 \\times 10^{-4}$
          - Gamma : 0.99 (discount factor √©lev√© pour planification)
          - GAE lambda : 0.95
        
        **√âtape 3 : Phase 3 - Hybridation (optionnelle)**
        - **Objectif** : Int√©grer le mod√®le physique pour garantir la robustesse
        - **Processus** :
          1. Wrapper de l'environnement qui combine World Model et physique
          2. √Ä chaque transition :
             - World Model pr√©dit : $\\hat{\\mathbf{o}}_{t+1}^{\\text{WM}}$
             - Mod√®le physique pr√©dit : $\\mathbf{o}_{t+1}^{\\text{physique}}$
             - Blend : $\\mathbf{o}_{t+1} = \\alpha \\cdot \\mathbf{o}_{t+1}^{\\text{physique}} + (1-\\alpha) \\cdot \\hat{\\mathbf{o}}_{t+1}^{\\text{WM}}$
          3. Entra√Ænement PPO sur l'environnement hybrid√©
        - **R√©glage de $\\alpha$** : Commencer √† 0.5, ajuster selon stabilit√©
        """)

    with st.expander("üìê Architecture d√©taill√©e des composants", expanded=False):
        st.markdown("""
        ### Encodeur PatchTST
        
        **R√¥le** : Compresse l'historique d'observations en repr√©sentation latente
        
        **Architecture** :
        - PatchTST pr√©-entra√Æn√© (r√©utilis√© du Sc√©nario 5)
        - Entr√©e : Historique $\\mathbf{X}_{t-L:t} \\in \\mathbb{R}^{L \\times 4}$ o√π $L$ = longueur de s√©quence (30 par d√©faut)
        - Sortie : √âtat latent $\\mathbf{z}_t \\in \\mathbb{R}^{d_z}$ o√π $d_z$ = dimension des features (16 par d√©faut)
        
        **Hyperparam√®tres** :
        - Longueur de s√©quence : $L = 30$ (recommand√©)
        - Dimension des features : $d_z = 16$ (recommand√©)
        - Dimension du mod√®le : $d_{\\text{model}} = 64$
        - Nombre de couches : 2
        - Nombre de t√™tes d'attention : 4
        
        ### Mod√®le de Transition - Neural ODE (Phase 1)
        
        **R√¥le** : Mod√©lise la dynamique de transition dans l'espace latent
        
        **Architecture** :
        - R√©seau de neurones $f_\\theta$ qui d√©finit le champ de vecteurs
        - Int√©gration num√©rique (m√©thode d'Euler ou Runge-Kutta) pour r√©soudre l'ODE
        
        **√âquation** :
        $$
        \\frac{d\\mathbf{z}(t)}{dt} = f_\\theta(\\mathbf{z}(t), a(t))
        $$
        
        **Int√©gration** :
        $$
        \\mathbf{z}_{t+1} = \\mathbf{z}_t + \\int_t^{t+1} f_\\theta(\\mathbf{z}(s), a(s)) ds
        $$
        
        **Architecture de $f_\\theta$** :
        - MLP avec 2-3 couches cach√©es
        - Dimension cach√©e : 64-128
        - Activation : ReLU ou Tanh
        
        ### Mod√®le de Transition - Neural CDE (Phase 2)
        
        **R√¥le** : Mod√©lise la dynamique avec m√©moire temporelle
        
        **Architecture** :
        - Neural CDE qui prend en compte l'historique via un chemin contr√¥l√©
        - Permet de capturer des d√©pendances temporelles plus longues que Neural ODE
        
        **√âquation** :
        $$
        d\\mathbf{z}_t = f_\\theta(\\mathbf{z}_t, a_t, \\mathbf{h}_t) dt + g_\\theta(\\mathbf{z}_t, a_t) d\\mathbf{X}_t
        $$
        
        o√π $\\mathbf{h}_t$ est un √©tat de m√©moire et $\\mathbf{X}_t$ un chemin contr√¥l√©
        
        **Avantage** : Meilleure capture des d√©pendances temporelles longues
        
        ### D√©codeur (Phase 2)
        
        **R√¥le** : Reconstruit les observables depuis l'√©tat latent
        
        **Architecture** :
        - MLP avec 2-3 couches
        - Entr√©e : √âtat latent $\\mathbf{z}_t \\in \\mathbb{R}^{d_z}$
        - Sortie : Observables reconstruits $\\hat{\\mathbf{o}}_t = [\\hat{\\psi}_t, \\hat{S}_t, \\hat{R}_t, \\widehat{ET0}_t] \\in \\mathbb{R}^4$
        
        **Formule** :
        $$
        \\hat{\\mathbf{o}}_t = \\text{Decoder}(\\mathbf{z}_t) = \\text{MLP}(\\mathbf{z}_t)
        $$
        
        **Objectif** : Permettre des rollouts longs avec observables v√©rifiables
        """)
    
    with st.expander("üìä Variables et notations", expanded=False):
        st.markdown("""
        ### Variables principales
        
        **Observations et √©tats** :
        - $\\mathbf{o}_t = [\\psi_t, S_t, R_t, ET0_t] \\in \\mathbb{R}^4$ : Observation au temps $t$
        - $\\mathbf{X}_{t-L:t} = [\\mathbf{o}_{t-L}, \\ldots, \\mathbf{o}_t] \\in \\mathbb{R}^{L \\times 4}$ : Historique de $L$ observations
        - $\\mathbf{z}_t \\in \\mathbb{R}^{d_z}$ : √âtat latent au temps $t$
        - $a_t \\in [0, I_{\\max}]$ : Action (irrigation) au temps $t$
        - $r_t$ : R√©compense au temps $t$
        
        **Architecture** :
        - $L$ : Longueur de s√©quence d'historique (30 par d√©faut)
        - $d_z$ : Dimension de l'espace latent (16 par d√©faut)
        - $H$ : Horizon d'imagination (5-30 selon phase)
        - $\\alpha$ : Param√®tre de blend pour Phase 3 (0.3-0.7)
        
        **Fonctions et mod√®les** :
        - $\\text{Encoder}(\\cdot)$ : Encodeur PatchTST
        - $f_{\\text{transition}}(\\cdot)$ : Mod√®le de transition (Neural ODE ou CDE)
        - $\\text{Decoder}(\\cdot)$ : D√©codeur (Phase 2 uniquement)
        - $f_{\\text{FAO}}(\\cdot)$ : Mod√®le physique FAO
        - $\\pi_\\theta(\\cdot | \\mathbf{z}_t)$ : Politique PPO
        
        **Entra√Ænement** :
        - $N_{\\text{traj}}$ : Nombre de trajectoires pour pr√©-entra√Ænement
        - $N_{\\text{epochs}}$ : Nombre d'epochs d'entra√Ænement
        - $\\mathcal{L}_{\\text{transition}}$ : Perte du mod√®le de transition
        - $\\mathcal{L}_{\\text{decodeur}}$ : Perte du d√©codeur
        """)
    
    with st.expander("‚öñÔ∏è Avantages et limitations", expanded=False):
        st.markdown("""
        ### ‚úÖ Avantages du Sc√©nario 6
        
        **1. Planification et anticipation** :
        - ‚úÖ **Rollouts d'imagination** : Permet de simuler les cons√©quences des actions sur plusieurs pas de temps
        - ‚úÖ **Anticipation** : L'agent peut pr√©voir l'effet de ses d√©cisions avant de les ex√©cuter
        - ‚úÖ **Exploration efficace** : Teste diff√©rentes strat√©gies dans le mod√®le interne sans co√ªt r√©el
        
        **2. Efficacit√© sample** :
        - ‚úÖ **Moins d'interactions r√©elles** : R√©duit le nombre d'√©pisodes n√©cessaires pour apprendre
        - ‚úÖ **Apprentissage acc√©l√©r√©** : Les rollouts imaginaires enrichissent l'apprentissage
        - ‚úÖ **R√©utilisation** : Le World Model peut √™tre r√©utilis√© pour diff√©rents objectifs
        
        **3. Robustesse physique (Phase 3)** :
        - ‚úÖ **Coh√©rence garantie** : L'hybridation avec le mod√®le physique limite les d√©rives
        - ‚úÖ **Compromis r√©glable** : Le param√®tre $\\alpha$ permet d'ajuster le blend
        - ‚úÖ **S√©curit√©** : Le mod√®le physique corrige les pr√©dictions aberrantes
        
        **4. M√©moire temporelle** :
        - ‚úÖ **D√©pendances longues** : PatchTST + Neural CDE capturent des patterns √† long terme
        - ‚úÖ **Tendances** : Comprend l'√©volution saisonni√®re et les tendances
        - ‚úÖ **Contexte riche** : L'historique complet est utilis√© pour les d√©cisions
        
        **5. G√©n√©ralisation** :
        - ‚úÖ **Compr√©hension profonde** : Le World Model apprend la dynamique sous-jacente
        - ‚úÖ **Transfert** : Peut s'adapter √† des conditions l√©g√®rement diff√©rentes
        - ‚úÖ **Flexibilit√©** : Peut √™tre utilis√© pour diff√©rents objectifs sans r√©-entra√Ænement complet
        
        ### ‚ö†Ô∏è Limitations et d√©fis
        
        **1. Complexit√©** :
        - ‚ö†Ô∏è **Architecture complexe** : Plusieurs composants (encodeur, transition, d√©codeur) √† entra√Æner
        - ‚ö†Ô∏è **Hyperparam√®tres sensibles** : Nombreux hyperparam√®tres √† r√©gler (horizon, $\\alpha$, dimensions, etc.)
        - ‚ö†Ô∏è **Debugging difficile** : Plus difficile √† d√©boguer qu'un mod√®le simple
        
        **2. Co√ªt computationnel** :
        - ‚ö†Ô∏è **Pr√©-entra√Ænement** : N√©cessite un pr√©-entra√Ænement de plusieurs composants
        - ‚ö†Ô∏è **Rollouts** : G√©n√©ration de rollouts imaginaires ajoute du temps de calcul
        - ‚ö†Ô∏è **M√©moire** : N√©cessite de stocker l'historique et les √©tats latents
        
        **3. D√©pendance aux donn√©es** :
        - ‚ö†Ô∏è **Qualit√© du pr√©-entra√Ænement** : La qualit√© du World Model d√©pend de la qualit√© des trajectoires d'entra√Ænement
        - ‚ö†Ô∏è **Couvre des distributions** : Si le pr√©-entra√Ænement ne couvre pas toutes les situations, le mod√®le peut d√©river
        - ‚ö†Ô∏è **Sim-to-real gap** : √âcart entre donn√©es simul√©es et r√©elles peut affecter les performances
        
        **4. Risque de d√©rive** :
        - ‚ö†Ô∏è **Erreur cumulative** : Les erreurs dans les pr√©dictions peuvent s'accumuler sur les rollouts longs
        - ‚ö†Ô∏è **Instabilit√©** : Sans Phase 3, le mod√®le peut s'√©loigner de la physique si mal entra√Æn√©
        - ‚ö†Ô∏è **Mode collapse** : Le World Model peut apprendre des modes simplifi√©s qui ne capturent pas toute la complexit√©
        
        **5. Tuning d√©licat** :
        - ‚ö†Ô∏è **Horizon d'imagination** : Doit √™tre choisi avec soin (trop court = peu de planification, trop long = instabilit√©)
        - ‚ö†Ô∏è **Param√®tre $\\alpha$** : Le blend physique/World Model doit √™tre ajust√© selon les performances
        - ‚ö†Ô∏è **Dimensions** : Les dimensions de l'espace latent impactent la capacit√© et la complexit√©
        
        **6. Interpr√©tabilit√©** :
        - ‚ö†Ô∏è **Espace latent abstrait** : L'espace latent n'est pas directement interpr√©table
        - ‚ö†Ô∏è **Black box** : Plus difficile de comprendre pourquoi le mod√®le prend certaines d√©cisions
        - ‚ö†Ô∏è **Rollouts v√©rifiables** : N√©cessite le d√©codeur (Phase 2+) pour v√©rifier la coh√©rence
        """)
    
    with st.expander("üîß Param√®tres recommand√©s et tuning", expanded=False):
        st.markdown("""
        ### Hyperparam√®tres par phase
        
        **Phase 1 - World Model Simple** :
        - **Horizon d'imagination** : 5-10 pas de temps
          - Trop court (< 5) : Peu de planification
          - Trop long (> 10) : Instabilit√© (pas de d√©codeur pour v√©rifier)
        - **Longueur de s√©quence PatchTST** : $L = 30$ (coh√©rent avec Sc√©nario 5)
        - **Dimension latente** : $d_z = 16$ (dimension des features PatchTST)
        - **Learning rate transition** : $10^{-3}$ pour Neural ODE
        - **Nombre de trajectoires** : 32-50 pour pr√©-entra√Ænement
        - **Nombre d'epochs** : 20-50 selon convergence
        
        **Phase 2 - World Model Complet** :
        - **Horizon d'imagination** : 20-30 pas de temps
          - Permet planification √† long terme
          - Avec d√©codeur, peut v√©rifier la coh√©rence
        - **Longueur de s√©quence PatchTST** : $L = 30$
        - **Longueur de s√©quence CDE** : $L_{\\text{CDE}} = 8-12$ pour la m√©moire
        - **Dimension latente** : $d_z = 16$
        - **Learning rate transition** : $10^{-3}$ pour Neural CDE
        - **Learning rate d√©codeur** : $10^{-3}$
        - **Nombre de trajectoires** : 50-100 pour pr√©-entra√Ænement robuste
        
        **Phase 3 - Hybridation** :
        - **Param√®tre $\\alpha$** : 0.3-0.6 (recommand√©)
          - $\\alpha = 0.5$ : √âquilibre
          - $\\alpha = 0.6-0.7$ : Privil√©gie physique (si World Model d√©rive)
          - $\\alpha = 0.3-0.4$ : Privil√©gie World Model (si bien entra√Æn√©)
        - **Autres param√®tres** : Identiques √† Phase 2
        
        **Hyperparam√®tres PPO** :
        - **Gamma** : 0.99 (discount factor √©lev√© pour planification long terme)
        - **GAE lambda** : 0.95 (pour estimation de la valeur)
        - **Learning rate** : $3 \\times 10^{-4}$ (standard)
        - **Entropy coefficient** : 0.01-0.05 (contr√¥le exploration)
        - **Clip range** : 0.2 (pour PPO)
        - **Batch size** : 64-256
        - **Number of steps** : 2048 par rollout
        
        ### Strat√©gie de tuning
        
        **1. Commencer simple (Phase 1)** :
        - Valider le concept avec Phase 1 (moins de param√®tres)
        - V√©rifier que les rollouts courts sont coh√©rents
        - √âvaluer si l'am√©lioration est significative
        
        **2. √âtendre progressivement (Phase 2)** :
        - Si Phase 1 montre des b√©n√©fices, passer √† Phase 2
        - Augmenter l'horizon progressivement (10, 15, 20, 30)
        - V√©rifier la reconstruction du d√©codeur
        
        **3. Stabiliser (Phase 3)** :
        - Si Phase 2 d√©rive, activer Phase 3
        - Commencer avec $\\alpha = 0.5$, ajuster selon r√©sultats
        - Surveiller les m√©triques de stabilit√©
        
        **4. Optimisation fine** :
        - Ajuster $\\alpha$ selon la d√©rive observ√©e
        - R√©duire l'horizon si instabilit√©
        - Augmenter le nombre de trajectoires de pr√©-entra√Ænement si qualit√© insuffisante
        """)
    
    with st.expander("üß≠ Quand choisir le Sc√©nario 6 ?", expanded=False):
        st.markdown("""
        ### Indicateurs pour choisir le Sc√©nario 6
        
        **‚úÖ Choisir le Sc√©nario 6 si** :
        
        - **Besoin de planification** :
          - Vous voulez anticiper les effets des d√©cisions sur plusieurs jours/semaines
          - La strat√©gie optimale n√©cessite de penser √† long terme
          - Les autres sc√©narios montrent des limites sur la planification
        
        - **Donn√©es limit√©es en r√©el** :
          - Vous avez peu de donn√©es r√©elles mais pouvez g√©n√©rer des simulations
          - Le World Model permet de capitaliser sur un monde simul√©
          - Sample efficiency est critique
        
        - **Besoin de robustesse** :
          - Vous voulez combiner l'efficacit√© du model-based RL avec la robustesse physique
          - Phase 3 permet de rester coh√©rent physiquement
          - Vous voulez un compromis entre innovation et s√©curit√©
        
        - **Sc√©nario 5 insuffisant** :
          - PatchTST seul (Sc√©nario 5) ne capture pas assez les dynamiques longues
          - Vous voulez une planification plus explicite que l'enrichissement d'observation
          - Les features temporelles ne suffisent pas pour les d√©cisions complexes
        
        - **Recherche/exp√©rimentation** :
          - Vous explorez les approches model-based RL
          - Vous voulez comparer diff√©rents horizons de planification
          - Vous testez l'hybridation physique/neural
        
        **‚ùå Ne pas choisir le Sc√©nario 6 si** :
        
        - **Simplicit√© recherch√©e** :
          - Vous voulez une solution simple et rapide √† d√©ployer
          - Le Sc√©nario 2 ou 3 suffit pour vos besoins
        
        - **Temps de calcul limit√©** :
          - Le pr√©-entra√Ænement et les rollouts sont trop co√ªteux
          - Vous avez besoin de r√©sultats rapides
        
        - **Donn√©es abondantes** :
          - Vous avez beaucoup de donn√©es r√©elles et le sample efficiency n'est pas un probl√®me
          - Model-free RL (Sc√©narios 2-5) suffit
        
        - **Pas de besoin de planification** :
          - Les d√©cisions sont principalement r√©actives (court terme)
          - La planification √† long terme n'apporte pas de b√©n√©fice
        """)
    
    with st.expander("üõ†Ô∏è Conseils pratiques et troubleshooting", expanded=False):
        st.markdown("""
        ### Workflow recommand√©
        
        **1. Pr√©paration** :
        - ‚úÖ V√©rifier la **coh√©rence m√©t√©o** : Utiliser les m√™mes seeds et param√®tres que Sc√©nario 1 pour comparaison
        - ‚úÖ Pr√©-entra√Æner PatchTST dans Sc√©nario 5 d'abord
        - ‚úÖ S'assurer que les param√®tres du sol sont coh√©rents
        
        **2. D√©marrage progressif** :
        - ‚úÖ Commencer avec **Phase 1** (simple) pour valider le concept
        - ‚úÖ Utiliser des horizons courts (5-7) pour Phase 1
        - ‚úÖ V√©rifier que les rollouts sont coh√©rents
        
        **3. Extension** :
        - ‚úÖ Si Phase 1 r√©ussit, passer √† **Phase 2** avec d√©codeur
        - ‚úÖ Augmenter progressivement l'horizon (10, 15, 20, 30)
        - ‚úÖ Surveiller la qualit√© de reconstruction du d√©codeur
        
        **4. Stabilisation** :
        - ‚úÖ Si Phase 2 d√©rive, activer **Phase 3** avec hybridation
        - ‚úÖ Commencer avec $\\alpha = 0.5$
        - ‚úÖ Ajuster $\\alpha$ selon les r√©sultats (augmenter si d√©rive, diminuer si stable)
        
        ### Troubleshooting
        
        **Probl√®me : Rollouts incoh√©rents**
        - **Sympt√¥me** : Les rollouts imaginaires produisent des valeurs aberrantes
        - **Solutions** :
          - R√©duire l'horizon d'imagination
          - Augmenter le nombre de trajectoires de pr√©-entra√Ænement
          - V√©rifier la qualit√© du mod√®le de transition (loss √©lev√©e = mauvais pr√©-entra√Ænement)
          - Passer √† Phase 3 pour limiter la d√©rive
        
        **Probl√®me : Performance d√©grad√©e vs Sc√©nario 5**
        - **Sympt√¥me** : Le Sc√©nario 6 ne performe pas mieux que le Sc√©nario 5
        - **Solutions** :
          - V√©rifier que l'horizon est adapt√© (pas trop court, pas trop long)
          - Augmenter le nombre de rollouts par pas
          - V√©rifier que le World Model est bien pr√©-entra√Æn√©
          - Comparer les m√©triques d√©taill√©es (pas juste la r√©compense finale)
        
        **Probl√®me : D√©rive physique (Phase 2)**
        - **Sympt√¥me** : Les pr√©dictions s'√©loignent de la physique
        - **Solutions** :
          - Activer Phase 3 avec $\\alpha$ √©lev√© (0.6-0.7)
          - Augmenter le poids de la physique dans le blend
          - V√©rifier le pr√©-entra√Ænement du d√©codeur
          - R√©duire l'horizon si trop long
        
        **Probl√®me : Instabilit√© de l'entra√Ænement PPO**
        - **Sympt√¥me** : Les m√©triques PPO oscillent ou divergent
        - **Solutions** :
          - R√©duire le learning rate
          - R√©duire l'horizon d'imagination
          - Augmenter la stabilit√© du World Model (Phase 3)
          - V√©rifier les hyperparam√®tres PPO (gamma, GAE lambda)
        
        **Probl√®me : Temps d'entra√Ænement trop long**
        - **Sympt√¥me** : Le pr√©-entra√Ænement prend trop de temps
        - **Solutions** :
          - R√©duire le nombre de trajectoires (minimum 32)
          - R√©duire le nombre d'epochs (minimum 20)
          - Utiliser un batch size plus grand si m√©moire disponible
          - Commencer avec Phase 1 (plus rapide)
        
        ### M√©triques √† surveiller
        
        - **Loss du mod√®le de transition** : Doit d√©cro√Ætre et converger (< 0.01 id√©alement)
        - **Loss du d√©codeur** (Phase 2+) : Doit √™tre faible pour reconstruction fiable
        - **Coh√©rence des rollouts** : Les valeurs doivent rester dans des plages r√©alistes
        - **R√©compense moyenne** : Doit augmenter avec l'entra√Ænement
        - **Longueur d'√©pisode** : Doit √™tre stable
        - **Variance des actions** : Ne doit pas exploser (signe d'instabilit√©)
        """)
    
    with st.expander("üîó Comparaison avec les autres sc√©narios", expanded=False):
        st.markdown("""
        ### Sc√©nario 6 vs Sc√©nario 2 (RL basique)
        
        **Sc√©nario 2** :
        - Model-free RL direct sur mod√®le physique
        - Pas de planification explicite
        - Simple et rapide
        
        **Sc√©nario 6** :
        - Model-based RL avec World Model
        - Planification via rollouts d'imagination
        - Plus complexe mais meilleure sample efficiency
        
        **Quand choisir Sc√©nario 6** : Besoin de planification et sample efficiency
        
        ### Sc√©nario 6 vs Sc√©nario 3-4 (Neural ODE/CDE)
        
        **Sc√©narios 3-4** :
        - Correction r√©siduelle du mod√®le physique
        - Am√©liore la pr√©diction mais pas la planification
        - L'agent apprend directement dans l'environnement
        
        **Sc√©nario 6** :
        - World Model s√©par√© pour planification
        - Permet rollouts d'imagination
        - Architecture plus complexe
        
        **Quand choisir Sc√©nario 6** : Besoin de planification explicite
        
        ### Sc√©nario 6 vs Sc√©nario 5 (PatchTST)
        
        **Sc√©nario 5** :
        - PatchTST enrichit l'observation de l'agent
        - Model-free RL avec observation am√©lior√©e
        - Pas de rollouts d'imagination
        
        **Sc√©nario 6** :
        - PatchTST comme encodeur du World Model
        - Model-based RL avec planification
        - Rollouts d'imagination pour am√©liorer l'apprentissage
        
        **Relation** : Sc√©nario 6 r√©utilise PatchTST mais l'int√®gre dans une architecture model-based
        
        **Quand choisir Sc√©nario 6** : Sc√©nario 5 insuffisant pour planification √† long terme
        
        ### Synth√®se comparative
        
        | Crit√®re | Sc√©nario 2 | Sc√©nario 3-4 | Sc√©nario 5 | Sc√©nario 6 |
        |---------|------------|--------------|------------|------------|
        | **Complexit√©** | Faible | Moyenne | Moyenne | √âlev√©e |
        | **Planification** | ‚ùå | ‚ùå | ‚ùå | ‚úÖ |
        | **Sample Efficiency** | Faible | Moyenne | Moyenne | **√âlev√©e** |
        | **Robustesse physique** | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ (Phase 3) |
        | **Temps d'entra√Ænement** | Rapide | Moyen | Moyen | **Long** |
        | **Interpr√©tabilit√©** | ‚úÖ | ‚úÖ | Moyenne | Faible |
        """)


def render_doc_scenarios():
    """
    Affiche le contenu de l'onglet de documentation : Sc√©narios.
    """
    st.markdown('<h2 class="section-header">üìã Les sc√©narios d\'√©tude : du mod√®le physique au jumeau num√©rique cognitif</h2>', unsafe_allow_html=True)
    
    st.markdown("""
    Pr√©sentation des sc√©narios et de leur principe g√©n√©ral.
    """)
    
    st.markdown("### üå± Sc√©nario 1 ‚Äî Mod√®le physique + r√®gle simple")
    st.markdown("""
    - **Principe**: utiliser un mod√®le bucket (bilan hydrique) et une r√®gle fixe d'irrigation bas√©e sur un seuil de $\\psi$.
    """)
    
    with st.expander("üìã Types de mod√®les physiques disponibles", expanded=False):
        st.markdown("""
        - **FAO**
          - Mod√®le de bilan hydrique simplifi√© inspir√© de la m√©thodologie FAO-56
          - **√âquations du mod√®le** :
            - Bilan hydrique : $S_{t+1} = S_t + \\eta_I I_t + R_t + G_t - ETc_t - D(S_t) - Q_t$
            - Courbe de r√©tention : $\\psi_{t+1} = f_{\\text{retention}}(S_{t+1})$
            - o√π $S_t$ est la r√©serve en eau (mm), $\\psi_t$ la tension matricielle (cbar), $I_t$ l'irrigation (mm), $R_t$ la pluie (mm), $G_t$ la remont√©e capillaire (mm), $ETc_t$ l'√©vapotranspiration culture (mm), $D(S_t)$ le drainage (mm), $Q_t$ le ruissellement (mm), et $\\eta_I$ l'efficacit√© d'irrigation
          - Utilise un mod√®le bucket (r√©servoir), le coefficient cultural $Kc$ et l'√©vapotranspiration de r√©f√©rence $ET0$
          - Bilan hydrique journalier avec courbe de r√©tention simplifi√©e
          - Adapt√© pour des applications pratiques et rapides
        
        - **HYDRUS**
          - Mod√®le sophistiqu√© r√©solvant l'√©quation de Richards (√©quation diff√©rentielle partielle)
          - Mod√©lise le flux d'eau dans le sol en 1D, 2D ou 3D avec r√©solution num√©rique
          - Utilise des courbes de r√©tention compl√®tes (van Genuchten)
          - Tr√®s pr√©cis mais complexe et co√ªteux en calcul
        
        - **Aquacrop**
          - Mod√®le FAO avanc√© incluant la croissance de la culture, le d√©veloppement des racines, et des processus biologiques d√©taill√©s
          - Plus sophistiqu√© que le mod√®le bucket simple mais toujours bas√© sur les concepts FAO ($Kc$, $ET0$)
          - Adapt√© pour la mod√©lisation compl√®te du syst√®me culture-sol
        """)
    
    with st.expander("üîß Mod√®le impl√©ment√© : FAO", expanded=False):
        st.markdown("""
        **Raisons du choix** :
        
        - **Simplicit√© et rapidit√©** : Le mod√®le bucket permet des calculs instantan√©s, essentiel pour l'apprentissage par renforcement qui n√©cessite de nombreuses simulations
        
        - **Concepts FAO standardis√©s** : Utilisation de $Kc$ et $ET0$ (m√©thodologie FAO-56) reconnus et valid√©s internationalement
        
        - **Ad√©quation avec les observations** : Le mod√®le utilise directement la tension $\\psi_t$ mesur√©e par les tensiom√®tres, variable cl√© pour l'irrigation
        
        - **Efficacit√© computationnelle** : Pas de r√©solution d'√©quations diff√©rentielles complexes, permettant des milliers d'√©pisodes d'entra√Ænement RL en temps raisonnable
        
        - **Compromis pr√©cision/complexit√©** : Suffisamment pr√©cis pour capturer la dynamique essentielle du bilan hydrique tout en restant simple √† impl√©menter et calibrer
        
        - **Compatibilit√© RL** : La structure simple du mod√®le bucket facilite l'int√©gration avec les algorithmes d'apprentissage par renforcement
        """)
    
    with st.expander("‚öñÔ∏è Avantages et limitations", expanded=False):
        st.markdown("""
        ### ‚úÖ Avantages du Sc√©nario 1
        
        - **Simplicit√©** : 
          - Impl√©mentation directe : r√®gles bas√©es sur des seuils de tension ($\\psi$) faciles √† comprendre
          - Pas besoin d'entra√Ænement : r√®gles d√©finies manuellement, pas de phase d'apprentissage
          - D√©ploiement imm√©diat : peut √™tre mis en place rapidement sans infrastructure complexe
        
        - **Interpr√©tabilit√©** : 
          - Logique transparente : "si $\\psi_t > \\psi_{\\text{seuil}}$, alors irriguer $I_{\\text{fixe}}$"
          - Facile √† expliquer aux agriculteurs : r√®gles compr√©hensibles sans expertise en IA
          - D√©bogage simple : comportement pr√©visible et tra√ßable
        
        - **Rapidit√© d'ex√©cution** : 
          - Calculs instantan√©s : √©valuation de la r√®gle en temps constant
          - Pas de calculs lourds : pas de r√©seau de neurones √† √©valuer
          - Adapt√© aux syst√®mes embarqu√©s : faible consommation de ressources
        
        - **Robustesse** : 
          - Comportement stable : pas de variabilit√© due √† l'apprentissage
          - Pas de sur-apprentissage : r√®gles fixes garantissent un comportement coh√©rent
          - Pr√©visible : r√©sultats reproductibles pour les m√™mes conditions
        
        - **Co√ªt faible** : 
          - Pas d'infrastructure d'entra√Ænement n√©cessaire
          - Maintenance minimale : r√®gles simples √† maintenir
          - Pas de donn√©es d'entra√Ænement requises
        
        ### ‚ö†Ô∏è Limitations du Sc√©nario 1
        
        - **Manque d'adaptabilit√©** : 
          - Conditions m√©t√©orologiques : ne prend pas en compte les pr√©visions de pluie ou d'ET0 de mani√®re optimale
          - Variabilit√© saisonni√®re : seuils fixes ne s'adaptent pas aux changements de saison
          - Conditions locales : r√®gles g√©n√©riques qui ne s'adaptent pas aux sp√©cificit√©s de chaque parcelle
        
        - **Pas d'optimisation** : 
          - Efficacit√© de l'eau : ne minimise pas n√©cessairement la consommation d'eau
          - √âquilibre stress/co√ªt : ne trouve pas le compromis optimal entre stress hydrique et co√ªt de l'eau
          - Pas d'apprentissage : ne s'am√©liore pas avec l'exp√©rience
        
        - **Rigidit√© des r√®gles** : 
          - Seuils fixes : ne s'adaptent pas aux variations de conditions
          - Doses fixes : irrigation toujours de la m√™me quantit√©, sans gradation fine
          - Pas de strat√©gie pr√©ventive : r√©agit seulement quand le seuil est d√©pass√©, pas de pr√©vision
        
        - **Performance sous-optimale** : 
          - Gaspillage potentiel : peut irriguer m√™me si la pluie est imminente
          - Stress hydrique : peut laisser le sol se dess√©cher avant d'intervenir
          - Drainage excessif : peut provoquer des pertes d'eau par drainage si irrigation mal calibr√©e
        
        - **Maintenance manuelle** : 
          - Calibration n√©cessaire : les seuils doivent √™tre ajust√©s manuellement selon les conditions
          - Pas d'auto-ajustement : n√©cessite une intervention humaine pour optimiser les param√®tres
          - Expertise requise : besoin de connaissances agronomiques pour d√©finir les bons seuils
        """)
    
    with st.expander("üîß Param√®tres recommand√©s et tuning", expanded=False):
        st.markdown("""
        ### Param√®tres des r√®gles d'irrigation
        
        **R√®gle √† seuil unique** :
        - **Seuil de tension** ($\\psi_{\\text{seuil}}$) : 40-60 cbar (recommand√©)
          - Trop bas (< 30) : Irrigation trop fr√©quente, gaspillage d'eau
          - Trop √©lev√© (> 80) : Stress hydrique important avant irrigation
        - **Dose d'irrigation** : 10-20 mm (recommand√©)
          - Doit √™tre adapt√©e √† la capacit√© du sol et √† la culture
        
        **R√®gle √† bande de confort** :
        - **Tension minimale** ($\\psi_{\\min}$) : 20-30 cbar
        - **Tension maximale** ($\\psi_{\\max}$) : 50-70 cbar
        - **Dose proportionnelle** : Ajust√©e selon l'√©cart √† la zone de confort
        
        **R√®gle proportionnelle** :
        - **Coefficient de proportionnalit√©** ($k_I$) : 0.1-0.3
          - Plus √©lev√© = irrigation plus agressive
          - Plus faible = irrigation plus conservatrice
        
        ### Tuning recommand√©
        
        **√âtape 1 : Calibration initiale** :
        - Commencer avec des valeurs standard (seuil = 50 cbar, dose = 15 mm)
        - Observer le comportement sur une saison compl√®te
        
        **√âtape 2 : Ajustement selon r√©sultats** :
        - Si stress hydrique fr√©quent : R√©duire le seuil ou augmenter la dose
        - Si gaspillage d'eau : Augmenter le seuil ou r√©duire la dose
        - Si drainage excessif : R√©duire la dose
        
        **√âtape 3 : Ajustement saisonnier** :
        - Adapter les seuils selon la phase de croissance (Kc variable)
        - Tenir compte des pr√©visions m√©t√©orologiques
        """)
    
    with st.expander("üß≠ Quand utiliser le Sc√©nario 1 ?", expanded=False):
        st.markdown("""
        ### ‚úÖ Choisir le Sc√©nario 1 si :
        
        - **Simplicit√© recherch√©e** :
          - Vous voulez une solution simple et rapide √† d√©ployer
          - Pas d'infrastructure d'entra√Ænement disponible
          - Besoin de r√©sultats imm√©diats sans phase d'apprentissage
        
        - **Interpr√©tabilit√© importante** :
          - Les r√®gles doivent √™tre compr√©hensibles par les utilisateurs finaux
          - Besoin d'expliquer facilement les d√©cisions prises
          - Conformit√© r√©glementaire n√©cessitant de la transparence
        
        - **Ressources limit√©es** :
          - Pas de ressources computationnelles pour l'entra√Ænement RL
          - Pas de donn√©es historiques suffisantes
          - Syst√®me embarqu√© avec contraintes de calcul
        
        - **Conditions stables** :
          - Conditions m√©t√©orologiques et p√©dologiques relativement stables
          - Pas de variabilit√© saisonni√®re importante
          - Parcelle bien caract√©ris√©e avec param√®tres connus
        
        - **Baseline de r√©f√©rence** :
          - Point de d√©part pour comparer avec d'autres approches
          - Validation du mod√®le physique avant d'ajouter de la complexit√©
        
        ### ‚ùå Ne pas choisir le Sc√©nario 1 si :
        
        - **Optimisation n√©cessaire** :
          - Besoin de minimiser la consommation d'eau
          - Recherche du compromis optimal stress/co√ªt
          - Conditions variables n√©cessitant adaptation
        
        - **Donn√©es disponibles** :
          - Vous avez des donn√©es historiques pour entra√Æner un mod√®le RL
          - Possibilit√© de g√©n√©rer des simulations pour l'entra√Ænement
        
        - **Performance maximale recherch√©e** :
          - Les r√®gles simples ne suffisent pas pour vos objectifs
          - Besoin d'une strat√©gie adaptative et optimis√©e
        """)
    
    with st.expander("üõ†Ô∏è Conseils pratiques", expanded=False):
        st.markdown("""
        ### Workflow recommand√©
        
        **1. Calibration initiale** :
        - Utiliser les valeurs par d√©faut comme point de d√©part
        - Observer le comportement sur une saison compl√®te
        - Documenter les performances (stress, consommation d'eau, drainage)
        
        **2. Ajustement it√©ratif** :
        - Ajuster un param√®tre √† la fois (seuil OU dose, pas les deux)
        - Tester sur plusieurs saisons avec conditions vari√©es
        - Comparer les m√©triques avant/apr√®s ajustement
        
        **3. Validation** :
        - V√©rifier la coh√©rence avec les connaissances agronomiques
        - Comparer avec les pratiques locales
        - Valider sur diff√©rentes conditions m√©t√©orologiques
        
        ### Troubleshooting
        
        **Probl√®me : Stress hydrique fr√©quent**
        - **Solution** : R√©duire le seuil de tension (ex: 50 ‚Üí 40 cbar) ou augmenter la dose
        
        **Probl√®me : Gaspillage d'eau**
        - **Solution** : Augmenter le seuil (ex: 50 ‚Üí 60 cbar) ou r√©duire la dose
        
        **Probl√®me : Drainage excessif**
        - **Solution** : R√©duire la dose d'irrigation ou augmenter le seuil
        
        **Probl√®me : Irrigation trop tardive**
        - **Solution** : Utiliser une r√®gle pr√©ventive ou r√©duire le seuil
        """)
    
    with st.expander("üîó Comparaison avec les autres sc√©narios", expanded=False):
        st.markdown("""
        ### Sc√©nario 1 vs Sc√©nario 2 (RL basique)
        
        **Sc√©nario 1** :
        - R√®gles fixes, pas d'apprentissage
        - Simple et rapide
        - Performance sous-optimale
        
        **Sc√©nario 2** :
        - Apprentissage automatique de la politique
        - Plus complexe mais meilleure performance
        - N√©cessite entra√Ænement
        
        **Quand choisir Sc√©nario 1** : Simplicit√© et rapidit√© prioritaires
        
        ### Sc√©nario 1 vs Sc√©narios 3-6
        
        **Sc√©nario 1** :
        - Baseline simple
        - Pas d'optimisation
        - Pas d'adaptation
        
        **Sc√©narios 3-6** :
        - Approches avanc√©es avec apprentissage
        - Optimisation et adaptation
        - Meilleure performance mais plus complexe
        
        **Relation** : Sc√©nario 1 sert de r√©f√©rence pour √©valuer les gains des autres sc√©narios
        """)
    
    st.markdown("### üéì Sc√©nario 2 ‚Äî RL sur mod√®le physique (avec $\\psi_t$ observ√©e)")
    st.markdown("""
    - **Principe**: un agent RL observe $\\psi_t$ (et le contexte m√©t√©o) et choisit $I_t$ dans un environnement simul√© par le mod√®le physique.
      - **Espace d'observation**: $o_t = (\\psi_t,\\ t/T,\\ R_{t-k:t},\\ ET0_t,\\ \\hat R_{t+1:t+h},\\ \\widehat{ET0}_{t+1:t+h})$
      - **Espace d'actions**: $I_t \\in [0,\\ I_{\\max}]$ (mm) - continu
      - **R√©compense**: $r_t = -\\alpha\\,\\text{stress}(\\psi_t) - \\beta\\, I_t - \\gamma\\, D(S_t)$
      - **Algorithme**: PPO (Proximal Policy Optimization)
    """)
    
    with st.expander("‚öñÔ∏è Avantages et limitations", expanded=False):
        st.markdown("""
        ### ‚úÖ Avantages du Sc√©nario 2
        
        - **Apprentissage d'une politique optimale** : 
          - Optimisation automatique : l'agent RL apprend √† minimiser le stress hydrique tout en √©conomisant l'eau
          - Compromis optimal : trouve automatiquement le meilleur √©quilibre entre performance agronomique et co√ªt de l'eau
          - Strat√©gie adaptative : s'ajuste selon les conditions m√©t√©orologiques et l'√©tat du sol
        
        - **Adaptabilit√© aux conditions** : 
          - Pr√©visions m√©t√©o : utilise les pr√©visions de pluie et d'ET0 pour anticiper et ajuster l'irrigation
          - Conditions variables : s'adapte aux variations saisonni√®res et aux √©v√©nements m√©t√©orologiques
          - Historique contextuel : prend en compte l'historique r√©cent (pluie, tension) pour des d√©cisions inform√©es
        
        - **Respect de la physique** : 
          - Mod√®le physique fiable : utilise un mod√®le bucket valid√© pour simuler la dynamique du sol
          - Courbe de r√©tention : respecte la relation $S \\leftrightarrow \\psi$ bas√©e sur les propri√©t√©s p√©dophysiques
          - Bilan hydrique coh√©rent : les √©quations physiques garantissent la coh√©rence des pr√©dictions
        
        - **Flexibilit√© des actions** : 
          - Actions continues : permet des doses d'irrigation pr√©cises et gradu√©es (pas seulement 0 ou dose fixe)
          - Doses adaptatives : ajuste la quantit√© d'eau selon l'intensit√© du stress et les conditions
          - Strat√©gie pr√©ventive : peut irriguer pr√©ventivement avant que le stress ne devienne critique
        
        - **Performance sup√©rieure** : 
          - Efficacit√© de l'eau : g√©n√©ralement meilleure que les r√®gles fixes en termes de consommation d'eau
          - R√©duction du stress : maintient mieux la tension dans la zone de confort
          - Minimisation du drainage : apprend √† √©viter les pertes d'eau par drainage excessif
        
        - **R√©utilisabilit√©** : 
          - Mod√®le entra√Æn√© : une fois entra√Æn√©, le mod√®le peut √™tre utilis√© sur diff√©rentes saisons
          - Transfert possible : peut √™tre adapt√© √† d'autres parcelles avec r√©-entra√Ænement
          - Am√©lioration continue : peut √™tre r√©-entra√Æn√© avec de nouvelles donn√©es pour s'am√©liorer
        
        ### ‚ö†Ô∏è Limitations du Sc√©nario 2
        
        - **D√©pendance √† la qualit√© du mod√®le physique** : 
          - Biais du mod√®le : si le mod√®le bucket a des biais (param√®tres mal calibr√©s, processus n√©glig√©s), 
            la politique apprise sera biais√©e
          - Erreurs de param√©trisation : des erreurs dans les param√®tres du sol ($S_{fc}$, $\\psi_{fc}$, $k_d$) 
            se propagent dans les d√©cisions
          - Processus non mod√©lis√©s : ph√©nom√®nes non captur√©s par le mod√®le (h√©t√©rog√©n√©it√© spatiale, 
            interactions complexes) ne sont pas pris en compte
        
        - **Phase d'entra√Ænement n√©cessaire** : 
          - Temps d'entra√Ænement : n√©cessite une phase d'apprentissage (plusieurs milliers de timesteps) 
            avant d'√™tre utilisable
          - Ressources computationnelles : entra√Ænement PPO n√©cessite des ressources CPU/GPU
          - Expertise technique : n√©cessite des comp√©tences en RL pour l'entra√Ænement et le r√©glage
        
        - **Donn√©es d'entra√Ænement** : 
          - Simulation requise : besoin de g√©n√©rer des donn√©es de simulation pour l'entra√Ænement
          - Qualit√© de la simulation : la qualit√© de l'entra√Ænement d√©pend de la qualit√© de la simulation m√©t√©o
          - Robustesse : n√©cessite d'entra√Æner sur plusieurs saisons/sc√©narios pour √™tre robuste
        
        - **Complexit√© de d√©ploiement** : 
          - Infrastructure : n√©cessite une infrastructure pour ex√©cuter le mod√®le entra√Æn√©
          - Maintenance : le mod√®le peut n√©cessiter un r√©-entra√Ænement p√©riodique
          - Interpr√©tabilit√© r√©duite : moins interpr√©table que les r√®gles simples (bo√Æte noire)
        
        - **Hyperparam√®tres √† r√©gler** : 
          - Tuning n√©cessaire : nombreux hyperparam√®tres √† ajuster (learning rate, gamma, GAE-$\\lambda$, etc.)
          - Sensibilit√© : la performance peut √™tre sensible aux choix d'hyperparam√®tres
          - Expertise requise : n√©cessite une compr√©hension du RL pour optimiser les hyperparam√®tres
        
        - **Stabilit√© de l'apprentissage** : 
          - Convergence : l'entra√Ænement peut ne pas converger ou converger vers un optimum local
          - Variabilit√© : la performance peut varier entre diff√©rentes ex√©cutions d'entra√Ænement
          - Normalisation : n√©cessite une normalisation soigneuse des observations et r√©compenses
        
        - **Observations coh√©rentes** : 
          - Alignement temporel : n√©cessite que les observations soient align√©es temporellement
          - Donn√©es manquantes : doit g√©rer les cas de donn√©es manquantes ou irr√©guli√®res
          - Pr√©visions m√©t√©o : d√©pend de la qualit√© des pr√©visions m√©t√©orologiques disponibles
        """)
    
    with st.expander("üîß Param√®tres recommand√©s et tuning", expanded=False):
        st.markdown("""
        ### Hyperparam√®tres PPO
        
        **Learning rate** : $3 \\times 10^{-4}$ (recommand√©)
        - Trop √©lev√© (> $10^{-3}$) : Instabilit√©, oscillations
        - Trop faible (< $10^{-5}$) : Apprentissage trop lent
        - Tuning : R√©duire si loss oscille, augmenter si convergence lente
        
        **Gamma (discount factor)** : 0.99 (recommand√©)
        - Contr√¥le l'importance des r√©compenses futures
        - √âlev√© (0.99) : Planification √† long terme
        - Faible (0.95) : Focus sur court terme
        
        **GAE lambda** : 0.95 (recommand√©)
        - Contr√¥le le biais/variance de l'estimation de la valeur
        - √âlev√© (0.95-0.99) : Moins de variance, plus de biais
        - Faible (0.8-0.9) : Plus de variance, moins de biais
        
        **Entropy coefficient** : 0.01-0.05
        - Encourage l'exploration
        - √âlev√© : Plus d'exploration, convergence plus lente
        - Faible : Moins d'exploration, risque de sous-optimum local
        
        **Clip range** : 0.2 (standard PPO)
        - Limite les changements de politique
        - √âlev√© : Permet plus de changements, moins stable
        - Faible : Changements limit√©s, plus stable
        
        **Batch size** : 64-256
        - Plus grand : Gradients plus stables mais plus de m√©moire
        - Plus petit : Moins de m√©moire mais gradients plus variables
        
        **Number of steps per rollout** : 2048
        - Plus grand : Meilleure estimation mais plus de m√©moire
        - Plus petit : Moins de m√©moire mais estimation moins pr√©cise
        
        ### Hyperparam√®tres de l'environnement
        
        **Param√®tres de r√©compense** :
        - $\\alpha$ (p√©nalit√© stress) : 1.0 (recommand√©)
        - $\\beta$ (p√©nalit√© irrigation) : 0.05 (recommand√©)
        - $\\gamma$ (p√©nalit√© drainage) : 0.01 (recommand√©)
        - Tuning : Ajuster selon priorit√©s (eau vs stress)
        
        **Param√®tres du sol** :
        - Utiliser les valeurs par d√©faut sauf si donn√©es sp√©cifiques disponibles
        - Calibrer $S_{fc}$, $\\psi_{fc}$ selon mesures r√©elles si possible
        
        ### Strat√©gie de tuning
        
        **1. Commencer avec valeurs par d√©faut** :
        - Utiliser les valeurs recommand√©es ci-dessus
        - Entra√Æner sur 50,000-100,000 timesteps
        
        **2. Observer les m√©triques** :
        - R√©compense moyenne : Doit augmenter
        - Longueur d'√©pisode : Doit √™tre stable
        - Variance des actions : Ne doit pas exploser
        
        **3. Ajuster si n√©cessaire** :
        - Si instabilit√© : R√©duire learning rate, augmenter clip range
        - Si convergence lente : Augmenter learning rate, r√©duire entropy
        - Si sous-optimum : Augmenter entropy, r√©duire clip range
        """)
    
    with st.expander("üß≠ Quand utiliser le Sc√©nario 2 ?", expanded=False):
        st.markdown("""
        ### ‚úÖ Choisir le Sc√©nario 2 si :
        
        - **Optimisation recherch√©e** :
          - Besoin de minimiser la consommation d'eau
          - Recherche du compromis optimal stress/co√ªt
          - Performance sup√©rieure aux r√®gles simples
        
        - **Donn√©es disponibles** :
          - Possibilit√© de g√©n√©rer des simulations pour l'entra√Ænement
          - Mod√®le physique fiable et bien calibr√©
          - Conditions m√©t√©orologiques vari√©es pour robustesse
        
        - **Ressources computationnelles** :
          - Infrastructure disponible pour l'entra√Ænement PPO
          - Temps d'entra√Ænement acceptable (quelques heures)
          - Expertise en RL disponible
        
        - **Adaptabilit√© n√©cessaire** :
          - Conditions variables n√©cessitant adaptation
          - Besoin de strat√©gie pr√©ventive
          - Optimisation selon objectifs multiples
        
        - **Point de d√©part pour approches avanc√©es** :
          - Baseline pour comparer avec Sc√©narios 3-6
          - Validation de l'approche RL avant complexification
        
        ### ‚ùå Ne pas choisir le Sc√©nario 2 si :
        
        - **Simplicit√© prioritaire** :
          - Besoin de solution simple et rapide
          - Pas d'infrastructure d'entra√Ænement
          - R√®gles simples suffisent
        
        - **Mod√®le physique incertain** :
          - Param√®tres du sol mal connus
          - Mod√®le physique non valid√©
          - Donn√©es de simulation de mauvaise qualit√©
        
        - **Donn√©es limit√©es** :
          - Pas de possibilit√© de g√©n√©rer des simulations
          - Conditions trop sp√©cifiques pour g√©n√©raliser
        
        - **Besoin de correction physique** :
          - Mod√®le physique a des biais connus
          - N√©cessit√© de corriger les pr√©dictions physiques
          - ‚Üí Pr√©f√©rer Sc√©narios 3-4
        """)
    
    with st.expander("üõ†Ô∏è Conseils pratiques", expanded=False):
        st.markdown("""
        ### Workflow recommand√©
        
        **1. Pr√©paration** :
        - V√©rifier la coh√©rence m√©t√©o (m√™mes seeds/params que Sc√©nario 1)
        - Valider le mod√®le physique sur quelques √©pisodes
        - Configurer les hyperparam√®tres avec valeurs par d√©faut
        
        **2. Entra√Ænement initial** :
        - Commencer avec 50,000 timesteps
        - Observer les m√©triques (r√©compense, longueur d'√©pisode)
        - V√©rifier la convergence
        
        **3. Tuning it√©ratif** :
        - Ajuster les hyperparam√®tres si n√©cessaire
        - R√©-entra√Æner avec nouveaux param√®tres
        - Comparer les performances
        
        **4. √âvaluation** :
        - Tester sur nouvelles saisons (seeds diff√©rents)
        - Comparer avec Sc√©nario 1 (baseline)
        - Analyser les d√©cisions prises
        
        ### Troubleshooting
        
        **Probl√®me : Instabilit√© de l'entra√Ænement**
        - **Sympt√¥me** : Loss oscille, r√©compense ne converge pas
        - **Solutions** :
          - R√©duire learning rate (ex: $3 \\times 10^{-4} \\to 10^{-4}$)
          - Augmenter clip range (ex: 0.2 ‚Üí 0.3)
          - Normaliser les observations et r√©compenses
        
        **Probl√®me : Convergence lente**
        - **Sympt√¥me** : R√©compense augmente tr√®s lentement
        - **Solutions** :
          - Augmenter learning rate (avec prudence)
          - Augmenter entropy coefficient pour plus d'exploration
          - V√©rifier la normalisation des r√©compenses
        
        **Probl√®me : Sous-optimum local**
        - **Sympt√¥me** : Performance plafonne √† un niveau sous-optimal
        - **Solutions** :
          - Augmenter entropy coefficient
          - R√©duire clip range pour permettre plus de changements
          - Augmenter le nombre de timesteps d'entra√Ænement
        
        **Probl√®me : Politique trop conservatrice**
        - **Sympt√¥me** : Irrigation insuffisante, stress hydrique
        - **Solutions** :
          - Ajuster les poids de r√©compense ($\\alpha$ vs $\\beta$)
          - Augmenter la p√©nalit√© de stress ($\\alpha$)
          - R√©duire la p√©nalit√© d'irrigation ($\\beta$)
        
        ### M√©triques √† surveiller
        
        - **R√©compense moyenne** : Doit augmenter avec l'entra√Ænement
        - **Longueur d'√©pisode** : Doit √™tre stable (‚âà longueur de saison)
        - **Variance des actions** : Ne doit pas exploser (signe d'instabilit√©)
        - **Policy loss** : Doit d√©cro√Ætre et converger
        - **Value loss** : Doit d√©cro√Ætre (estimation de la valeur)
        """)
    
    with st.expander("üîó Comparaison avec les autres sc√©narios", expanded=False):
        st.markdown("""
        ### Sc√©nario 2 vs Sc√©nario 1 (R√®gles simples)
        
        **Sc√©nario 1** :
        - R√®gles fixes, pas d'apprentissage
        - Simple et rapide
        - Performance sous-optimale
        
        **Sc√©nario 2** :
        - Apprentissage automatique
        - Plus complexe mais meilleure performance
        - N√©cessite entra√Ænement
        
        **Quand choisir Sc√©nario 2** : Optimisation et adaptabilit√© recherch√©es
        
        ### Sc√©nario 2 vs Sc√©narios 3-4 (Neural ODE/CDE)
        
        **Sc√©nario 2** :
        - RL direct sur mod√®le physique
        - Pas de correction du mod√®le physique
        - Plus simple
        
        **Sc√©narios 3-4** :
        - Correction r√©siduelle du mod√®le physique
        - Am√©liore la pr√©diction physique
        - Plus complexe
        
        **Quand choisir Sc√©narios 3-4** : Mod√®le physique a des biais connus
        
        ### Sc√©nario 2 vs Sc√©nario 5 (PatchTST)
        
        **Sc√©nario 2** :
        - Observation standard (4 dimensions)
        - Pas de m√©moire temporelle explicite
        
        **Sc√©nario 5** :
        - Observation enrichie avec features temporelles
        - M√©moire longue via PatchTST
        
        **Quand choisir Sc√©nario 5** : Besoin de comprendre tendances et saisonnalit√©
        
        ### Sc√©nario 2 vs Sc√©nario 6 (World Model)
        
        **Sc√©nario 2** :
        - Model-free RL
        - Pas de planification explicite
        
        **Sc√©nario 6** :
        - Model-based RL avec planification
        - Rollouts d'imagination
        
        **Quand choisir Sc√©nario 6** : Besoin de planification et sample efficiency
        """)
    
    st.markdown("### üî¨ Sc√©nario 3 ‚Äî RL sur mod√®le hybride Physique + Neural ODE")
    st.markdown("""
    - **Principe**: corriger la pr√©diction physique de $\\psi_{t+1}$ par une correction neuronale locale $\\Delta \\psi$ 
      apprise √† partir de $(\\psi_t, I_t, R_t, ET0_t)$.
    """)
    
    with st.expander("‚öñÔ∏è Avantages et limitations", expanded=False):
        st.markdown("""
        ### ‚úÖ Avantages du Neural ODE
        
        - **Combinaison physique et donn√©es** : Le Neural ODE (Ordinary Differential Equation) combine le meilleur des deux mondes :
          
          - **Mod√®le physique comme base** : Le mod√®le bucket fournit une pr√©diction initiale $\\psi_{t+1}^{phys}$ bas√©e sur 
            les lois physiques connues (bilan hydrique, courbe de r√©tention). Cette base garantit que les pr√©dictions 
            respectent les principes fondamentaux de la dynamique du sol.
          
          - **Correction neuronale adaptative** : Un r√©seau de neurones apprend une correction locale $\\Delta \\psi$ qui 
            ajuste la pr√©diction physique en fonction des donn√©es observ√©es. Cette correction capture les ph√©nom√®nes 
            non mod√©lis√©s ou mal param√©tr√©s dans le mod√®le physique.
        
        - **Correction des biais du mod√®le** : Le Neural ODE peut corriger plusieurs types de biais :
          
          - **Biais de param√©trisation** : Si les param√®tres du sol (par exemple $S_{fc}$, $\\psi_{fc}$, $k_d$) sont mal 
            estim√©s ou varient dans l'espace, le Neural ODE apprend √† compenser ces erreurs.
          
          - **Ph√©nom√®nes non mod√©lis√©s** : Certains processus physiques peuvent √™tre n√©glig√©s ou simplifi√©s dans le mod√®le 
            bucket (par exemple, h√©t√©rog√©n√©it√© spatiale, effets de la structure du sol, interactions racines-sol complexes). 
            Le Neural ODE peut apprendre √† capturer ces effets √† partir des donn√©es.
          
          - **Erreurs de mesure** : Les capteurs peuvent avoir des biais syst√©matiques ou des erreurs de calibration. 
            Le Neural ODE peut apprendre √† les corriger si ces erreurs sont coh√©rentes dans le temps.
        
        - **Apprentissage continu** : Contrairement aux mod√®les purement physiques qui sont statiques, le Neural ODE 
          peut √™tre r√©-entra√Æn√© avec de nouvelles donn√©es pour s'adapter √† l'√©volution des conditions (par exemple, 
          changement de structure du sol, vieillissement des capteurs).
        
        - **Interpr√©tabilit√©** : La structure hybride permet de s√©parer la contribution du mod√®le physique (interpr√©table) 
          de la correction neuronale (qui peut √™tre analys√©e pour comprendre quels ph√©nom√®nes sont mal captur√©s).
        
        - **Efficacit√© computationnelle** : En utilisant le mod√®le physique comme base, le Neural ODE n√©cessite moins 
          de donn√©es et d'entra√Ænement qu'un mod√®le purement neuronal, tout en √©tant plus performant qu'un mod√®le 
          purement physique.
        
        ### ‚ö†Ô∏è Limitations du Neural ODE
        
        - **Donn√©es n√©cessaires** : N√©cessite des donn√©es r√©elles pour l'apprentissage du r√©seau de correction. 
          Plus les donn√©es sont nombreuses et repr√©sentatives, meilleure sera la correction.
        
        - **Hypoth√®se de r√©gularit√©** : Le Neural ODE suppose g√©n√©ralement des observations √† intervalles r√©guliers. 
          Les donn√©es manquantes ou irr√©guli√®res n√©cessitent un pr√©-traitement (interpolation, imputation).
        
        - **Complexit√©** : Plus complexe qu'un mod√®le purement physique, n√©cessite une expertise en deep learning 
          pour l'entra√Ænement et le r√©glage des hyperparam√®tres.
        
        - **D√©pendance aux donn√©es d'entra√Ænement** : La qualit√© de la correction d√©pend de la repr√©sentativit√© 
          des donn√©es d'entra√Ænement. Si les conditions changent significativement (nouveau type de sol, nouvelle culture), 
          le mod√®le peut n√©cessiter un r√©-entra√Ænement.
        """)
    
    st.markdown("### üß† Sc√©nario 4 ‚Äî RL sur mod√®le hybride Physique + Neural CDE")
    st.markdown("""
    - **Principe**: exploiter des trajectoires (possiblement irr√©guli√®res) $[\\psi, I, R, ET0]$ via un √©tat latent (CDE) 
      pour produire une correction temporelle coh√©rente.
    """)
    
    with st.expander("‚öñÔ∏è Avantages et limitations", expanded=False):
        st.markdown("""
        ### ‚úÖ Avantages du Neural CDE
        
        - **Gestion des donn√©es irr√©guli√®res** : Le Neural CDE (Controlled Differential Equation) est particuli√®rement 
          adapt√© aux s√©ries temporelles avec des observations √† intervalles irr√©guliers. En pratique, cela signifie :
          
          - **Fiabilit√© des capteurs** : Les tensiom√®tres et autres capteurs peuvent avoir des pannes temporaires, 
            des d√©faillances de communication, ou n√©cessiter des calibrations p√©riodiques. Le Neural CDE peut 
            g√©rer ces p√©riodes de donn√©es manquantes sans n√©cessiter d'interpolation artificielle.
          
          - **Fr√©quence d'√©chantillonnage variable** : Contrairement aux mod√®les classiques qui supposent des mesures 
            √† intervalles r√©guliers (par exemple, toutes les heures ou tous les jours), le Neural CDE peut traiter 
            des donn√©es qui arrivent √† des moments diff√©rents (ex. : mesure √† 8h un jour, √† 10h le lendemain, puis 
            aucune mesure pendant 2 jours).
          
          - **Robustesse aux pannes** : Si un capteur tombe en panne pendant plusieurs jours, le mod√®le peut continuer 
            √† fonctionner en utilisant les derni√®res observations valides et en extrapolant de mani√®re coh√©rente gr√¢ce 
            √† l'√©tat latent du CDE.
        
        - **Meilleure mod√©lisation temporelle** : Le CDE mod√©lise explicitement l'√©volution continue du syst√®me, 
          ce qui permet une meilleure compr√©hension de la dynamique du sol entre les observations.
        
        - **Adaptation aux contraintes op√©rationnelles** : Dans un contexte r√©el, les mesures peuvent √™tre prises 
          √† des moments opportuns (visites de terrain, maintenance), pas n√©cessairement √† intervalles fixes.
        
        ### ‚ö†Ô∏è Limitations du Neural CDE
        
        - **Complexit√©** : Plus complexe √† impl√©menter et √† entra√Æner que les mod√®les pr√©c√©dents
        - **Donn√©es n√©cessaires** : N√©cessite plus de donn√©es pour l'apprentissage, notamment pour calibrer 
          l'√©tat latent du CDE
        - **Temps de calcul** : G√©n√©ralement plus co√ªteux en temps de calcul que les approches plus simples
        """)

    st.markdown("### üîÆ Sc√©nario 5 ‚Äî RL + PatchTST (features temporelles)")
    st.markdown("""
    - **Principe** : utiliser PatchTST comme encodeur de s√©quences $[\\psi, I, R, ET0]$ pour enrichir l'observation de l'agent.
    - **R√¥le** : pas de correction physique, mais un meilleur contexte temporel pour l'agent RL.
    - **Pipeline** : pr√©-entra√Ænement auto-supervis√© sur donn√©es simul√©es ‚Üí wrapper d'environnement qui concat√®ne les features.
    - **Quand l'utiliser** : besoin de m√©moire longue (tendance/seasonality) sans toucher au mod√®le physique.
    """)

    st.markdown("### üåç Sc√©nario 6 ‚Äî World Model (model-based RL)")
    st.markdown("""
    - **Principe** : apprendre un mod√®le du monde (PatchTST + transition ODE/CDE + d√©codeur) pour faire des rollouts d'imagination.
    - **Phases** :
        - **Phase 1** : Transition ODE, rollouts courts.
        - **Phase 2** : Transition CDE + d√©codeur, rollouts longs.
        - **Phase 3** : Physics-informed (blend world model + mod√®le physique).
    - **Objectif** : planification, efficacit√© sample et robustesse via hybridation.
    """)

    st.markdown("### üìä Comparaison des sc√©narios")
    st.markdown("""
    | Sc√©nario | Complexit√© | Adaptabilit√© | Donn√©es n√©cessaires | Performance attendue |
    |----------|------------|--------------|---------------------|----------------------|
    | 1. R√®gle simple | Faible | Faible | Aucune | Basique |
    | 2. RL physique | Moyenne | √âlev√©e | Simulation | Bonne |
    | 3. RL + Neural ODE | √âlev√©e | Tr√®s √©lev√©e | R√©elles + Simulation | Tr√®s bonne |
    | 4. RL + Neural CDE | Tr√®s √©lev√©e | Tr√®s √©lev√©e | R√©elles + Simulation | Excellente |
    | 5. RL + PatchTST | √âlev√©e | √âlev√©e | Simulation (pr√©-train) | Tr√®s bonne (contexte temporel) |
    | 6. World Model (Phases 1-3) | Tr√®s √©lev√©e | Tr√®s √©lev√©e | Simulation + pr√©-train | Excellente (planification) |
    """)
    
