"""
Utilitaires pour la documentation de l'interface utilisateur.

Ce module contient les fonctions qui g√©n√®rent le contenu des onglets de documentation
de l'application Streamlit. Chaque fonction correspond √† un onglet de documentation
et retourne le contenu markdown/HTML √† afficher.
"""

import streamlit as st


def render_doc_irrigation_intelligente():
    """
    Affiche le contenu de l'onglet de documentation : Irrigation Intelligente.
    """
    st.markdown('<h2 class="section-header">üíß Pourquoi s\'int√©resser √† l\'irrigation intelligente ?</h2>', unsafe_allow_html=True)
    
    #st.image("images/logo_uttop.jpg", width=200)
        
    st.markdown("""
    Dans un contexte de changement climatique, l'eau devient une ressource rare, co√ªteuse et de plus en plus incertaine. 
    Les agriculteurs doivent arbitrer entre :
    
    - **Maintenir un niveau d'humidit√© adapt√© √† la culture** : assurer une croissance optimale et un rendement satisfaisant
    - **√âconomiser l'eau** : respecter les quotas et r√©duire les co√ªts
    - **√âviter la lixiviation des nutriments** : pr√©venir les pertes en cas d'arrosage excessif
    
    ### Pratiques actuelles
    
    Aujourd'hui, de nombreux irrigants prennent leurs d√©cisions √† partir de :
    - **Seuils simples de tension** : ex. "si la tension d√©passe 80 cbar, j'irrigue"
    - **Calendriers d'irrigation** : programmes fixes bas√©s sur l'exp√©rience
    - **Exp√©rience personnelle** : intuition et connaissance du terrain
    """)
    
    st.markdown("### Opportunit√©s technologiques")
    
    st.markdown("""
    <div style="background-color: #E3F2FD; padding: 20px; border-radius: 10px; border-left: 4px solid #2196F3; margin: 15px 0;">
    <p style="margin: 0 0 10px 0;"><strong>Avec l'essor de :</strong></p>
    <ul style="margin: 0 0 15px 0; padding-left: 20px;">
    <li><strong>Tensiom√®tres</strong> : mesure de la tension matricielle du sol</li>
    <li><strong>Pr√©visions m√©t√©o</strong> : donn√©es de pluie et d'√©vapotranspiration</li>
    <li><strong>Techniques d'IA</strong> : apprentissage automatique et optimisation</li>
    </ul>
    <p style="margin: 10px 0 0 0;"><strong>La question devient :</strong></p>
    <blockquote style="margin: 15px 0 0 0; padding: 10px 15px; background-color: #BBDEFB; border-left: 3px solid #2196F3; border-radius: 5px; font-style: italic;">
    <strong>"Peut-on apprendre automatiquement une politique d'irrigation qui utilise les tensions mesur√©es, 
    respecte la physique du sol, et s'adapte √† la parcelle r√©elle ?"</strong>
    </blockquote>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("""
    ### Approche propos√©e
    
    Le travail pr√©sent√© ici combine :
    - **Mod√®les physiques** : simulation du bilan hydrique du sol
    - **Mod√®les neuronaux** : Neural ODE / Neural CDE pour la correction
    - **Apprentissage par renforcement (RL)** : pour piloter l'irrigation √† partir des s√©ries temporelles de tension de l'eau ($\\psi_t$)
    """)


def render_doc_variables_etat():
    """
    Affiche le contenu de l'onglet de documentation : Variables d'√âtat.
    """
    st.markdown('<h2 class="section-header">üìä Les variables cl√©s : ce que l\'on mesure vraiment</h2>', unsafe_allow_html=True)
    
    st.markdown("""
    Une saison culturale est d√©crite jour par jour : $t = 0,1,\\dots,T$
    
    ### Variable effectivement mesur√©e : la tension de l'eau
    
    Le tensiom√®tre donne directement :
    
    $$\\psi_t \\quad (\\text{cbar})$$
    
    La tension est la force que la plante doit exercer pour extraire l'eau.
    C'est la **variable observ√©e**, et c'est aussi ce que "ressent" r√©ellement la culture.
    
    ### Variable interne non observ√©e : la r√©serve en eau du sol
    
    Le mod√®le physique du bilan hydrique travaille pourtant avec une variable interne :
    
    $$S_t \\quad (\\text{mm})$$
    
    C'est la quantit√© d'eau stock√©e dans la zone racinaire.
    Mais **nous ne l'observons jamais directement**.
    
    ### Comment relier les deux ?
    
    On utilise une **courbe de r√©tention** propre au sol :
    
    $$\\psi_t = f_{\\text{retention}}(S_t) \\quad\\text{et id√©alement}\\quad S_t = f_{\\text{retention}}^{-1}(\\psi_t)$$
    
    - Le tensiom√®tre mesure $\\psi_t$.
    - Le mod√®le reconstruit une estimation de $S_t$.
    - Le bilan hydrique agit sur $S_t$.
    - Puis on reconvertit en $\\psi_{t+1}$ pour comparaison.
    
    C'est une architecture √† **√©tats cach√©s**, courante en agro-hydrologie.
    """)
    
    st.markdown("### Param√®tres de sol (p√©dophysique)")
    st.markdown("""
    - $Z_r$ : profondeur de la zone racinaire (mm)
    - $\\theta_s, \\ \\theta_{fc}, \\ \\theta_{wp}$ : saturation, capacit√© au champ, point de fl√©trissement
    - $S_{\\max} = \\theta_s \\cdot Z_r$, $S_{fc} = \\theta_{fc} \\cdot Z_r$, $S_{wp} = \\theta_{wp} \\cdot Z_r$
    """)
    
    st.markdown("### Flux entrants")
    st.markdown("""
    - $I_t$ : dose d'irrigation (mm)
    - $\\eta_I$ : efficacit√© d'irrigation (0‚Äì1)
    - $R_t$ : pluie (mm)
    - $G_t$ : remont√©e capillaire (optionnelle, mm)
    """)
    
    st.markdown("### Flux sortants")
    st.markdown("""
    - $ET0_t$ : √©vapotranspiration de r√©f√©rence (mm/j)
    - $Kc_t$ : coefficient cultural (adimensionnel) - repr√©sente la demande en eau de la culture selon son stade de d√©veloppement
    - $f_{ET}(\\psi_t)$ : facteur de stress hydrique (0‚Äì1)
    - $ETc_t = Kc_t \\cdot ET0_t \\cdot f_{ET}(\\psi_t)$
    - $D(S_t)$ : drainage/percolation (mm)
    - $Q_t$ : ruissellement (optionnel, mm)
    """)
    
    st.markdown("### Dynamique physique (bilan hydrique)")
    st.markdown("""
    $$S_{t+1} = S_t + \\eta_I I_t + R_t + G_t - ETc_t - D(S_t) - Q_t$$
    
    $$\\psi_{t+1} = f_{\\text{retention}}(S_{t+1})$$
    
    Cette √©quation d√©crit l'√©volution temporelle de la r√©serve en eau du sol en fonction des flux entrants 
    (irrigation $I_t$, pluie $R_t$, remont√©e capillaire $G_t$) et des flux sortants 
    (√©vapotranspiration $ETc_t$, drainage $D(S_t)$, ruissellement $Q_t$).
    """)
    
    st.markdown("### Contraintes op√©rationnelles")
    st.markdown("""
    - $I_{\\max}$ : dose journali√®re max
    - Quotas d'eau saisonniers
    - Fen√™tres d'irrigation (heures/jours)
    - Pas de temps (journalier ; infra-journalier possible avec CDE)
    """)
    
    st.markdown("### Unit√©s (rappel)")
    st.markdown("""
    - $\\psi$ : cbar
    - $S, I, R, G, D, Q$ : mm
    - $ET0$ : mm/j, $Kc$ : adimensionnel
    """)


def render_doc_apprentissage_renforcement():
    """
    Affiche le contenu de l'onglet de documentation : Apprentissage par Renforcement.
    """
    st.markdown('<h2 class="section-header">ü§ñ Rappel : qu\'est-ce que l\'Apprentissage par Renforcement (RL) ?</h2>', unsafe_allow_html=True)
    
    st.markdown("""
    Le RL mod√©lise un probl√®me de d√©cision s√©quentielle sous forme de MDP (Markov Decision Process).
    
    - **√âtat**: $s_t \\in \\mathcal{S}$ (ici, observables li√©es √† la tension $\\psi_t$ et √† la m√©t√©o)
    - **Action**: $a_t \\in \\mathcal{A}$ (ici, dose d'irrigation $I_t$)
    - **Transition**: $p(s_{t+1}\\mid s_t, a_t)$ (dynamique du sol + m√©t√©o)
    - **R√©compense**: $r_t = r(s_t, a_t)$ (ex. stress hydrique, eau utilis√©e, drainage)
    - **Politique**: $\\pi_\\theta(a\\mid s)$, param√©tr√©e (r√©seau de neurones)
    - **Objectif**: maximiser le retour $J(\\theta)=\\mathbb{E}_\\pi\\!\\left[\\sum_{t=0}^{T}\\gamma^t r_t\\right]$
    """)
    
    st.markdown("### Ce que voit l'agent (exemple d'observation)")
    st.markdown("""
    L'observation $o_t$ que re√ßoit l'agent RL combine les variables d'√©tat physiques d√©crites pr√©c√©demment 
    avec des informations contextuelles :
    
    $$o_t = (\\psi_t,\\ t/T,\\ R_{t-k:t},\\ ET0_t,\\ \\hat{R}_{t+1:t+h},\\ \\widehat{ET0}_{t+1:t+h},\\ \\text{√©vent. } \\psi_{t-k:t-1})$$
    
    O√π :
    - **$\\psi_t$** : tension matricielle actuelle (variable mesur√©e, voir onglet "Variables d'√©tat")
    - **$t/T$** : progression temporelle dans la saison (normalis√©e entre 0 et 1)
    - **$R_{t-k:t}$** : historique de pluie sur les $k$ jours pr√©c√©dents
    - **$ET0_t$** : √©vapotranspiration de r√©f√©rence actuelle
    - **$\\hat{R}_{t+1:t+h}$** : pr√©visions de pluie pour les $h$ prochains jours
    - **$\\widehat{ET0}_{t+1:t+h}$** : pr√©visions d'ET0 pour les $h$ prochains jours
    - **$\\psi_{t-k:t-1}$** (optionnel) : historique de tension pour capturer les tendances
    
    **Normalisation** : Les observations sont g√©n√©ralement standardis√©es ou clipp√©es pour stabiliser l'apprentissage.
    """)
    
    st.markdown("### Espace d'actions")
    st.markdown("""
    L'action $a_t$ de l'agent correspond √† la dose d'irrigation $I_t$ √† appliquer :
    
    - **Continu** (Box): $I_t \\in [0, I_{\\max}]$ (mm) ‚Äî cas le plus r√©aliste
      - Permet des doses pr√©cises et adaptatives
      - $I_{\\max}$ est une contrainte op√©rationnelle (d√©bit maximal du syst√®me)
    
    - **Discret**: choix parmi des doses pr√©-d√©finies (ex. $I_t \\in \\{0, 5, 10, 15, 20\\}$ mm)
      - Plus simple √† impl√©menter mais moins flexible
    
    Le choix entre continu et discret est influenc√© par les contraintes op√©rationnelles et l'impl√©mentation RL.
    """)
    
    st.markdown("### Conception de la r√©compense (exemples)")
    st.markdown("""
    - **Stress hydrique**: p√©naliser les $\\psi_t$ hors zone de confort
      - $r^{stress}_t = -\\alpha\\ \\text{stress}(\\psi_t)$
    - **Eau utilis√©e**: p√©naliser la quantit√© d'irrigation
      - $r^{eau}_t = -\\beta\\, I_t$
    - **Drainage/pertes**: p√©naliser $D(S_t)$
      - $r^{drain}_t = -\\gamma\\, D(S_t)$
    - **Terminaison**: bonus de rendement ‚Äì p√©nalit√© eau
      - $R_{final} = Y - \\lambda \\sum_t I_t$, avec $Y = Y_{\\max} \\exp(-k_{CS}\\sum_t \\text{stress}(\\psi_t))$
    
    R√©compense totale typique: $r_t = r^{stress}_t + r^{eau}_t + r^{drain}_t$, puis ajout de $R_{final}$ en fin d'√©pisode.
    """)
    
    st.markdown("### PPO en bref (Proximal Policy Optimization)")
    st.markdown("""
    - **Type**: on-policy, gradient de politique
    - **Id√©e cl√©**: mise √† jour "proche" de la politique courante via un objectif avec **clipping**
    - **Avantage**: variance r√©duite via l'estimation d'**avantage** $\\hat{A}_t$
    - **Compatibilit√©**: actions continues (Box) et discr√®tes
    - **Stabilit√©**: bonnes propri√©t√©s empiriques, tuning mod√©r√©
    
    Objectif PPO (sch√©matique):
    - Maximiser $\\mathbb{E}_t\\left[\\min\\left(r_t(\\theta)\\hat{A}_t,\\ \\text{clip}(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_t\\right)\\right]$
    - Avec $r_t(\\theta)=\\frac{\\pi_\\theta(a_t\\mid s_t)}{\\pi_{\\theta_{old}}(a_t\\mid s_t)}$, et une perte valeur + entropie
    """)
    
    st.markdown("### Boucle d'apprentissage (sch√©ma)")
    st.markdown("""
    1. **Collecte**: rouler la politique $\\pi_\\theta$ dans l'environnement, stocker $(s_t,a_t,r_t,s_{t+1})$
    2. **Calcul**: retours, avantages (ex. GAE-$\\lambda$)
    3. **Update**: optimiser politique et critique (r√©seau valeur) via PPO
    4. **√âvaluer/valider**: sur seeds/√©pisodes non vus
    5. **R√©p√©ter** jusqu'√† convergence ou budget
    """)
    
    st.markdown("### Bonnes pratiques pour l'irrigation")
    st.markdown("""
    - **Exploration vs exploitation**: contr√¥ler la stochasticit√© (entropie), garder des doses plausibles
    - **Contraintes**: int√©grer $I_{\\max}$, quotas, fen√™tres temporelles (via clipping, masques, p√©nalit√©s)
    - **Robustesse**: randomiser m√©t√©o/sols (domain randomization), g√©rer donn√©es manquantes
    - **Observations**: inclure pr√©visions m√©t√©o, historiques, et indicateurs de confiance
    - **√âchelle temporelle**: choisir $\\Delta t$ (journalier vs infra-journalier), coh√©rent avec le mod√®le (CDE si irr√©gulier)
    - **√âvaluation**: multi-saisons, sc√©narios secs/humides, m√©triques eau, stress, rendement
    """)
    
    st.markdown("### Exemple d'√©tat et de politique (illustratifs)")
    st.markdown("""
    - √âtat: $s_t = (\\psi_t,\\ t/T,\\ R_{t-2:t},\\ ET0_t,\\ \\hat{R}_{t+1:t+2})$
    - Heuristiques utiles que PPO peut apprendre:
      - **Pluie pr√©vue** ‚Üí r√©duire $I_t$
      - **$\\psi_t$ √©lev√© (sol sec)** ‚Üí irriguer
      - **$\\psi_t$ mod√©r√© mais ET0 fort** ‚Üí irrigation pr√©ventive
    
    En r√©sum√©, le RL (et en particulier PPO) offre un cadre pour apprendre automatiquement une strat√©gie d'irrigation 
    tenant compte de la dynamique du sol, des pr√©visions et des co√ªts, tout en g√©rant des espaces d'actions continus 
    et des observations riches.
    """)

# ========================================================================
# ONGLET DOCUMENTATION 4 : SC√âNARIO 2 (RL SUR MOD√àLE PHYSIQUE)
# ========================================================================

def render_doc_scenario2():
    """
    Affiche le contenu de l'onglet de documentation : Sc√©nario 2 (RL sur mod√®le physique).
    """
    st.markdown('<h2 class="section-header">üéì RL sur mod√®le physique (avec tension observ√©e)</h2>', unsafe_allow_html=True)
    
    st.markdown("""
    ### ‚ùì Qu'est-ce que RL sur mod√®le physique ?
    
    Le **RL sur mod√®le physique** impl√©mente un agent d'apprentissage par renforcement (RL) qui apprend une politique d'irrigation optimale
    en interagissant directement avec un environnement simul√© par le mod√®le physique FAO. Contrairement au Sc√©nario 1 qui utilise
    des r√®gles fixes, ce Sc√©nario apprend automatiquement √† optimiser l'irrigation en minimisant le stress hydrique tout en
    √©conomisant l'eau.
    
    **Principe fondamental** : Un agent RL observe la tension matricielle $\\psi_t$ (et le contexte m√©t√©orologique) et choisit
    une dose d'irrigation $I_t$ dans un environnement simul√© par le mod√®le physique. L'agent apprend √† partir de ses interactions
    avec l'environnement pour am√©liorer progressivement sa strat√©gie.
    """)
    
    with st.expander("üî¨ Architecture MDP (Markov Decision Process)", expanded=False):
        st.markdown("""
        Le Sc√©nario 2 mod√©lise le probl√®me d'irrigation comme un MDP :
        
        **1. Espace d'observation** ($\\mathcal{S}$) :
        - Observation standard : $o_t = [\\psi_t, S_t, R_t, ET0_t] \\in \\mathbb{R}^4$
          - $\\psi_t$ : Tension matricielle actuelle (cbar) - variable cl√© mesur√©e par tensiom√®tre
          - $S_t$ : R√©serve en eau du sol (mm)
          - $R_t$ : Pluie du jour (mm)
          - $ET0_t$ : √âvapotranspiration de r√©f√©rence (mm/j)
        - Observation enrichie (optionnelle) : $o_t = (\\psi_t, t/T, R_{t-k:t}, ET0_t, \\hat{R}_{t+1:t+h}, \\widehat{ET0}_{t+1:t+h})$
          - $t/T$ : Progression temporelle dans la saison (normalis√©e)
          - $R_{t-k:t}$ : Historique de pluie sur $k$ jours pr√©c√©dents
          - $\\hat{R}_{t+1:t+h}$ : Pr√©visions de pluie pour les $h$ prochains jours
          - $\\widehat{ET0}_{t+1:t+h}$ : Pr√©visions d'ET0 pour les $h$ prochains jours
        
        **2. Espace d'actions** ($\\mathcal{A}$) :
        - Action continue : $a_t = I_t \\in [0, I_{\\max}]$ (mm)
          - $I_{\\max}$ : Irrigation maximale par jour (contrainte op√©rationnelle)
          - Permet des doses pr√©cises et adaptatives
        
        **3. Fonction de transition** ($p(s_{t+1} | s_t, a_t)$) :
        - Mod√®le physique FAO : $S_{t+1} = f_{\\text{FAO}}(S_t, I_t, R_t, ET0_t, Kc_t)$
        - Conversion : $\\psi_{t+1} = f_{\\text{retention}}(S_{t+1})$
        - Le mod√®le physique garantit la coh√©rence des pr√©dictions
        
        **4. Fonction de r√©compense** ($r_t = r(s_t, a_t)$) :
        - R√©compense journali√®re : $r_t = -\\alpha \\cdot \\text{stress}(\\psi_t) - \\beta \\cdot I_t - \\gamma \\cdot D(S_t)$
          - $\\alpha$ : Poids de la p√©nalit√© de stress hydrique
          - $\\beta$ : Poids de la p√©nalit√© d'irrigation (co√ªt de l'eau)
          - $\\gamma$ : Poids de la p√©nalit√© de drainage (pertes d'eau)
        - R√©compense terminale : $R_{\\text{final}} = Y(\\text{cum\_stress}) - \\lambda_{\\text{water}} \\cdot \\sum_t I_t$
          - $Y$ : Rendement de la culture (d√©cro√Æt avec le stress cumul√©)
          - $\\lambda_{\\text{water}}$ : Poids de la p√©nalit√© d'eau totale
        
        **5. Algorithme RL** :
        - **PPO (Proximal Policy Optimization)** : Algorithme on-policy, gradient de politique
        - Objectif : Maximiser $J(\\theta) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{T} \\gamma^t r_t\\right]$
        - $\\gamma$ : Discount factor (0.99 recommand√© pour planification long terme)
        """)
    
    with st.expander("üéØ Application dans notre projet : Pipeline d'entra√Ænement", expanded=False):
        st.markdown("""
        ### Processus d'entra√Ænement du Sc√©nario 2
        
        **1. G√©n√©ration de l'environnement** :
        - Cr√©ation de l'environnement Gymnasium avec le mod√®le physique FAO
        - Configuration des param√®tres (sol, m√©t√©o, r√©compenses)
        - G√©n√©ration de s√©ries m√©t√©orologiques avec seed pour reproductibilit√©
        
        **2. Initialisation de l'agent PPO** :
        - R√©seau de politique (policy network) : MLP qui mappe observation ‚Üí distribution d'actions
        - R√©seau de valeur (value network) : MLP qui estime $V(s_t)$ pour r√©duire la variance
        - Hyperparam√®tres PPO (learning rate, gamma, GAE lambda, etc.)
        
        **3. Boucle d'entra√Ænement** :
        - **Collecte de donn√©es** :
          - Rollout de la politique actuelle $\\pi_\\theta$ dans l'environnement
          - Stockage des trajectoires : $(s_t, a_t, r_t, s_{t+1})$
          - Calcul des retours et avantages (GAE-$\\lambda$)
        - **Mise √† jour de la politique** :
          - Optimisation de l'objectif PPO avec clipping
          - Mise √† jour du r√©seau de valeur
          - Contr√¥le de l'exploration via coefficient d'entropie
        - **√âvaluation** :
          - Test sur √©pisodes de validation (seeds diff√©rents)
          - Calcul de m√©triques (r√©compense moyenne, longueur d'√©pisode, etc.)
        
        **4. Utilisation du mod√®le entra√Æn√©** :
        - Chargement du mod√®le PPO sauvegard√©
        - √âvaluation sur nouvelles saisons
        - D√©ploiement pour prise de d√©cision en temps r√©el
        """)
    
    with st.expander("üìê Architecture de l'agent PPO", expanded=False):
        st.markdown("""
        ### R√©seau de politique (Policy Network)
        
        **Architecture** :
        - **Type** : MLP (Multi-Layer Perceptron)
        - **Entr√©e** : Observation $o_t \\in \\mathbb{R}^d$ o√π $d$ est la dimension de l'observation (4 par d√©faut)
        - **Couches cach√©es** : 2-3 couches avec 64-256 neurones chacune
        - **Activation** : Tanh ou ReLU
        - **Sortie** : Param√®tres d'une distribution d'actions
          - Pour actions continues : Moyenne $\\mu(o_t)$ et √©cart-type $\\sigma(o_t)$ d'une distribution normale
          - Action √©chantillonn√©e : $a_t \\sim \\mathcal{N}(\\mu(o_t), \\sigma(o_t))$
        
        **Formule** :
        $$
        \\begin{aligned}
        \\mathbf{h}_1 &= \\text{ReLU}(\\text{Linear}_1(o_t)) \\\\
        \\mathbf{h}_2 &= \\text{ReLU}(\\text{Linear}_2(\\mathbf{h}_1)) \\\\
        \\mu_t &= \\text{Linear}_\\mu(\\mathbf{h}_2) \\\\
        \\sigma_t &= \\text{softplus}(\\text{Linear}_\\sigma(\\mathbf{h}_2)) \\\\
        a_t &\\sim \\mathcal{N}(\\mu_t, \\sigma_t)
        \\end{aligned}
        $$
        
        ### R√©seau de valeur (Value Network)
        
        **Architecture** :
        - **Type** : MLP similaire au r√©seau de politique
        - **Entr√©e** : Observation $o_t$
        - **Sortie** : Estimation de la valeur $V(o_t) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{T-t} \\gamma^k r_{t+k} | o_t\\right]$
        
        **R√¥le** :
        - R√©duit la variance de l'estimation du gradient
        - Utilis√© dans le calcul de l'avantage : $\\hat{A}_t = \\delta_t + (\\gamma \\lambda) \\delta_{t+1} + \\ldots$
        - O√π $\\delta_t = r_t + \\gamma V(o_{t+1}) - V(o_t)$ est le TD-error
        
        ### Objectif PPO
        
        L'objectif PPO combine plusieurs termes :
        
        $$
        L^{\\text{PPO}}(\\theta) = \\mathbb{E}_t\\left[L^{\\text{CLIP}}(\\theta) - c_v L^V(\\theta) + c_e H[\\pi_\\theta](o_t)\\right]
        $$
        
        o√π :
        - $L^{\\text{CLIP}}(\\theta) = \\min\\left(r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t\\right)$
        - $r_t(\\theta) = \\frac{\\pi_\\theta(a_t | o_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | o_t)}$ est le ratio de probabilit√©
        - $L^V(\\theta) = (V_\\theta(o_t) - \\hat{V}_t)^2$ est la perte de valeur
        - $H[\\pi_\\theta](o_t)$ est l'entropie de la politique (encourage l'exploration)
        - $c_v$ et $c_e$ sont des coefficients de pond√©ration
        """)
    
    with st.expander("‚öñÔ∏è Avantages et limitations", expanded=False):
        st.markdown("""
        ### ‚úÖ Avantages du Sc√©nario 2
        
        **1. Apprentissage d'une politique optimale** :
        - ‚úÖ **Optimisation automatique** : L'agent RL apprend √† minimiser le stress hydrique tout en √©conomisant l'eau
        - ‚úÖ **Compromis optimal** : Trouve automatiquement le meilleur √©quilibre entre performance agronomique et co√ªt de l'eau
        - ‚úÖ **Strat√©gie adaptative** : S'ajuste selon les conditions m√©t√©orologiques et l'√©tat du sol
        
        **2. Adaptabilit√© aux conditions** :
        - ‚úÖ **Pr√©visions m√©t√©o** : Utilise les pr√©visions de pluie et d'ET0 pour anticiper et ajuster l'irrigation
        - ‚úÖ **Conditions variables** : S'adapte aux variations saisonni√®res et aux √©v√©nements m√©t√©orologiques
        - ‚úÖ **Historique contextuel** : Prend en compte l'historique r√©cent (pluie, tension) pour des d√©cisions inform√©es
        
        **3. Respect de la physique** :
        - ‚úÖ **Mod√®le physique fiable** : Utilise un mod√®le bucket valid√© pour simuler la dynamique du sol
        - ‚úÖ **Courbe de r√©tention** : Respecte la relation $S \\leftrightarrow \\psi$ bas√©e sur les propri√©t√©s p√©dophysiques
        - ‚úÖ **Bilan hydrique coh√©rent** : Les √©quations physiques garantissent la coh√©rence des pr√©dictions
        
        **4. Flexibilit√© des actions** :
        - ‚úÖ **Actions continues** : Permet des doses d'irrigation pr√©cises et gradu√©es (pas seulement 0 ou dose fixe)
        - ‚úÖ **Doses adaptatives** : Ajuste la quantit√© d'eau selon l'intensit√© du stress et les conditions
        - ‚úÖ **Strat√©gie pr√©ventive** : Peut irriguer pr√©ventivement avant que le stress ne devienne critique
        
        **5. Performance sup√©rieure** :
        - ‚úÖ **Efficacit√© de l'eau** : G√©n√©ralement meilleure que les r√®gles fixes en termes de consommation d'eau
        - ‚úÖ **R√©duction du stress** : Maintient mieux la tension dans la zone de confort
        - ‚úÖ **Minimisation du drainage** : Apprend √† √©viter les pertes d'eau par drainage excessif
        
        **6. R√©utilisabilit√©** :
        - ‚úÖ **Mod√®le entra√Æn√©** : Une fois entra√Æn√©, le mod√®le peut √™tre utilis√© sur diff√©rentes saisons
        - ‚úÖ **Transfert possible** : Peut √™tre adapt√© √† d'autres parcelles avec r√©-entra√Ænement
        - ‚úÖ **Am√©lioration continue** : Peut √™tre r√©-entra√Æn√© avec de nouvelles donn√©es pour s'am√©liorer
        
        ### ‚ö†Ô∏è Limitations du Sc√©nario 2
        
        **1. D√©pendance √† la qualit√© du mod√®le physique** :
        - ‚ö†Ô∏è **Biais du mod√®le** : Si le mod√®le bucket a des biais (param√®tres mal calibr√©s, processus n√©glig√©s), 
          la politique apprise sera biais√©e
        - ‚ö†Ô∏è **Erreurs de param√©trisation** : Des erreurs dans les param√®tres du sol ($S_{fc}$, $\\psi_{fc}$, $k_d$) 
          se propagent dans les d√©cisions
        - ‚ö†Ô∏è **Processus non mod√©lis√©s** : Ph√©nom√®nes non captur√©s par le mod√®le (h√©t√©rog√©n√©it√© spatiale, 
          interactions complexes) ne sont pas pris en compte
        
        **2. Phase d'entra√Ænement n√©cessaire** :
        - ‚ö†Ô∏è **Temps d'entra√Ænement** : N√©cessite une phase d'apprentissage (plusieurs milliers de timesteps) 
          avant d'√™tre utilisable
        - ‚ö†Ô∏è **Ressources computationnelles** : Entra√Ænement PPO n√©cessite des ressources CPU/GPU
        - ‚ö†Ô∏è **Expertise technique** : N√©cessite des comp√©tences en RL pour l'entra√Ænement et le r√©glage
        
        **3. Donn√©es d'entra√Ænement** :
        - ‚ö†Ô∏è **Simulation requise** : Besoin de g√©n√©rer des donn√©es de simulation pour l'entra√Ænement
        - ‚ö†Ô∏è **Qualit√© de la simulation** : La qualit√© de l'entra√Ænement d√©pend de la qualit√© de la simulation m√©t√©o
        - ‚ö†Ô∏è **Robustesse** : N√©cessite d'entra√Æner sur plusieurs saisons/sc√©narios pour √™tre robuste
        
        **4. Complexit√© de d√©ploiement** :
        - ‚ö†Ô∏è **Infrastructure** : N√©cessite une infrastructure pour ex√©cuter le mod√®le entra√Æn√©
        - ‚ö†Ô∏è **Maintenance** : Le mod√®le peut n√©cessiter un r√©-entra√Ænement p√©riodique
        - ‚ö†Ô∏è **Interpr√©tabilit√© r√©duite** : Moins interpr√©table que les r√®gles simples (bo√Æte noire)
        
        **5. Hyperparam√®tres √† r√©gler** :
        - ‚ö†Ô∏è **Tuning n√©cessaire** : Nombreux hyperparam√®tres √† ajuster (learning rate, gamma, GAE-$\\lambda$, etc.)
        - ‚ö†Ô∏è **Sensibilit√©** : La performance peut √™tre sensible aux choix d'hyperparam√®tres
        - ‚ö†Ô∏è **Expertise requise** : N√©cessite une compr√©hension du RL pour optimiser les hyperparam√®tres
        
        **6. Stabilit√© de l'apprentissage** :
        - ‚ö†Ô∏è **Convergence** : L'entra√Ænement peut ne pas converger ou converger vers un optimum local
        - ‚ö†Ô∏è **Variabilit√©** : La performance peut varier entre diff√©rentes ex√©cutions d'entra√Ænement
        - ‚ö†Ô∏è **Normalisation** : N√©cessite une normalisation soigneuse des observations et r√©compenses
        
        **7. Observations coh√©rentes** :
        - ‚ö†Ô∏è **Alignement temporel** : N√©cessite que les observations soient align√©es temporellement
        - ‚ö†Ô∏è **Donn√©es manquantes** : Doit g√©rer les cas de donn√©es manquantes ou irr√©guli√®res
        - ‚ö†Ô∏è **Pr√©visions m√©t√©o** : D√©pend de la qualit√© des pr√©visions m√©t√©orologiques disponibles
        """)
    
    with st.expander("üîß Param√®tres recommand√©s et tuning", expanded=False):
        st.markdown("""
        ### Hyperparam√®tres PPO
        
        **Learning rate** : $3 \\times 10^{-4}$ (recommand√©)
        - Trop √©lev√© (> $10^{-3}$) : Instabilit√©, oscillations
        - Trop faible (< $10^{-5}$) : Apprentissage trop lent
        - Tuning : R√©duire si loss oscille, augmenter si convergence lente
        
        **Gamma (discount factor)** : 0.99 (recommand√©)
        - Contr√¥le l'importance des r√©compenses futures
        - √âlev√© (0.99) : Planification √† long terme
        - Faible (0.95) : Focus sur court terme
        
        **GAE lambda** : 0.95 (recommand√©)
        - Contr√¥le le biais/variance de l'estimation de la valeur
        - √âlev√© (0.95-0.99) : Moins de variance, plus de biais
        - Faible (0.8-0.9) : Plus de variance, moins de biais
        
        **Entropy coefficient** : 0.01-0.05
        - Encourage l'exploration
        - √âlev√© : Plus d'exploration, convergence plus lente
        - Faible : Moins d'exploration, risque de sous-optimum local
        
        **Clip range** : 0.2 (standard PPO)
        - Limite les changements de politique
        - √âlev√© : Permet plus de changements, moins stable
        - Faible : Changements limit√©s, plus stable
        
        **Batch size** : 64-256
        - Plus grand : Gradients plus stables mais plus de m√©moire
        - Plus petit : Moins de m√©moire mais gradients plus variables
        
        **Number of steps per rollout** : 2048
        - Plus grand : Meilleure estimation mais plus de m√©moire
        - Plus petit : Moins de m√©moire mais estimation moins pr√©cise
        
        ### Hyperparam√®tres de l'environnement
        
        **Param√®tres de r√©compense** :
        - $\\alpha$ (p√©nalit√© stress) : 1.0 (recommand√©)
        - $\\beta$ (p√©nalit√© irrigation) : 0.05 (recommand√©)
        - $\\gamma$ (p√©nalit√© drainage) : 0.01 (recommand√©)
        - Tuning : Ajuster selon priorit√©s (eau vs stress)
        
        **Param√®tres du sol** :
        - Utiliser les valeurs par d√©faut sauf si donn√©es sp√©cifiques disponibles
        - Calibrer $S_{fc}$, $\\psi_{fc}$ selon mesures r√©elles si possible
        
        ### Strat√©gie de tuning
        
        **1. Commencer avec valeurs par d√©faut** :
        - Utiliser les valeurs recommand√©es ci-dessus
        - Entra√Æner sur 50,000-100,000 timesteps
        
        **2. Observer les m√©triques** :
        - R√©compense moyenne : Doit augmenter
        - Longueur d'√©pisode : Doit √™tre stable
        - Variance des actions : Ne doit pas exploser
        
        **3. Ajuster si n√©cessaire** :
        - Si instabilit√© : R√©duire learning rate, augmenter clip range
        - Si convergence lente : Augmenter learning rate, r√©duire entropy
        - Si sous-optimum : Augmenter entropy, r√©duire clip range
        """)
    
    with st.expander("üß≠ Quand utiliser le Sc√©nario 2 ?", expanded=False):
        st.markdown("""
        ### ‚úÖ Choisir le Sc√©nario 2 si :
        
        - **Optimisation recherch√©e** :
          - Besoin de minimiser la consommation d'eau
          - Recherche du compromis optimal stress/co√ªt
          - Performance sup√©rieure aux r√®gles simples
        
        - **Donn√©es disponibles** :
          - Possibilit√© de g√©n√©rer des simulations pour l'entra√Ænement
          - Mod√®le physique fiable et bien calibr√©
          - Conditions m√©t√©orologiques vari√©es pour robustesse
        
        - **Ressources computationnelles** :
          - Infrastructure disponible pour l'entra√Ænement PPO
          - Temps d'entra√Ænement acceptable (quelques heures)
          - Expertise en RL disponible
        
        - **Adaptabilit√© n√©cessaire** :
          - Conditions variables n√©cessitant adaptation
          - Besoin de strat√©gie pr√©ventive
          - Optimisation selon objectifs multiples
        
        - **Point de d√©part pour approches avanc√©es** :
          - Baseline pour comparer avec Sc√©narios 3-6
          - Validation de l'approche RL avant complexification
        
        ### ‚ùå Ne pas choisir le Sc√©nario 2 si :
        
        - **Simplicit√© prioritaire** :
          - Besoin de solution simple et rapide
          - Pas d'infrastructure d'entra√Ænement
          - R√®gles simples suffisent
        
        - **Mod√®le physique incertain** :
          - Param√®tres du sol mal connus
          - Mod√®le physique non valid√©
          - Donn√©es de simulation de mauvaise qualit√©
        
        - **Donn√©es limit√©es** :
          - Pas de possibilit√© de g√©n√©rer des simulations
          - Conditions trop sp√©cifiques pour g√©n√©raliser
        
        - **Besoin de correction physique** :
          - Mod√®le physique a des biais connus
          - N√©cessit√© de corriger les pr√©dictions physiques
          - ‚Üí Pr√©f√©rer Sc√©narios 3-4
        """)
    
    with st.expander("üõ†Ô∏è Conseils pratiques", expanded=False):
        st.markdown("""
        ### Workflow recommand√©
        
        **1. Pr√©paration** :
        - V√©rifier la coh√©rence m√©t√©o (m√™mes seeds/params que Sc√©nario 1)
        - Valider le mod√®le physique sur quelques √©pisodes
        - Configurer les hyperparam√®tres avec valeurs par d√©faut
        
        **2. Entra√Ænement initial** :
        - Commencer avec 50,000 timesteps
        - Observer les m√©triques (r√©compense, longueur d'√©pisode)
        - V√©rifier la convergence
        
        **3. Tuning it√©ratif** :
        - Ajuster les hyperparam√®tres si n√©cessaire
        - R√©-entra√Æner avec nouveaux param√®tres
        - Comparer les performances
        
        **4. √âvaluation** :
        - Tester sur nouvelles saisons (seeds diff√©rents)
        - Comparer avec Sc√©nario 1 (baseline)
        - Analyser les d√©cisions prises
        
        ### Troubleshooting
        
        **Probl√®me : Instabilit√© de l'entra√Ænement**
        - **Sympt√¥me** : Loss oscille, r√©compense ne converge pas
        - **Solutions** :
          - R√©duire learning rate (ex: $3 \\times 10^{-4} \\to 10^{-4}$)
          - Augmenter clip range (ex: 0.2 ‚Üí 0.3)
          - Normaliser les observations et r√©compenses
        
        **Probl√®me : Convergence lente**
        - **Sympt√¥me** : R√©compense augmente tr√®s lentement
        - **Solutions** :
          - Augmenter learning rate (avec prudence)
          - Augmenter entropy coefficient pour plus d'exploration
          - V√©rifier la normalisation des r√©compenses
        
        **Probl√®me : Sous-optimum local**
        - **Sympt√¥me** : Performance plafonne √† un niveau sous-optimal
        - **Solutions** :
          - Augmenter entropy coefficient
          - R√©duire clip range pour permettre plus de changements
          - Augmenter le nombre de timesteps d'entra√Ænement
        
        **Probl√®me : Politique trop conservatrice**
        - **Sympt√¥me** : Irrigation insuffisante, stress hydrique
        - **Solutions** :
          - Ajuster les poids de r√©compense ($\\alpha$ vs $\\beta$)
          - Augmenter la p√©nalit√© de stress ($\\alpha$)
          - R√©duire la p√©nalit√© d'irrigation ($\\beta$)
        
        ### M√©triques √† surveiller
        
        - **R√©compense moyenne** : Doit augmenter avec l'entra√Ænement
        - **Longueur d'√©pisode** : Doit √™tre stable (‚âà longueur de saison)
        - **Variance des actions** : Ne doit pas exploser (signe d'instabilit√©)
        - **Policy loss** : Doit d√©cro√Ætre et converger
        - **Value loss** : Doit d√©cro√Ætre (estimation de la valeur)
        """)
    
    with st.expander("üîó Comparaison avec les autres sc√©narios", expanded=False):
        st.markdown("""
        ### Sc√©nario 2 vs Sc√©nario 1 (R√®gles simples)
        
        **Sc√©nario 1** :
        - R√®gles fixes, pas d'apprentissage
        - Simple et rapide
        - Performance sous-optimale
        
        **Sc√©nario 2** :
        - Apprentissage automatique
        - Plus complexe mais meilleure performance
        - N√©cessite entra√Ænement
        
        **Quand choisir Sc√©nario 2** : Optimisation et adaptabilit√© recherch√©es
        
        ### Sc√©nario 2 vs Sc√©narios 3-4 (Neural ODE/CDE)
        
        **Sc√©nario 2** :
        - RL direct sur mod√®le physique
        - Pas de correction du mod√®le physique
        - Plus simple
        
        **Sc√©narios 3-4** :
        - Correction r√©siduelle du mod√®le physique
        - Am√©liore la pr√©diction physique
        - Plus complexe
        
        **Quand choisir Sc√©narios 3-4** : Mod√®le physique a des biais connus
        """)
    
    with st.expander("üìä Variables et notations", expanded=False):
        st.markdown("""
        ### Variables principales
        
        **Observations et √©tats** :
        - $o_t = [\\psi_t, S_t, R_t, ET0_t] \\in \\mathbb{R}^4$ : Observation au temps $t$
        - $s_t$ : √âtat du MDP (peut √™tre identique √† $o_t$ ou enrichi)
        - $a_t = I_t \\in [0, I_{\\max}]$ : Action (irrigation) au temps $t$
        - $r_t$ : R√©compense au temps $t$
        
        **Mod√®le physique** :
        - $S_t$ : R√©serve en eau du sol (mm)
        - $\\psi_t$ : Tension matricielle (cbar)
        - $R_t$ : Pluie (mm)
        - $ET0_t$ : √âvapotranspiration de r√©f√©rence (mm/j)
        - $Kc_t$ : Coefficient cultural
        - $ETc_t = Kc_t \\times ET0_t \\times f_{ET}(\\psi_t)$ : √âvapotranspiration culturelle
        - $D(S_t)$ : Drainage (mm)
        
        **Fonctions et mod√®les** :
        - $\\pi_\\theta(a_t | o_t)$ : Politique PPO (distribution d'actions)
        - $V_\\theta(o_t)$ : Fonction de valeur (estimation du retour futur)
        - $g(\\psi_t; \\theta_{\\text{r√®gle}})$ : R√®gle d'irrigation d√©terministe du Sc√©nario 1 (seuil, bande de confort, proportionnelle) qui renvoie $I_t$
        - $f_{\\text{FAO}}(\\cdot)$ : Mod√®le physique FAO
        - $f_{\\text{retention}}(\\cdot)$ : Courbe de r√©tention $S \\leftrightarrow \\psi$
        
        **Entra√Ænement** :
        - $\\theta$ : Param√®tres de la politique et de la fonction de valeur
        - $\\gamma$ : Discount factor (0.99)
        - $\\lambda$ : Param√®tre GAE (0.95)
        - $\\epsilon$ : Clip range (0.2)
        - $\\hat{A}_t$ : Estimation de l'avantage (GAE)
        """)

# ========================================================================
# ONGLET DOCUMENTATION 5 : NEURAL ODE
# ========================================================================

def render_doc_neural_ode():
    """
    Affiche le contenu de l'onglet de documentation : Neural ODE.
    """
    st.markdown('<h2 class="section-header">üß† Neural ODE : Mod√®le hybride physique-neuronal</h2>', unsafe_allow_html=True)
    
    st.markdown("""
    ### ‚ùì Qu'est-ce qu'un Neural ODE ?
    
    Un **Neural ODE** (Neural Ordinary Differential Equation) est un mod√®le qui combine des √©quations diff√©rentielles ordinaires (ODE) avec des r√©seaux de neurones. 
    Dans notre contexte, il s'agit d'un **mod√®le r√©siduel** qui apprend √† corriger les pr√©dictions d'un mod√®le physique.
    """)
    
    with st.expander("üî¨ Principe g√©n√©ral des Neural ODE", expanded=False):
        st.markdown("""
        Les Neural ODE mod√©lisent la dynamique d'un syst√®me par une √©quation diff√©rentielle o√π la d√©riv√©e est apprise par un r√©seau de neurones :
        
        $$
        \\frac{d\\mathbf{z}(t)}{dt} = f_\\theta(\\mathbf{z}(t), t)
        $$
        
        o√π :
        - $\\mathbf{z}(t)$ est l'√©tat du syst√®me au temps $t$
        - $f_\\theta$ est un r√©seau de neurones param√©tr√© par $\\theta$
        - La solution est obtenue par int√©gration num√©rique (m√©thode d'Euler, Runge-Kutta, etc.)
        
        **Avantages** :
        - **Continuit√©** : Mod√©lise des processus continus naturellement
        - **Efficacit√© m√©moire** : Pas besoin de stocker tous les √©tats interm√©diaires
        - **Flexibilit√©** : Le r√©seau apprend la dynamique √† partir des donn√©es
        """)
    
    with st.expander("üéØ Application dans notre projet : Mod√®le hybride", expanded=False):
        st.markdown("""
        Dans notre projet d'irrigation intelligente, le Neural ODE est utilis√© comme **correction r√©siduelle** sur le mod√®le physique FAO.
        L'id√©e est de combiner :
        
        - **Mod√®le physique** : Fournit une pr√©diction de base bas√©e sur les lois de la physique (bilan hydrique)
        - **Neural ODE** : Apprend les √©carts syst√©matiques et les ph√©nom√®nes non mod√©lis√©s par le mod√®le physique
        
        Cette approche hybride permet de :
        - ‚úÖ B√©n√©ficier de la robustesse et de l'interpr√©tabilit√© du mod√®le physique
        - ‚úÖ Capturer les biais syst√©matiques et les ph√©nom√®nes complexes non mod√©lis√©s
        - ‚úÖ S'adapter aux sp√©cificit√©s locales (type de sol, conditions m√©t√©o, etc.)
        
        ### üìã Param√®tres de configuration du pr√©-entra√Ænement
        
        **Nombre de trajectoires** :
        - **Signification** : Nombre de simulations ind√©pendantes utilis√©es pour g√©n√©rer les donn√©es d'entra√Ænement du Neural ODE
        - **Valeur usuelle** : 32 trajectoires (par d√©faut)
        - **Impact** : Plus de trajectoires = plus de diversit√© dans les donn√©es (conditions m√©t√©o vari√©es, strat√©gies d'irrigation diff√©rentes)
        - **Tuning** : Augmenter (50-100) si le mod√®le ne g√©n√©ralise pas bien, r√©duire (10-20) pour acc√©l√©rer l'entra√Ænement
        - **Note** : Chaque trajectoire simule une saison compl√®te (120 jours par d√©faut)
        
        **Nombre d'epochs** :
        - **Signification** : Nombre de passages complets sur l'ensemble des donn√©es d'entra√Ænement
        - **Valeur usuelle** : 10 epochs (par d√©faut)
        - **Impact** : Plus d'epochs = meilleur apprentissage mais risque de surapprentissage
        - **Tuning** : Augmenter (20-50) si la loss continue √† diminuer, r√©duire si la loss stagne ou augmente
        - **Note** : Surveiller la loss de validation pour d√©tecter le surapprentissage
        
        **Taille des batches** :
        - **Signification** : Nombre d'√©chantillons trait√©s simultan√©ment lors de chaque mise √† jour des param√®tres
        - **Valeur usuelle** : 256 (par d√©faut)
        - **Impact** : Batch plus grand = gradients plus stables mais plus de m√©moire requise
        - **Tuning** : R√©duire (32-128) si m√©moire limit√©e, augmenter (512+) si disponible et pour plus de stabilit√©
        - **Note** : Doit √™tre adapt√© √† la taille du dataset (nombre de trajectoires √ó longueur de saison)
        
        **Taux d'apprentissage (Learning Rate)** :
        - **Signification** : Vitesse √† laquelle le mod√®le ajuste ses param√®tres lors de l'optimisation
        - **Valeur usuelle** : $10^{-3}$ (0.001) (par d√©faut)
        - **Impact** : LR trop √©lev√© = instabilit√©, LR trop faible = apprentissage lent
        - **Tuning** : R√©duire (10^{-4} - 10^{-5}) si la loss oscille, augmenter (10^{-2}) si l'apprentissage est trop lent
        - **Note** : Utilise l'optimiseur Adam qui adapte le LR par param√®tre
        """)


def render_doc_neural_ode_cont():
    """
    Affiche le contenu de l'onglet de documentation : Neural ODE continu (Sc√©nario 3b).
    """
    st.markdown('<h2 class="section-header">üß† Neural ODE continu : correction r√©siduelle liss√©e</h2>', unsafe_allow_html=True)

    st.markdown("""
    ### ‚ùì Qu'est-ce qu'un Neural ODE continu ?

    Variante continue du Neural ODE : le r√©seau apprend directement la d√©riv√©e $d\\psi/dt$
    et l'int√®gre sur un pas (1 jour) avec `torchdiffeq` (Runge-Kutta) ou un Euler explicite fallback.
    R√©sultat : corrections $\\Delta \\psi$ plus lisses que la version discr√®te.
    """)

    with st.expander("üî¨ Principe g√©n√©ral", expanded=False):
        st.markdown("""
        $$
        \\frac{d\\psi}{dt} = f_\\theta(\\psi, I, R, ET0) \\quad\\Rightarrow\\quad
        \\Delta\\psi = \\int_{t}^{t+\\Delta t} f_\\theta(\\psi, I, R, ET0)\\, dt
        $$

        - **Int√©gration** : `torchdiffeq.odeint` (rk4) si dispo, sinon Euler multi-sous-pas.
        - **Avantage** : correction temporellement plus r√©guli√®re (moins d'oscillations sur œà et I).
        """)

    with st.expander("üéØ Application hybride (Sc√©nario 3b)", expanded=False):
        st.markdown("""
        - **Base physique** : m√™me bucket FAO que le sc√©nario 3.
        - **Correction continue** : $\\psi_{t+1} = \\psi_{t+1}^{phys} + \\Delta\\psi_{cont}$.
        - **Impact attendu** : transitions plus douces, politique PPO moins sujette aux √†-coups.
        """)

    with st.expander("üß∞ Hyperparam√®tres cl√©s", expanded=False):
        st.markdown("""
        - **Trajectoires (N_traj)** : 32 par d√©faut. Augmenter (50-100) si la loss continue de baisser.
        - **Epochs** : 10 par d√©faut. Monter √† 20-50 si sous-apprentissage.
        - **Batch size** : 256 par d√©faut. R√©duire (64-128) si m√©moire limit√©e.
        - **Learning rate** : $10^{-3}$ par d√©faut. Baisser (1e-4) si la loss oscille.
        - **Solver** : rk4 via `torchdiffeq` si install√© ; sinon Euler avec sous-pas (param√®tre `substeps`).
        """)

    with st.expander("üìê Architecture", expanded=False):
        st.markdown("""
        - **R√©seau f_Œ∏ (dœà/dt)** : MLP 4‚Üí64‚Üí64‚Üí1 avec Tanh.
        - **Entr√©e** : [œà, I, R, ET0].
        - **Sortie** : dœà/dt, int√©gr√© sur Œît=1 jour pour produire Œîœà.
        """)

    with st.expander("üöÄ Int√©gration RL", expanded=False):
        st.markdown("""
        - **Pr√©-entra√Æner** le mod√®le continu sur donn√©es simul√©es (√âtape 1 onglet 3b).
        - **Entra√Æner PPO** sur l'environnement hybride (√âtape 2 onglet 3b) : m√™mes r√©compenses que sc√©narios 2/3.
        - **√âvaluation** : onglet √âvaluation ‚Üí choisir "Sc√©nario 3b", puis Visualisation/Comparaison.
        """)
    
    with st.expander("üìê Architecture du mod√®le hybride", expanded=False):
        st.markdown("""
        ### Architecture du mod√®le hybride
    
    Le mod√®le hybride combine deux composantes :
    
    **a) Pr√©diction physique** :
    
    Le mod√®le physique calcule d'abord la r√©serve en eau $S_{t+1}^{\\text{phys}}$ selon le bilan hydrique :
    
    $$
    S_{t+1}^{\\text{phys}} = S_t + \\eta_I I_t + R_t - ETc_t - D(S_t)
    $$
    
    o√π :
    - $S_t$ : R√©serve en eau au jour $t$ (mm)
    - $\\eta_I$ : Efficacit√© d'irrigation (fraction de l'eau d'irrigation effectivement disponible)
    - $I_t$ : Dose d'irrigation appliqu√©e au jour $t$ (mm)
    - $R_t$ : Pluie au jour $t$ (mm)
    - $ETc_t$ : √âvapotranspiration culture au jour $t$ (mm), calcul√©e comme $ETc_t = Kc_t \\cdot ET0_t \\cdot f_{ET}(\\psi_t)$
    - $D(S_t)$ : Drainage (perte d'eau par percolation) au jour $t$ (mm)
    
    La tension matricielle pr√©dite par le mod√®le physique est ensuite obtenue via la courbe de r√©tention :
    
    $$
    \\psi_{t+1}^{\\text{phys}} = f_{\\text{retention}}(S_{t+1}^{\\text{phys}})
    $$
    
    o√π $f_{\\text{retention}}$ est la fonction de r√©tention d'eau du sol (relation $S \\leftrightarrow \\psi$).
    
    **b) Correction r√©siduelle par Neural ODE** :
    
    Le Neural ODE apprend une correction $\\Delta \\psi_t$ bas√©e sur l'√©tat actuel :
    
    $$
    \\Delta \\psi_t = f_\\theta(\\psi_t, I_t, R_t, ET0_t)
    $$
    
    o√π :
    - $\\psi_t$ : Tension matricielle actuelle (cbar)
    - $I_t$ : Irrigation appliqu√©e (mm)
    - $R_t$ : Pluie (mm)
    - $ET0_t$ : √âvapotranspiration de r√©f√©rence (mm/jour)
    - $f_\\theta$ : R√©seau de neurones (MLP) param√©tr√© par $\\theta$
    
    **c) Pr√©diction finale hybride** :
    
    La pr√©diction finale combine les deux composantes :
    
    $$
    \\psi_{t+1} = \\psi_{t+1}^{\\text{phys}} + \\Delta \\psi_t
    $$
    
    La r√©serve en eau corrig√©e est ensuite obtenue par inversion de la courbe de r√©tention :
    
    $$
    S_{t+1} = f_{\\text{retention}}^{-1}(\\psi_{t+1})
    $$
        """)
        
        with st.expander("üèóÔ∏è Architecture du r√©seau de neurones $f_\\theta$", expanded=False):
            st.markdown("""
            Le r√©seau de neurones $f_\\theta$ est un **MLP (Multi-Layer Perceptron)** avec :
            
            - **Couche d'entr√©e** : 4 neurones (pour $\\psi_t$, $I_t$, $R_t$, $ET0_t$)
            - **Couches cach√©es** : 2 couches de 64 neurones chacune avec activation $\\tanh$
            - **Couche de sortie** : 1 neurone (pour $\\Delta \\psi_t$)
            
            **√âquations du r√©seau** :
            
            $$
            \\mathbf{h}_1 = \\tanh(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1)
            $$
            
            $$
            \\mathbf{h}_2 = \\tanh(\\mathbf{W}_2 \\mathbf{h}_1 + \\mathbf{b}_2)
            $$
            
            $$
            \\Delta \\psi_t = \\mathbf{W}_3 \\mathbf{h}_2 + b_3
            $$
            
            o√π :
            - $\\mathbf{x} = [\\psi_t, I_t, R_t, ET0_t]^T$ est le vecteur d'entr√©e
            - $\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{W}_3$ sont les matrices de poids
            - $\\mathbf{b}_1, \\mathbf{b}_2, b_3$ sont les biais
            - $\\mathbf{h}_1, \\mathbf{h}_2$ sont les activations des couches cach√©es
            """)
    
    with st.expander("üìö Processus d'entra√Ænement", expanded=False):
        st.markdown("""
        ### Processus d'entra√Ænement
        
        Le Neural ODE est entra√Æn√© de mani√®re **supervis√©e** sur des donn√©es de simulation ou r√©elles :
        
        **a) G√©n√©ration des donn√©es d'entra√Ænement** :
        
        Pour chaque pas de temps $t$ d'une simulation, on collecte :
        - **Entr√©es** : $X_t = [\\psi_t, I_t, R_t, ET0_t]$
        - **Cible** : $y_t = \\psi_{t+1}^{\\text{r√©el}} - \\psi_{t+1}^{\\text{phys}}$
        
        o√π $\\psi_{t+1}^{\\text{r√©el}}$ peut √™tre :
        - Une mesure r√©elle de tension (si disponible)
        - Une simulation avec un mod√®le plus sophistiqu√© (HYDRUS, Aquacrop)
        - Une simulation physique avec biais artificiel pour tester la capacit√© de correction
        
        **b) Fonction de perte** :
        
        Le mod√®le est entra√Æn√© pour minimiser l'erreur quadratique moyenne :
        
        $$
        \\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\Delta \\psi_t^{(i)} - y_t^{(i)} \\right)^2
        $$
        
        o√π $N$ est le nombre d'√©chantillons d'entra√Ænement.
        
        **c) Optimisation** :
        
        Les param√®tres $\\theta$ sont optimis√©s via l'algorithme d'Adam avec un learning rate typiquement de $10^{-3}$ √† $10^{-4}$.
        """)
    
    with st.expander("üî¢ M√©thode d'int√©gration : Discr√©tisation temporelle", expanded=False):
        st.markdown("""
        ### M√©thode d'int√©gration : Approche discr√®te
        
        **Important** : Dans notre impl√©mentation, le Neural ODE utilise une **approche discr√®te** plut√¥t qu'une int√©gration continue.
        
        #### Principe de discr√©tisation
        
        Le mod√®le pr√©dit directement la correction r√©siduelle $\\Delta \\psi_t$ sur un **pas de temps fixe de 1 jour** :
        
        $$
        \\Delta \\psi_t = f_\\theta(\\psi_t, I_t, R_t, ET0_t)
        $$
        
        o√π $f_\\theta$ est un r√©seau de neurones (MLP) qui apprend directement la variation de tension sur un jour.
        
        #### Pourquoi une approche discr√®te ?
        
        - **Simplicit√©** : Pas besoin de solveurs d'ODE complexes (Euler, Runge-Kutta, etc.)
        - **Efficacit√©** : Un seul forward pass du r√©seau par pas de temps
        - **Ad√©quation au probl√®me** : Les donn√©es m√©t√©orologiques et les d√©cisions d'irrigation sont disponibles √† l'√©chelle journali√®re
        - **Stabilit√©** : √âvite les probl√®mes num√©riques li√©s aux solveurs d'ODE adaptatifs
        
        #### Comparaison avec une approche continue
        
        Dans un Neural ODE "classique" continu, on mod√©liserait :
        
        $$
        \\frac{d\\psi(t)}{dt} = f_\\theta(\\psi(t), I(t), R(t), ET0(t))
        $$
        
        et on int√©grerait cette √©quation diff√©rentielle avec un solveur (Euler, Runge-Kutta d'ordre 2 ou 4, etc.) :
        
        $$
        \\psi_{t+1} = \\psi_t + \\int_t^{t+1} f_\\theta(\\psi(\\tau), I(\\tau), R(\\tau), ET0(\\tau)) \\, d\\tau
        $$
        
        **Dans notre cas** : Le r√©seau apprend directement la solution discr√©tis√©e :
        
        $$
        \\Delta \\psi_t = \\psi_{t+1} - \\psi_t \\approx f_\\theta(\\psi_t, I_t, R_t, ET0_t)
        $$
        
        o√π l'int√©gration est implicite dans l'apprentissage du r√©seau.
        
        #### Impl√©mentation dans le code
        
        Dans l'environnement RL (`utils_env_modeles.py`), l'inf√©rence est tr√®s simple :
        
        ```python
        # Calcul direct de la correction (pas d'int√©gration)
        x = [psi_t, I_t, R_t, ET0_t]
        delta_psi = residual_ode(x)  # Un seul forward pass
        psi_next = psi_next_phys + delta_psi
        ```
        
        **Note** : Certaines variantes exp√©rimentales dans les notebooks utilisent une m√©thode du trap√®ze (Runge-Kutta d'ordre 2) avec deux √©valuations du r√©seau :
        
        $$
        k_1 = f_\\theta(\\psi_t, I_t, R_t, ET0_t)
        $$
        
        $$
        k_2 = f_\\theta(\\psi_t + 0.5 \\cdot k_1, I_t, R_t, ET0_t)
        $$
        
        $$
        \\Delta \\psi_t = \\frac{k_1 + k_2}{2}
        $$
        
        Cependant, l'impl√©mentation principale dans l'environnement RL utilise la version simple √† un seul pas.
        
        #### Avantages et limites
        
        **Avantages de l'approche discr√®te** :
        - ‚úÖ **Rapidit√©** : Calcul instantan√©, pas de solveur it√©ratif
        - ‚úÖ **Simplicit√©** : Facile √† impl√©menter et d√©boguer
        - ‚úÖ **Stabilit√©** : Pas de probl√®mes de convergence des solveurs
        - ‚úÖ **Ad√©quation** : Correspond √† la granularit√© temporelle des donn√©es disponibles
        
        **Limites** :
        - ‚ö†Ô∏è **Pas de r√©solution infra-journali√®re** : Ne peut pas mod√©liser des ph√©nom√®nes √† l'√©chelle horaire
        - ‚ö†Ô∏è **Pas de pas de temps adaptatif** : Le pas de temps est fixe (1 jour)
        - ‚ö†Ô∏è **Moins pr√©cis pour des dynamiques rapides** : Si des changements importants se produisent en moins d'un jour, ils peuvent √™tre mal captur√©s
        
        #### Alternative : Neural CDE (Sc√©nario 4)
        
        Pour des besoins de mod√©lisation plus sophistiqu√©s avec d√©pendances temporelles, le projet impl√©mente √©galement un **Neural CDE** (Controlled Differential Equation) qui utilise un sch√©ma d'Euler discretis√© sur une s√©quence d'√©tats pass√©s :
        
        $$
        Z_{k+1} = Z_k + f_\\theta(Z_k, X_k) \\cdot \\Delta X_k
        $$
        
        o√π $\\Delta X_k = X_{k+1} - X_k$ et $X_k = [\\psi_k, I_k, R_k, ET0_k]$.
        
        Cette approche capture mieux les d√©pendances temporelles longues mais n√©cessite de maintenir un historique des √©tats.
        """)
    
    with st.expander("ü§ñ Utilisation dans l'environnement RL", expanded=False):
        st.markdown("""
        ### Utilisation dans l'environnement RL
        
        Lors de l'ex√©cution dans l'environnement Gymnasium pour l'apprentissage par renforcement :
        
        **a) Inf√©rence** :
        
        √Ä chaque pas de temps $t$ :
        1. Le mod√®le physique calcule $\\psi_{t+1}^{\\text{phys}}$ √† partir de $S_t$, $I_t$, $R_t$, $ETc_t$, $D_t$
        2. Le Neural ODE calcule $\\Delta \\psi_t = f_\\theta(\\psi_t, I_t, R_t, ET0_t)$ (mode √©valuation, sans gradient)
        3. La pr√©diction finale est $\\psi_{t+1} = \\psi_{t+1}^{\\text{phys}} + \\Delta \\psi_t$
        4. La r√©serve en eau est mise √† jour : $S_{t+1} = f_{\\text{retention}}^{-1}(\\psi_{t+1})$
        
        **b) Avantages pour le RL** :
        
        - **Meilleure pr√©cision** : Le mod√®le hybride capture mieux la dynamique r√©elle du syst√®me
        - **Apprentissage plus efficace** : L'agent RL apprend sur un mod√®le plus fid√®le √† la r√©alit√©
        - **Robustesse** : Le mod√®le physique garantit des pr√©dictions dans des plages physiquement r√©alistes
        - **Adaptabilit√©** : Le Neural ODE peut √™tre r√©-entra√Æn√© avec de nouvelles donn√©es pour s'adapter aux conditions locales
        """)
    
    with st.expander("üìä Variables et notations compl√®tes", expanded=False):
        st.markdown("""
        **Variables d'√©tat** :
        - $S_t$ : R√©serve en eau du sol au jour $t$ (mm)
        - $\\psi_t$ : Tension matricielle de l'eau au jour $t$ (cbar)
        
        **Variables d'action** :
        - $I_t$ : Dose d'irrigation appliqu√©e au jour $t$ (mm)
        
        **Variables m√©t√©orologiques** :
        - $R_t$ : Pluie au jour $t$ (mm)
        - $ET0_t$ : √âvapotranspiration de r√©f√©rence au jour $t$ (mm/jour)
        - $Kc_t$ : Coefficient cultural au jour $t$ (dimensionless)
        - $ETc_t = Kc_t \\cdot ET0_t \\cdot f_{ET}(\\psi_t)$ : √âvapotranspiration culture (mm)
        
        **Variables de perte** :
        - $D(S_t)$ : Drainage (perte par percolation) au jour $t$ (mm)
        - $Q_t$ : Ruissellement au jour $t$ (mm, g√©n√©ralement n√©glig√© dans notre mod√®le)
        
        **Param√®tres du sol** :
        - $\\eta_I$ : Efficacit√© d'irrigation (fraction, typiquement 0.8)
        - $S_{\\max}$ : Capacit√© maximale de stockage (mm)
        - $S_{fc}$ : R√©serve √† la capacit√© au champ (mm)
        - $S_{wp}$ : R√©serve au point de fl√©trissement (mm)
        - $\\psi_{sat}$ : Tension √† saturation (cbar, typiquement ~10 cbar)
        - $\\psi_{wp}$ : Tension au point de fl√©trissement (cbar, typiquement ~150 cbar)
        
        **Fonctions** :
        - $f_{\\text{retention}}(S)$ : Courbe de r√©tention (relation $S \\to \\psi$)
        - $f_{\\text{retention}}^{-1}(\\psi)$ : Inversion de la courbe de r√©tention (relation $\\psi \\to S$)
        - $f_{ET}(\\psi)$ : Fonction de r√©duction de l'√©vapotranspiration selon la tension
        - $f_\\theta(\\psi, I, R, ET0)$ : R√©seau de neurones du Neural ODE
        
        **Corrections r√©siduelles** :
        - $\\Delta \\psi_t$ : Correction r√©siduelle apprise par le Neural ODE (cbar)
        - $\\psi_{t+1}^{\\text{phys}}$ : Pr√©diction du mod√®le physique (cbar)
        - $\\psi_{t+1}$ : Pr√©diction finale hybride (cbar)
        """)
    
    with st.expander("üîÑ Diff√©rence avec Neural CDE", expanded=False):
        st.markdown("""
        Le projet impl√©mente √©galement un **Neural CDE** (Controlled Differential Equation) qui est une extension plus sophistiqu√©e :
        
        - **Neural ODE** : Utilise uniquement l'√©tat actuel $[\\psi_t, I_t, R_t, ET0_t]$ pour pr√©dire $\\Delta \\psi_t$
        - **Neural CDE** : Utilise une s√©quence d'√©tats pass√©s $\\{X_{t-L+1}, \\ldots, X_t\\}$ o√π $X_k = [\\psi_k, I_k, R_k, ET0_k]$
        
        Le Neural CDE capture des d√©pendances temporelles plus longues mais n√©cessite de maintenir un historique des √©tats.
        """)
    
    with st.expander("‚öñÔ∏è Avantages et limitations", expanded=False):
        st.markdown("""
        ### ‚úÖ Avantages du mod√®le hybride Neural ODE
        
        - **Pr√©cision am√©lior√©e** : Capture les biais syst√©matiques du mod√®le physique
        - **Interpr√©tabilit√© pr√©serv√©e** : Le mod√®le physique reste la base, la correction est additive
        - **Efficacit√© computationnelle** : Inf√©rence rapide (un seul forward pass du r√©seau)
        - **Flexibilit√©** : Peut √™tre r√©-entra√Æn√© avec de nouvelles donn√©es
        - **Robustesse** : Le mod√®le physique garantit des pr√©dictions dans des plages r√©alistes
        
        ### ‚ö†Ô∏è Limitations
        
        - **Donn√©es d'entra√Ænement** : N√©cessite des donn√©es pour apprendre la correction
        - **G√©n√©ralisation** : Peut ne pas g√©n√©raliser √† des conditions tr√®s diff√©rentes de l'entra√Ænement
        - **Complexit√©** : Ajoute une couche de complexit√© par rapport au mod√®le physique pur
        """)
    
    with st.expander("üîß Param√®tres recommand√©s et tuning", expanded=False):
        st.markdown("""
        ### Hyperparam√®tres du Neural ODE
        
        **Architecture du r√©seau** :
        - **Nombre de couches cach√©es** : 2-3 (recommand√©)
        - **Dimension cach√©e** : 64-128
        - **Activation** : ReLU ou Tanh
        - **Dimension d'entr√©e** : 4 ($[\\psi_t, I_t, R_t, ET0_t]$)
        - **Dimension de sortie** : 1 ($\\Delta \\psi_t$)
        
        **Pr√©-entra√Ænement** :
        - **Nombre de trajectoires** : 32-50 (recommand√©)
        - **Nombre d'epochs** : 20-50 selon convergence
        - **Batch size** : 64-128
        - **Learning rate** : $10^{-3}$ (recommand√©)
        - **Optimiseur** : Adam
        
        **Hyperparam√®tres PPO** :
        - Identiques au Sc√©nario 2
        - Learning rate : $3 \\times 10^{-4}$
        - Gamma : 0.99
        - GAE lambda : 0.95
        
        ### Strat√©gie de tuning
        
        **1. Pr√©-entra√Ænement du Neural ODE** :
        - G√©n√©rer des trajectoires avec le mod√®le physique
        - Entra√Æner le Neural ODE √† pr√©dire $\\Delta \\psi$
        - V√©rifier que la loss converge (< 0.01 id√©alement)
        
        **2. Entra√Ænement PPO** :
        - Utiliser le Neural ODE pr√©-entra√Æn√©
        - Entra√Æner PPO comme Sc√©nario 2
        - Observer si la correction am√©liore les performances
        
        **3. Ajustements** :
        - Si correction trop faible : Augmenter la capacit√© du r√©seau
        - Si correction instable : R√©duire learning rate, ajouter r√©gularisation
        - Si pas d'am√©lioration : V√©rifier qualit√© des donn√©es d'entra√Ænement
        """)
    
    with st.expander("üß≠ Quand utiliser le Sc√©nario 3 ?", expanded=False):
        st.markdown("""
        ### ‚úÖ Choisir le Sc√©nario 3 si :
        
        - **Biais du mod√®le physique connu** :
          - Param√®tres du sol mal calibr√©s
          - Ph√©nom√®nes non mod√©lis√©s (h√©t√©rog√©n√©it√©, structure du sol)
          - Erreurs syst√©matiques dans les pr√©dictions
        
        - **Donn√©es r√©elles disponibles** :
          - Mesures de tension r√©elles pour entra√Æner la correction
          - Donn√©es historiques suffisantes
          - Qualit√© des donn√©es acceptable
        
        - **Besoin de pr√©cision am√©lior√©e** :
          - Sc√©nario 2 ne suffit pas
          - Besoin de corriger les biais du mod√®le physique
          - Performance sup√©rieure recherch√©e
        
        - **Interpr√©tabilit√© importante** :
          - Besoin de s√©parer physique et correction
          - Analyse des biais du mod√®le physique
          - Compr√©hension des ph√©nom√®nes non mod√©lis√©s
        
        ### ‚ùå Ne pas choisir le Sc√©nario 3 si :
        
        - **Mod√®le physique fiable** :
          - Pas de biais connus
          - Param√®tres bien calibr√©s
          - Sc√©nario 2 suffit
        
        - **Pas de donn√©es r√©elles** :
          - Impossible d'entra√Æner la correction
          - Seulement des simulations disponibles
          - ‚Üí Pr√©f√©rer Sc√©nario 2 ou 5
        
        - **Simplicit√© recherch√©e** :
          - Pas besoin de correction
          - Approche simple suffit
          - ‚Üí Pr√©f√©rer Sc√©nario 1 ou 2
        
        - **D√©pendances temporelles longues** :
          - Besoin de m√©moire temporelle
          - ‚Üí Pr√©f√©rer Sc√©nario 4 (Neural CDE)
        """)
    
    with st.expander("üõ†Ô∏è Conseils pratiques", expanded=False):
        st.markdown("""
        ### Workflow recommand√©
        
        **1. Validation du mod√®le physique** :
        - Comparer pr√©dictions physiques avec donn√©es r√©elles
        - Identifier les biais syst√©matiques
        - Quantifier l'erreur √† corriger
        
        **2. Pr√©-entra√Ænement Neural ODE** :
        - G√©n√©rer trajectoires avec mod√®le physique
        - Calculer $\\Delta \\psi = \\psi_{\\text{r√©el}} - \\psi_{\\text{physique}}$
        - Entra√Æner Neural ODE √† pr√©dire $\\Delta \\psi$
        - V√©rifier convergence (loss < 0.01)
        
        **3. Entra√Ænement PPO** :
        - Int√©grer Neural ODE dans l'environnement
        - Entra√Æner PPO comme Sc√©nario 2
        - Comparer performances avec Sc√©nario 2
        
        **4. Analyse** :
        - Analyser la correction apprise
        - Identifier quels ph√©nom√®nes sont corrig√©s
        - V√©rifier la coh√©rence physique
        
        ### Troubleshooting
        
        **Probl√®me : Correction trop faible**
        - **Sympt√¥me** : $\\Delta \\psi$ toujours proche de 0
        - **Solutions** :
          - Augmenter capacit√© du r√©seau (plus de couches/neurones)
          - V√©rifier qualit√© des donn√©es d'entra√Ænement
          - Augmenter nombre d'epochs
        
        **Probl√®me : Correction instable**
        - **Sympt√¥me** : $\\Delta \\psi$ oscille, pr√©dictions erratiques
        - **Solutions** :
          - R√©duire learning rate
          - Ajouter r√©gularisation (L2, dropout)
          - R√©duire capacit√© du r√©seau
        
        **Probl√®me : Pas d'am√©lioration vs Sc√©nario 2**
        - **Sympt√¥me** : Performance similaire ou pire
        - **Solutions** :
          - V√©rifier que le Neural ODE est bien pr√©-entra√Æn√©
          - V√©rifier qualit√© des donn√©es
          - Augmenter nombre de trajectoires d'entra√Ænement
        """)
    
    with st.expander("üîó Comparaison avec les autres sc√©narios", expanded=False):
        st.markdown("""
        ### Sc√©nario 3 vs Sc√©nario 2 (RL basique)
        
        **Sc√©nario 2** :
        - RL direct sur mod√®le physique
        - Pas de correction du mod√®le
        
        **Sc√©nario 3** :
        - Correction r√©siduelle du mod√®le physique
        - Am√©liore la pr√©diction
        
        **Quand choisir Sc√©nario 3** : Biais du mod√®le physique connus
        
        ### Sc√©nario 3 vs Sc√©nario 4 (Neural CDE)
        
        **Sc√©nario 3** :
        - Neural ODE : Pas de m√©moire temporelle
        - Correction locale bas√©e sur √©tat actuel
        
        **Sc√©nario 4** :
        - Neural CDE : M√©moire temporelle
        - Correction bas√©e sur historique
        
        **Quand choisir Sc√©nario 4** : Besoin de d√©pendances temporelles
        """)
    
    # ========================================================================
    # ONGLET DOCUMENTATION 5 : NEURAL CDE
    # ========================================================================

def render_doc_neural_cde():
    """
    Affiche le contenu de l'onglet de documentation : Neural CDE.
    """
    st.markdown('<h2 class="section-header">üåÄ Neural CDE : Mod√®le hybride avec d√©pendances temporelles</h2>', unsafe_allow_html=True)
    
    st.markdown("""
    ### ‚ùì Qu'est-ce qu'un Neural CDE ?
    
    Un **Neural CDE** (Neural Controlled Differential Equation) est une extension des Neural ODE qui utilise une s√©quence d'√©tats pass√©s pour capturer des d√©pendances temporelles plus longues. 
    Dans notre contexte, il s'agit d'un **mod√®le r√©siduel** qui apprend √† corriger les pr√©dictions d'un mod√®le physique en exploitant l'historique des √©tats.
    """)
    
    with st.expander("üî¨ Principe g√©n√©ral des Neural CDE", expanded=False):
        st.markdown("""
        Les Neural CDE mod√©lisent la dynamique d'un syst√®me par une √©quation diff√©rentielle contr√¥l√©e o√π la d√©riv√©e d√©pend d'une s√©quence d'√©tats pass√©s :
        
        $$
        dZ_t = f_\\theta(Z_t, X_t) \\, dX_t
        $$
        
        o√π :
        - $Z_t$ est l'√©tat latent du syst√®me au temps $t$
        - $X_t$ est le processus de contr√¥le (s√©quence d'observations)
        - $f_\\theta$ est un r√©seau de neurones param√©tr√© par $\\theta$
        - L'int√©gration se fait le long du chemin de $X_t$
        
        **Diff√©rence avec Neural ODE** :
        - **Neural ODE** : $\\frac{dZ(t)}{dt} = f_\\theta(Z(t), t)$ d√©pend uniquement de l'√©tat actuel
        - **Neural CDE** : $dZ_t = f_\\theta(Z_t, X_t) \\, dX_t$ d√©pend de la s√©quence compl√®te $\\{X_s : s \\leq t\\}$
        
        **Avantages** :
        - **D√©pendances temporelles** : Capture des effets √† long terme et des dynamiques complexes
        - **Robustesse** : G√®re mieux les donn√©es irr√©guli√®res ou manquantes
        - **Flexibilit√©** : Peut mod√©liser des processus avec m√©moire longue
        """)
    
    with st.expander("üéØ Application dans notre projet : Mod√®le hybride avec m√©moire", expanded=False):
        st.markdown("""
        Dans notre projet d'irrigation intelligente, le Neural CDE est utilis√© comme **correction r√©siduelle** sur le mod√®le physique FAO, 
        mais avec une **m√©moire temporelle** pour capturer des effets √† long terme :
        
        - **Mod√®le physique** : Fournit une pr√©diction de base bas√©e sur les lois de la physique (bilan hydrique)
        - **Neural CDE** : Apprend les √©carts syst√©matiques en utilisant une s√©quence d'√©tats pass√©s
        
        Cette approche hybride permet de :
        - ‚úÖ B√©n√©ficier de la robustesse et de l'interpr√©tabilit√© du mod√®le physique
        - ‚úÖ Capturer les biais syst√©matiques et les ph√©nom√®nes complexes non mod√©lis√©s
        - ‚úÖ Mod√©liser des d√©pendances temporelles longues (effets cumulatifs de l'irrigation, s√©cheresses prolong√©es, etc.)
        - ‚úÖ S'adapter aux sp√©cificit√©s locales avec une meilleure compr√©hension de l'historique
        
        ### üìã Param√®tres de configuration du pr√©-entra√Ænement
        
        **Nombre de trajectoires** :
        - **Signification** : Nombre de simulations ind√©pendantes utilis√©es pour g√©n√©rer les donn√©es d'entra√Ænement du Neural CDE
        - **Valeur usuelle** : 32 trajectoires (par d√©faut)
        - **Impact** : Plus de trajectoires = plus de diversit√© dans les s√©quences temporelles
        - **Tuning** : Augmenter (50-100) pour capturer plus de variabilit√©, r√©duire (10-20) pour acc√©l√©rer
        - **Note** : Important pour avoir suffisamment de s√©quences vari√©es pour l'apprentissage s√©quentiel
        
        **Nombre d'epochs** :
        - **Signification** : Nombre de passages complets sur l'ensemble des donn√©es d'entra√Ænement
        - **Valeur usuelle** : 10 epochs (par d√©faut)
        - **Impact** : Plus d'epochs = meilleur apprentissage des d√©pendances temporelles
        - **Tuning** : Augmenter (20-50) si le mod√®le continue √† apprendre, r√©duire si surapprentissage
        - **Note** : Les mod√®les s√©quentiels peuvent n√©cessiter plus d'epochs que les mod√®les sans m√©moire
        
        **Taille des batches** :
        - **Signification** : Nombre de s√©quences trait√©es simultan√©ment lors de chaque mise √† jour
        - **Valeur usuelle** : 256 (par d√©faut)
        - **Impact** : Batch plus grand = gradients plus stables pour les s√©quences
        - **Tuning** : R√©duire (32-128) si m√©moire limit√©e, augmenter (512+) pour plus de stabilit√©
        - **Note** : Doit tenir compte de la longueur de s√©quence (plus de m√©moire requise)
        
        **Taux d'apprentissage (Learning Rate)** :
        - **Signification** : Vitesse d'ajustement des param√®tres lors de l'optimisation
        - **Valeur usuelle** : $10^{-3}$ (0.001) (par d√©faut)
        - **Impact** : LR trop √©lev√© = instabilit√© dans l'apprentissage s√©quentiel
        - **Tuning** : R√©duire (10^{-4} - 10^{-5}) si la loss oscille, augmenter (10^{-2}) si trop lent
        - **Note** : Les mod√®les s√©quentiels sont souvent plus sensibles au LR que les mod√®les sans m√©moire
        
        **Longueur de s√©quence** :
        - **Signification** : Nombre de pas de temps dans l'historique utilis√© par le Neural CDE
        - **Valeur usuelle** : 5 pas de temps (par d√©faut)
        - **Impact** : S√©quence plus longue = capture de d√©pendances plus longues mais plus de complexit√©
        - **Tuning** : Augmenter (7-10) pour effets cumulatifs longs, r√©duire (3-4) pour d√©pendances courtes
        - **Note** : Doit √™tre adapt√© √† la nature des d√©pendances temporelles du probl√®me (effets d'irrigation sur plusieurs jours)
        """)
    
    with st.expander("üìê Architecture du mod√®le hybride Neural CDE", expanded=False):
        st.markdown("""
        ### Architecture du mod√®le hybride Neural CDE
    
    Le mod√®le hybride combine deux composantes, avec une approche s√©quentielle pour la correction :
    
    **a) Pr√©diction physique** :
    
    Le mod√®le physique calcule d'abord la r√©serve en eau $S_{t+1}^{\\text{phys}}$ selon le bilan hydrique :
    
    $$
    S_{t+1}^{\\text{phys}} = S_t + \\eta_I I_t + R_t - ETc_t - D(S_t)
    $$
    
    o√π :
    - $S_t$ : R√©serve en eau au jour $t$ (mm)
    - $\\eta_I$ : Efficacit√© d'irrigation (fraction de l'eau d'irrigation effectivement disponible)
    - $I_t$ : Dose d'irrigation appliqu√©e au jour $t$ (mm)
    - $R_t$ : Pluie au jour $t$ (mm)
    - $ETc_t$ : √âvapotranspiration culture au jour $t$ (mm), calcul√©e comme $ETc_t = Kc_t \\cdot ET0_t \\cdot f_{ET}(\\psi_t)$
    - $D(S_t)$ : Drainage (perte d'eau par percolation) au jour $t$ (mm)
    
    La tension matricielle pr√©dite par le mod√®le physique est ensuite obtenue via la courbe de r√©tention :
    
    $$
    \\psi_{t+1}^{\\text{phys}} = f_{\\text{retention}}(S_{t+1}^{\\text{phys}})
    $$
    
    o√π $f_{\\text{retention}}$ est la fonction de r√©tention d'eau du sol (relation $S \\leftrightarrow \\psi$).
    
    **b) Correction r√©siduelle par Neural CDE** :
    
    Le Neural CDE apprend une correction $\\Delta \\psi_t$ bas√©e sur une **s√©quence d'√©tats pass√©s** :
    
    $$
    X_k = [\\psi_k, I_k, R_k, ET0_k] \\quad \\text{pour } k \\in \\{t-L+1, \\ldots, t\\}
    $$
    
    o√π $L$ est la longueur de la s√©quence (typiquement $L = 5$ jours).
    
    Le mod√®le utilise un sch√©ma d'Euler discretis√© pour int√©grer l'√©quation diff√©rentielle contr√¥l√©e :
    
    $$
    Z_{k+1} = Z_k + f_\\theta(Z_k, X_k) \\cdot \\Delta X_k
    $$
    
    o√π :
    - $Z_k$ est l'√©tat latent au pas $k$
    - $\\Delta X_k = X_{k+1} - X_k$ est l'incr√©ment du processus de contr√¥le
    - $f_\\theta$ est un r√©seau de neurones (MLP) param√©tr√© par $\\theta$
    
    La correction finale est obtenue √† partir de l'√©tat latent final :
    
    $$
    \\Delta \\psi_t = g_\\phi(Z_t)
    $$
    
    o√π $g_\\phi$ est une couche de sortie qui mappe l'√©tat latent vers la correction r√©siduelle.
    
    **c) Pr√©diction finale hybride** :
    
    La pr√©diction finale combine les deux composantes :
    
    $$
    \\psi_{t+1} = \\psi_{t+1}^{\\text{phys}} + \\Delta \\psi_t
    $$
    
    La r√©serve en eau corrig√©e est ensuite obtenue par inversion de la courbe de r√©tention :
    
    $$
    S_{t+1} = f_{\\text{retention}}^{-1}(\\psi_{t+1})
    $$
        """)
        
        with st.expander("üèóÔ∏è Architecture du r√©seau de neurones $f_\\theta$ et $g_\\phi$", expanded=False):
            st.markdown("""
            Le r√©seau de neurones $f_\\theta$ est un **MLP (Multi-Layer Perceptron)** avec :
            
            - **Couche d'entr√©e** : $\\dim(Z_k) + 4$ neurones (pour l'√©tat latent $Z_k$ et $X_k = [\\psi_k, I_k, R_k, ET0_k]$)
            - **Couches cach√©es** : 2 couches de 64 neurones chacune avec activation $\\tanh$
            - **Couche de sortie** : $\\dim(Z_k)$ neurones (pour la mise √† jour de l'√©tat latent)
            
            La couche de sortie $g_\\phi$ est un MLP simple :
            
            - **Couche d'entr√©e** : $\\dim(Z_t)$ neurones (pour l'√©tat latent final $Z_t$)
            - **Couche cach√©e** : 1 couche de 32 neurones avec activation $\\tanh$
            - **Couche de sortie** : 1 neurone (pour $\\Delta \\psi_t$)
            
            **√âquations du r√©seau** :
            
            Pour $f_\\theta$ :
            $$
            \\mathbf{h}_1 = \\tanh(\\mathbf{W}_1 [Z_k, X_k] + \\mathbf{b}_1)
            $$
            
            $$
            \\mathbf{h}_2 = \\tanh(\\mathbf{W}_2 \\mathbf{h}_1 + \\mathbf{b}_2)
            $$
            
            $$
            \\Delta Z_k = \\mathbf{W}_3 \\mathbf{h}_2 + \\mathbf{b}_3
            $$
            
            Pour $g_\\phi$ :
            $$
            \\mathbf{h}_{out} = \\tanh(\\mathbf{W}_{out} Z_t + \\mathbf{b}_{out})
            $$
            
            $$
            \\Delta \\psi_t = \\mathbf{W}_{final} \\mathbf{h}_{out} + b_{final}
            $$
            
            o√π :
            - $[Z_k, X_k]$ est la concat√©nation de l'√©tat latent et de l'observation
            - $\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{W}_3, \\mathbf{W}_{out}, \\mathbf{W}_{final}$ sont les matrices de poids
            - $\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{b}_3, \\mathbf{b}_{out}, b_{final}$ sont les biais
            """)
    
    with st.expander("üìö Processus d'entra√Ænement", expanded=False):
        st.markdown("""
        ### Processus d'entra√Ænement
        
        Le Neural CDE est entra√Æn√© de mani√®re **supervis√©e** sur des s√©quences de donn√©es de simulation ou r√©elles :
        
        **a) G√©n√©ration des donn√©es d'entra√Ænement** :
        
        Pour chaque pas de temps $t$ d'une simulation, on collecte :
        - **Entr√©es** : S√©quence $\\{X_{t-L+1}, \\ldots, X_t\\}$ o√π $X_k = [\\psi_k, I_k, R_k, ET0_k]$
        - **Cible** : $y_t = \\psi_{t+1}^{\\text{r√©el}} - \\psi_{t+1}^{\\text{phys}}$
        
        o√π $\\psi_{t+1}^{\\text{r√©el}}$ peut √™tre :
        - Une mesure r√©elle de tension (si disponible)
        - Une simulation avec un mod√®le plus sophistiqu√© (HYDRUS, Aquacrop)
        - Une simulation physique avec biais artificiel pour tester la capacit√© de correction
        
        **b) Fonction de perte** :
        
        Le mod√®le est entra√Æn√© pour minimiser l'erreur quadratique moyenne sur les s√©quences :
        
        $$
        \\mathcal{L}(\\theta, \\phi) = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\Delta \\psi_t^{(i)} - y_t^{(i)} \\right)^2
        $$
        
        o√π $N$ est le nombre d'√©chantillons d'entra√Ænement (s√©quences).
        
        **c) Optimisation** :
        
        Les param√®tres $\\theta$ et $\\phi$ sont optimis√©s via l'algorithme d'Adam avec un learning rate typiquement de $10^{-3}$ √† $10^{-4}$.
        L'entra√Ænement se fait par mini-batches de s√©quences.
        """)
    
    with st.expander("üî¢ M√©thode d'int√©gration : Sch√©ma d'Euler discretis√©", expanded=False):
        st.markdown("""
        ### M√©thode d'int√©gration : Approche discr√®te avec s√©quence
        
        **Important** : Dans notre impl√©mentation, le Neural CDE utilise un **sch√©ma d'Euler discretis√©** sur une s√©quence d'√©tats pass√©s.
        
        #### Principe d'int√©gration s√©quentielle
        
        Le mod√®le int√®gre l'√©quation diff√©rentielle contr√¥l√©e sur une s√©quence de $L$ pas de temps :
        
        $$
        Z_{k+1} = Z_k + f_\\theta(Z_k, X_k) \\cdot \\Delta X_k \\quad \\text{pour } k \\in \\{t-L+1, \\ldots, t-1\\}
        $$
        
        o√π :
        - $Z_{t-L+1} = 0$ (√©tat latent initialis√© √† z√©ro)
        - $\\Delta X_k = X_{k+1} - X_k$ est l'incr√©ment du processus de contr√¥le
        - $f_\\theta(Z_k, X_k)$ est √©valu√© √† chaque pas $k$
        
        La correction finale est obtenue √† partir de l'√©tat latent final :
        
        $$
        \\Delta \\psi_t = g_\\phi(Z_t)
        $$
        
        #### Pourquoi une approche s√©quentielle ?
        
        - **D√©pendances temporelles** : Capture les effets cumulatifs et les dynamiques √† long terme
        - **M√©moire** : Maintient un historique des √©tats pour mieux pr√©dire les corrections
        - **Robustesse** : G√®re mieux les variations temporelles complexes
        - **Ad√©quation au probl√®me** : Les effets de l'irrigation et de la m√©t√©o peuvent avoir des impacts sur plusieurs jours
        
        #### Comparaison avec Neural ODE
        
        **Neural ODE** :
        - Utilise uniquement l'√©tat actuel : $\\Delta \\psi_t = f_\\theta(\\psi_t, I_t, R_t, ET0_t)$
        - Pas de m√©moire temporelle
        - Plus simple et plus rapide
        
        **Neural CDE** :
        - Utilise une s√©quence d'√©tats : $\\{X_{t-L+1}, \\ldots, X_t\\}$
        - Capture des d√©pendances temporelles longues
        - Plus complexe mais plus expressif
        
        #### Impl√©mentation dans le code
        
        Dans l'environnement RL (`utils_env_modeles.py`), l'inf√©rence se fait s√©quentiellement :
        
        ```python
        # Initialisation
        Z = torch.zeros(hidden_dim)
        X_prev = X_sequence[0]  # Premier √©tat de la s√©quence
        
        # Int√©gration s√©quentielle
        for k in range(1, seq_len):
            X_k = X_sequence[k]
            dX = X_k - X_prev
            dZ = f_theta(Z, X_k) * dX
            Z = Z + dZ
            X_prev = X_k
        
        # Correction finale
        delta_psi = g_phi(Z)
        psi_next = psi_next_phys + delta_psi
        ```
        
        #### Avantages et limites
        
        **Avantages de l'approche s√©quentielle** :
        - ‚úÖ **D√©pendances temporelles** : Capture des effets √† long terme
        - ‚úÖ **M√©moire** : Exploite l'historique des √©tats
        - ‚úÖ **Expressivit√©** : Mod√©lise des dynamiques complexes
        - ‚úÖ **Robustesse** : G√®re mieux les variations temporelles
        
        **Limites** :
        - ‚ö†Ô∏è **Complexit√© computationnelle** : N√©cessite $L$ √©valuations du r√©seau par pr√©diction
        - ‚ö†Ô∏è **M√©moire** : Doit maintenir un historique de $L$ √©tats
        - ‚ö†Ô∏è **Temps d'entra√Ænement** : Plus long que Neural ODE
        - ‚ö†Ô∏è **Hyperparam√®tres** : N√©cessite de choisir $L$ (longueur de s√©quence)
        """)
    
    with st.expander("ü§ñ Utilisation dans l'environnement RL", expanded=False):
        st.markdown("""
        ### Utilisation dans l'environnement RL
        
        Lors de l'ex√©cution dans l'environnement Gymnasium pour l'apprentissage par renforcement :
        
        **a) Inf√©rence** :
        
        √Ä chaque pas de temps $t$ :
        1. Le mod√®le physique calcule $\\psi_{t+1}^{\\text{phys}}$ √† partir de $S_t$, $I_t$, $R_t$, $ETc_t$, $D_t$
        2. Le Neural CDE utilise la s√©quence $\\{X_{t-L+1}, \\ldots, X_t\\}$ pour calculer $\\Delta \\psi_t$ (mode √©valuation, sans gradient)
        3. La pr√©diction finale est $\\psi_{t+1} = \\psi_{t+1}^{\\text{phys}} + \\Delta \\psi_t$
        4. La r√©serve en eau est mise √† jour : $S_{t+1} = f_{\\text{retention}}^{-1}(\\psi_{t+1})$
        5. L'historique est mis √† jour : $X_{t+1} = [\\psi_{t+1}, I_{t+1}, R_{t+1}, ET0_{t+1}]$
        
        **b) Avantages pour le RL** :
        
        - **Meilleure pr√©cision** : Le mod√®le hybride capture mieux la dynamique r√©elle avec m√©moire temporelle
        - **Apprentissage plus efficace** : L'agent RL apprend sur un mod√®le plus fid√®le √† la r√©alit√©
        - **Robustesse** : Le mod√®le physique garantit des pr√©dictions dans des plages physiquement r√©alistes
        - **Adaptabilit√©** : Le Neural CDE peut √™tre r√©-entra√Æn√© avec de nouvelles donn√©es pour s'adapter aux conditions locales
        - **D√©pendances temporelles** : Capture mieux les effets cumulatifs de l'irrigation et de la m√©t√©o
        """)
    
    with st.expander("üìä Variables et notations compl√®tes", expanded=False):
        st.markdown("""
        **Variables d'√©tat** :
        - $S_t$ : R√©serve en eau du sol au jour $t$ (mm)
        - $\\psi_t$ : Tension matricielle de l'eau au jour $t$ (cbar)
        - $Z_t$ : √âtat latent du Neural CDE au jour $t$ (vecteur de dimension $d$)
        
        **Variables d'action** :
        - $I_t$ : Dose d'irrigation appliqu√©e au jour $t$ (mm)
        
        **Variables m√©t√©orologiques** :
        - $R_t$ : Pluie au jour $t$ (mm)
        - $ET0_t$ : √âvapotranspiration de r√©f√©rence au jour $t$ (mm/jour)
        - $Kc_t$ : Coefficient cultural au jour $t$ (dimensionless)
        - $ETc_t = Kc_t \\cdot ET0_t \\cdot f_{ET}(\\psi_t)$ : √âvapotranspiration culture (mm)
        
        **Variables de perte** :
        - $D(S_t)$ : Drainage (perte par percolation) au jour $t$ (mm)
        - $Q_t$ : Ruissellement au jour $t$ (mm, g√©n√©ralement n√©glig√© dans notre mod√®le)
        
        **Param√®tres du sol** :
        - $\\eta_I$ : Efficacit√© d'irrigation (fraction, typiquement 0.8)
        - $S_{\\max}$ : Capacit√© maximale de stockage (mm)
        - $S_{fc}$ : R√©serve √† la capacit√© au champ (mm)
        - $S_{wp}$ : R√©serve au point de fl√©trissement (mm)
        - $\\psi_{sat}$ : Tension √† saturation (cbar, typiquement ~10 cbar)
        - $\\psi_{wp}$ : Tension au point de fl√©trissement (cbar, typiquement ~150 cbar)
        
        **Param√®tres du Neural CDE** :
        - $L$ : Longueur de la s√©quence d'√©tats (typiquement $L = 5$ jours)
        - $d$ : Dimension de l'√©tat latent $Z_t$ (typiquement $d = 32$)
        
        **Fonctions** :
        - $f_{\\text{retention}}(S)$ : Courbe de r√©tention (relation $S \\to \\psi$)
        - $f_{\\text{retention}}^{-1}(\\psi)$ : Inversion de la courbe de r√©tention (relation $\\psi \\to S$)
        - $f_{ET}(\\psi)$ : Fonction de r√©duction de l'√©vapotranspiration selon la tension
        - $f_\\theta(Z, X)$ : R√©seau de neurones du Neural CDE pour la dynamique de l'√©tat latent
        - $g_\\phi(Z)$ : R√©seau de neurones du Neural CDE pour la correction r√©siduelle
        
        **S√©quences** :
        - $X_k = [\\psi_k, I_k, R_k, ET0_k]$ : Vecteur d'observation au pas $k$
        - $\\{X_{t-L+1}, \\ldots, X_t\\}$ : S√©quence d'observations utilis√©e pour la pr√©diction
        - $\\Delta X_k = X_{k+1} - X_k$ : Incr√©ment du processus de contr√¥le
        
        **Corrections r√©siduelles** :
        - $\\Delta \\psi_t$ : Correction r√©siduelle apprise par le Neural CDE (cbar)
        - $\\psi_{t+1}^{\\text{phys}}$ : Pr√©diction du mod√®le physique (cbar)
        - $\\psi_{t+1}$ : Pr√©diction finale hybride (cbar)
        """)
    
    with st.expander("üîÑ Diff√©rence avec Neural ODE", expanded=False):
        st.markdown("""
        **Neural ODE** :
        - Utilise uniquement l'√©tat actuel : $\\Delta \\psi_t = f_\\theta(\\psi_t, I_t, R_t, ET0_t)$
        - Pas de m√©moire temporelle
        - Plus simple et plus rapide
        - Ad√©quat pour des dynamiques √† court terme
        
        **Neural CDE** :
        - Utilise une s√©quence d'√©tats : $\\{X_{t-L+1}, \\ldots, X_t\\}$ o√π $X_k = [\\psi_k, I_k, R_k, ET0_k]$
        - Capture des d√©pendances temporelles longues
        - Plus complexe mais plus expressif
        - Ad√©quat pour des dynamiques √† long terme et des effets cumulatifs
        
        **Quand utiliser Neural CDE ?**
        - Effets cumulatifs de l'irrigation sur plusieurs jours
        - S√©cheresses prolong√©es avec impacts retard√©s
        - Dynamiques complexes n√©cessitant une m√©moire temporelle
        - Donn√©es avec d√©pendances temporelles importantes
        """)
    
    with st.expander("üî¨ Relation avec les Physics-Informed Neural Networks (PINN)", expanded=False):
        st.markdown("""
        ### Neural CDE comme mod√®le Physics-Informed
        
        Le Neural CDE utilis√© dans ce projet peut √™tre consid√©r√© comme un **mod√®le hybride physics-informed avec m√©moire temporelle** :
        
        **D√©finition des PINN** :
        Les Physics-Informed Neural Networks (PINN) sont des r√©seaux de neurones qui int√®grent explicitement les lois de la physique dans leur architecture ou leur fonction de perte.
        
        **Notre approche hybride** :
        - **Mod√®le physique** : Fournit la structure et les contraintes physiques (bilan hydrique FAO)
        - **Neural CDE** : Apprend une correction r√©siduelle bas√©e sur une s√©quence d'√©tats pass√©s, respectant la structure physique
        
        **Caract√©ristiques physics-informed** :
        - ‚úÖ **Int√©gration explicite de la physique** : Le mod√®le physique FAO est int√©gr√© directement dans l'architecture
        - ‚úÖ **Respect des contraintes physiques** : La correction $\\Delta \\psi$ est appliqu√©e de mani√®re coh√©rente avec le mod√®le physique
        - ‚úÖ **M√©moire temporelle guid√©e par la physique** : Le Neural CDE utilise l'historique des √©tats physiques pour produire une correction coh√©rente
        - ‚úÖ **Apprentissage guid√© par la physique** : Le mod√®le apprend √† partir de donn√©es mais dans le contexte d'un mod√®le physique avec d√©pendances temporelles
        
        **Avantage par rapport aux PINN classiques** :
        - **PINN classiques** : G√©n√©ralement sans m√©moire, mod√©lisent des processus instantan√©s
        - **Notre Neural CDE** : Capture des d√©pendances temporelles longues tout en respectant la physique, permettant de mod√©liser des effets cumulatifs (s√©chage progressif, pluies r√©p√©t√©es, etc.)
        
        **Conclusion** :
        Notre mod√®le Neural CDE est un **mod√®le hybride physics-informed avec m√©moire temporelle** qui combine :
        - La robustesse et l'interpr√©tabilit√© du mod√®le physique
        - La flexibilit√© d'apprentissage des r√©seaux de neurones
        - La capacit√© de capturer des d√©pendances temporelles longues
        """)
    
    with st.expander("‚öñÔ∏è Avantages et limitations", expanded=False):
        st.markdown("""
        ### ‚úÖ Avantages du mod√®le hybride Neural CDE
        
        - **Pr√©cision am√©lior√©e** : Capture les biais syst√©matiques du mod√®le physique avec m√©moire temporelle
        - **D√©pendances temporelles** : Mod√©lise des effets √† long terme et des dynamiques complexes
        - **Interpr√©tabilit√© pr√©serv√©e** : Le mod√®le physique reste la base, la correction est additive
        - **Efficacit√© computationnelle** : Inf√©rence rapide (s√©quentielle mais parall√©lisable)
        - **Flexibilit√©** : Peut √™tre r√©-entra√Æn√© avec de nouvelles donn√©es
        - **Robustesse** : Le mod√®le physique garantit des pr√©dictions dans des plages r√©alistes
        - **M√©moire** : Exploite l'historique des √©tats pour de meilleures pr√©dictions
        
        ### ‚ö†Ô∏è Limitations
        
        - **Donn√©es d'entra√Ænement** : N√©cessite des donn√©es s√©quentielles pour apprendre la correction
        - **Complexit√© computationnelle** : N√©cessite $L$ √©valuations du r√©seau par pr√©diction
        - **M√©moire** : Doit maintenir un historique de $L$ √©tats
        - **Hyperparam√®tres** : N√©cessite de choisir $L$ (longueur de s√©quence) et $d$ (dimension de l'√©tat latent)
        - **G√©n√©ralisation** : Peut ne pas g√©n√©raliser √† des conditions tr√®s diff√©rentes de l'entra√Ænement
        - **Temps d'entra√Ænement** : Plus long que Neural ODE
        """)
    
    with st.expander("üîß Param√®tres recommand√©s et tuning", expanded=False):
        st.markdown("""
        ### Hyperparam√®tres du Neural CDE
        
        **Longueur de s√©quence** ($L$) : 5-10 (recommand√©)
        - Trop court (< 5) : Pas assez de m√©moire temporelle
        - Trop long (> 15) : Complexit√© computationnelle √©lev√©e, risque de surapprentissage
        - Tuning : Commencer √† 8, ajuster selon r√©sultats
        
        **Dimension de l'√©tat latent** ($d$) : 16-32 (recommand√©)
        - Plus grande : Plus de capacit√© mais plus complexe
        - Plus petite : Plus simple mais moins de capacit√©
        - Tuning : Commencer √† 16, augmenter si n√©cessaire
        
        **Architecture du r√©seau** :
        - **Nombre de couches cach√©es** : 2-3
        - **Dimension cach√©e** : 64-128
        - **Activation** : ReLU ou Tanh
        
        **Pr√©-entra√Ænement** :
        - **Nombre de trajectoires** : 50-100 (plus que Neural ODE)
        - **Nombre d'epochs** : 30-60 selon convergence
        - **Batch size** : 32-64 (plus petit que Neural ODE)
        - **Learning rate** : $10^{-3}$ (recommand√©)
        
        **Hyperparam√®tres PPO** :
        - Identiques au Sc√©nario 2
        - Learning rate : $3 \\times 10^{-4}$
        - Gamma : 0.99
        
        ### Strat√©gie de tuning
        
        **1. Choix de $L$** :
        - Commencer avec $L = 8$
        - Augmenter si besoin de m√©moire plus longue
        - R√©duire si complexit√© trop √©lev√©e
        
        **2. Pr√©-entra√Ænement** :
        - Plus de trajectoires que Neural ODE (m√©moire n√©cessite plus de donn√©es)
        - V√©rifier convergence de la loss
        - Analyser la qualit√© de la correction
        
        **3. Entra√Ænement PPO** :
        - Comme Sc√©nario 3
        - Comparer avec Sc√©nario 3 pour √©valuer gain
        """)
    
    with st.expander("üß≠ Quand utiliser le Sc√©nario 4 ?", expanded=False):
        st.markdown("""
        ### ‚úÖ Choisir le Sc√©nario 4 si :
        
        - **D√©pendances temporelles importantes** :
          - Effets √† long terme des d√©cisions d'irrigation
          - Dynamiques complexes n√©cessitant m√©moire
          - Sc√©nario 3 (Neural ODE) insuffisant
        
        - **Donn√©es s√©quentielles disponibles** :
          - Historique de mesures disponible
          - Donn√©es temporelles de qualit√©
          - S√©ries temporelles compl√®tes
        
        - **Biais temporels du mod√®le** :
          - Mod√®le physique ne capture pas bien les effets retard√©s
          - N√©cessit√© de corriger avec m√©moire temporelle
          - Ph√©nom√®nes avec inertie (drainage, remont√©e capillaire)
        
        - **Performance maximale recherch√©e** :
          - Sc√©nario 3 ne suffit pas
          - Besoin de meilleure pr√©cision
          - Ressources computationnelles disponibles
        
        ### ‚ùå Ne pas choisir le Sc√©nario 4 si :
        
        - **Pas de d√©pendances temporelles** :
          - Effets locaux uniquement
          - Sc√©nario 3 (Neural ODE) suffit
          - Pas besoin de m√©moire
        
        - **Ressources limit√©es** :
          - Complexit√© computationnelle trop √©lev√©e
          - Temps d'entra√Ænement trop long
          - M√©moire insuffisante
        
        - **Donn√©es insuffisantes** :
          - Pas assez de donn√©es s√©quentielles
          - Qualit√© des donn√©es insuffisante
          - ‚Üí Pr√©f√©rer Sc√©nario 3
        
        - **Simplicit√© recherch√©e** :
          - Approche simple suffit
          - ‚Üí Pr√©f√©rer Sc√©narios 1-3
        """)
    
    with st.expander("üõ†Ô∏è Conseils pratiques", expanded=False):
        st.markdown("""
        ### Workflow recommand√©
        
        **1. √âvaluer besoin de m√©moire** :
        - Analyser si Sc√©nario 3 suffit
        - Identifier d√©pendances temporelles
        - Quantifier gain potentiel
        
        **2. Choix de $L$** :
        - Commencer avec $L = 8$
        - Tester avec $L = 5, 10, 12$
        - Choisir selon performance/complexit√©
        
        **3. Pr√©-entra√Ænement** :
        - G√©n√©rer plus de trajectoires que Neural ODE
        - Entra√Æner Neural CDE
        - V√©rifier qualit√© de correction
        
        **4. Entra√Ænement PPO** :
        - Comme Sc√©nario 3
        - Comparer performances
        
        ### Troubleshooting
        
        **Probl√®me : Complexit√© computationnelle trop √©lev√©e**
        - **Sympt√¥me** : Entra√Ænement tr√®s lent
        - **Solutions** :
          - R√©duire $L$ (ex: 8 ‚Üí 5)
          - R√©duire dimension latente $d$
          - R√©duire batch size
        
        **Probl√®me : Pas d'am√©lioration vs Sc√©nario 3**
        - **Sympt√¥me** : Performance similaire
        - **Solutions** :
          - V√©rifier que $L$ est adapt√©
          - Augmenter nombre de trajectoires
          - V√©rifier qualit√© des donn√©es s√©quentielles
        
        **Probl√®me : Surapprentissage**
        - **Sympt√¥me** : Bonne performance entra√Ænement, mauvaise g√©n√©ralisation
        - **Solutions** :
          - R√©duire $L$
          - Ajouter r√©gularisation
          - Augmenter nombre de trajectoires d'entra√Ænement
        """)
    
    with st.expander("üîó Comparaison avec les autres sc√©narios", expanded=False):
        st.markdown("""
        ### Sc√©nario 4 vs Sc√©nario 3 (Neural ODE)
        
        **Sc√©nario 3** :
        - Neural ODE : Pas de m√©moire
        - Correction locale
        
        **Sc√©nario 4** :
        - Neural CDE : M√©moire temporelle
        - Correction avec historique
        
        **Quand choisir Sc√©nario 4** : Besoin de d√©pendances temporelles
        ### Sc√©nario 4 vs Sc√©narios 1-2
        
        **Sc√©nario 4** :
        - Correction r√©siduelle avec m√©moire
        - Plus complexe
        
        **Sc√©narios 1-2** :
        - Pas de correction
        - Plus simple
        
        **Quand choisir Sc√©nario 4** : Biais temporels du mod√®le physique
        """)
    
def render_doc_scenarios():
    """
    Affiche le contenu de l'onglet de documentation : Sc√©narios.
    """
    st.markdown('<h2 class="section-header">üìã Les sc√©narios d\'√©tude : du mod√®le physique au jumeau num√©rique cognitif</h2>', unsafe_allow_html=True)
    
    st.markdown("""
    Pr√©sentation des sc√©narios et de leur principe g√©n√©ral.
    """)
    
    st.markdown("### üå± Sc√©nario 1 ‚Äî Mod√®le physique + r√®gle simple")
    st.markdown("""
    - **Principe**: utiliser un mod√®le bucket (bilan hydrique) et une r√®gle fixe d'irrigation bas√©e sur un seuil de $\\psi$.
    """)
    
    with st.expander("üìã Types de mod√®les physiques disponibles", expanded=False):
        st.markdown("""
        - **FAO**
          - Mod√®le de bilan hydrique simplifi√© inspir√© de la m√©thodologie FAO-56
          - **√âquations du mod√®le** :
            - Bilan hydrique : $S_{t+1} = S_t + \\eta_I I_t + R_t + G_t - ETc_t - D(S_t) - Q_t$
            - Courbe de r√©tention : $\\psi_{t+1} = f_{\\text{retention}}(S_{t+1})$
            - o√π $S_t$ est la r√©serve en eau (mm), $\\psi_t$ la tension matricielle (cbar), $I_t$ l'irrigation (mm), $R_t$ la pluie (mm), $G_t$ la remont√©e capillaire (mm), $ETc_t$ l'√©vapotranspiration culture (mm), $D(S_t)$ le drainage (mm), $Q_t$ le ruissellement (mm), et $\\eta_I$ l'efficacit√© d'irrigation
          - Utilise un mod√®le bucket (r√©servoir), le coefficient cultural $Kc$ et l'√©vapotranspiration de r√©f√©rence $ET0$
          - Bilan hydrique journalier avec courbe de r√©tention simplifi√©e
          - Adapt√© pour des applications pratiques et rapides
        
        - **HYDRUS**
          - Mod√®le sophistiqu√© r√©solvant l'√©quation de Richards (√©quation diff√©rentielle partielle)
          - Mod√©lise le flux d'eau dans le sol en 1D, 2D ou 3D avec r√©solution num√©rique
          - Utilise des courbes de r√©tention compl√®tes (van Genuchten)
          - Tr√®s pr√©cis mais complexe et co√ªteux en calcul
        
        - **Aquacrop**
          - Mod√®le FAO avanc√© incluant la croissance de la culture, le d√©veloppement des racines, et des processus biologiques d√©taill√©s
          - Plus sophistiqu√© que le mod√®le bucket simple mais toujours bas√© sur les concepts FAO ($Kc$, $ET0$)
          - Adapt√© pour la mod√©lisation compl√®te du syst√®me culture-sol
        """)
    
    with st.expander("üîß Mod√®le impl√©ment√© : FAO", expanded=False):
        st.markdown("""
        **Raisons du choix** :
        
        - **Simplicit√© et rapidit√©** : Le mod√®le bucket permet des calculs instantan√©s, essentiel pour l'apprentissage par renforcement qui n√©cessite de nombreuses simulations
        
        - **Concepts FAO standardis√©s** : Utilisation de $Kc$ et $ET0$ (m√©thodologie FAO-56) reconnus et valid√©s internationalement
        
        - **Ad√©quation avec les observations** : Le mod√®le utilise directement la tension $\\psi_t$ mesur√©e par les tensiom√®tres, variable cl√© pour l'irrigation
        
        - **Efficacit√© computationnelle** : Pas de r√©solution d'√©quations diff√©rentielles complexes, permettant des milliers d'√©pisodes d'entra√Ænement RL en temps raisonnable
        
        - **Compromis pr√©cision/complexit√©** : Suffisamment pr√©cis pour capturer la dynamique essentielle du bilan hydrique tout en restant simple √† impl√©menter et calibrer
        
        - **Compatibilit√© RL** : La structure simple du mod√®le bucket facilite l'int√©gration avec les algorithmes d'apprentissage par renforcement
        """)
    
    with st.expander("‚öñÔ∏è Avantages et limitations", expanded=False):
        st.markdown("""
        ### ‚úÖ Avantages du Sc√©nario 1
        
        - **Simplicit√©** : 
          - Impl√©mentation directe : r√®gles bas√©es sur des seuils de tension ($\\psi$) faciles √† comprendre
          - Pas besoin d'entra√Ænement : r√®gles d√©finies manuellement, pas de phase d'apprentissage
          - D√©ploiement imm√©diat : peut √™tre mis en place rapidement sans infrastructure complexe
        
        - **Interpr√©tabilit√©** : 
          - Logique transparente : "si $\\psi_t > \\psi_{\\text{seuil}}$, alors irriguer $I_{\\text{fixe}}$"
          - Facile √† expliquer aux agriculteurs : r√®gles compr√©hensibles sans expertise en IA
          - D√©bogage simple : comportement pr√©visible et tra√ßable
        
        - **Rapidit√© d'ex√©cution** : 
          - Calculs instantan√©s : √©valuation de la r√®gle en temps constant
          - Pas de calculs lourds : pas de r√©seau de neurones √† √©valuer
          - Adapt√© aux syst√®mes embarqu√©s : faible consommation de ressources
        
        - **Robustesse** : 
          - Comportement stable : pas de variabilit√© due √† l'apprentissage
          - Pas de sur-apprentissage : r√®gles fixes garantissent un comportement coh√©rent
          - Pr√©visible : r√©sultats reproductibles pour les m√™mes conditions
        
        - **Co√ªt faible** : 
          - Pas d'infrastructure d'entra√Ænement n√©cessaire
          - Maintenance minimale : r√®gles simples √† maintenir
          - Pas de donn√©es d'entra√Ænement requises
        
        ### ‚ö†Ô∏è Limitations du Sc√©nario 1
        
        - **Manque d'adaptabilit√©** : 
          - Conditions m√©t√©orologiques : ne prend pas en compte les pr√©visions de pluie ou d'ET0 de mani√®re optimale
          - Variabilit√© saisonni√®re : seuils fixes ne s'adaptent pas aux changements de saison
          - Conditions locales : r√®gles g√©n√©riques qui ne s'adaptent pas aux sp√©cificit√©s de chaque parcelle
        
        - **Pas d'optimisation** : 
          - Efficacit√© de l'eau : ne minimise pas n√©cessairement la consommation d'eau
          - √âquilibre stress/co√ªt : ne trouve pas le compromis optimal entre stress hydrique et co√ªt de l'eau
          - Pas d'apprentissage : ne s'am√©liore pas avec l'exp√©rience
        
        - **Rigidit√© des r√®gles** : 
          - Seuils fixes : ne s'adaptent pas aux variations de conditions
          - Doses fixes : irrigation toujours de la m√™me quantit√©, sans gradation fine
          - Pas de strat√©gie pr√©ventive : r√©agit seulement quand le seuil est d√©pass√©, pas de pr√©vision
        
        - **Performance sous-optimale** : 
          - Gaspillage potentiel : peut irriguer m√™me si la pluie est imminente
          - Stress hydrique : peut laisser le sol se dess√©cher avant d'intervenir
          - Drainage excessif : peut provoquer des pertes d'eau par drainage si irrigation mal calibr√©e
        
        - **Maintenance manuelle** : 
          - Calibration n√©cessaire : les seuils doivent √™tre ajust√©s manuellement selon les conditions
          - Pas d'auto-ajustement : n√©cessite une intervention humaine pour optimiser les param√®tres
          - Expertise requise : besoin de connaissances agronomiques pour d√©finir les bons seuils
        """)
    
    with st.expander("üîß Param√®tres recommand√©s et tuning", expanded=False):
        st.markdown("""
        ### Param√®tres des r√®gles d'irrigation
        
        **R√®gle √† seuil unique** :
        - **Seuil de tension** ($\\psi_{\\text{seuil}}$) : 40-60 cbar (recommand√©)
          - Trop bas (< 30) : Irrigation trop fr√©quente, gaspillage d'eau
          - Trop √©lev√© (> 80) : Stress hydrique important avant irrigation
        - **Dose d'irrigation** : 10-20 mm (recommand√©)
          - Doit √™tre adapt√©e √† la capacit√© du sol et √† la culture
        
        **R√®gle √† bande de confort** :
        - **Tension minimale** ($\\psi_{\\min}$) : 20-30 cbar
        - **Tension maximale** ($\\psi_{\\max}$) : 50-70 cbar
        - **Dose proportionnelle** : Ajust√©e selon l'√©cart √† la zone de confort
        
        **R√®gle proportionnelle** :
        - **Coefficient de proportionnalit√©** ($k_I$) : 0.1-0.3
          - Plus √©lev√© = irrigation plus agressive
          - Plus faible = irrigation plus conservatrice
        
        ### Tuning recommand√©
        
        **√âtape 1 : Calibration initiale** :
        - Commencer avec des valeurs standard (seuil = 50 cbar, dose = 15 mm)
        - Observer le comportement sur une saison compl√®te
        
        **√âtape 2 : Ajustement selon r√©sultats** :
        - Si stress hydrique fr√©quent : R√©duire le seuil ou augmenter la dose
        - Si gaspillage d'eau : Augmenter le seuil ou r√©duire la dose
        - Si drainage excessif : R√©duire la dose
        
        **√âtape 3 : Ajustement saisonnier** :
        - Adapter les seuils selon la phase de croissance (Kc variable)
        - Tenir compte des pr√©visions m√©t√©orologiques
        """)
    
    with st.expander("üß≠ Quand utiliser le Sc√©nario 1 ?", expanded=False):
        st.markdown("""
        ### ‚úÖ Choisir le Sc√©nario 1 si :
        
        - **Simplicit√© recherch√©e** :
          - Vous voulez une solution simple et rapide √† d√©ployer
          - Pas d'infrastructure d'entra√Ænement disponible
          - Besoin de r√©sultats imm√©diats sans phase d'apprentissage
        
        - **Interpr√©tabilit√© importante** :
          - Les r√®gles doivent √™tre compr√©hensibles par les utilisateurs finaux
          - Besoin d'expliquer facilement les d√©cisions prises
          - Conformit√© r√©glementaire n√©cessitant de la transparence
        
        - **Ressources limit√©es** :
          - Pas de ressources computationnelles pour l'entra√Ænement RL
          - Pas de donn√©es historiques suffisantes
          - Syst√®me embarqu√© avec contraintes de calcul
        
        - **Conditions stables** :
          - Conditions m√©t√©orologiques et p√©dologiques relativement stables
          - Pas de variabilit√© saisonni√®re importante
          - Parcelle bien caract√©ris√©e avec param√®tres connus
        
        - **Baseline de r√©f√©rence** :
          - Point de d√©part pour comparer avec d'autres approches
          - Validation du mod√®le physique avant d'ajouter de la complexit√©
        
        ### ‚ùå Ne pas choisir le Sc√©nario 1 si :
        
        - **Optimisation n√©cessaire** :
          - Besoin de minimiser la consommation d'eau
          - Recherche du compromis optimal stress/co√ªt
          - Conditions variables n√©cessitant adaptation
        
        - **Donn√©es disponibles** :
          - Vous avez des donn√©es historiques pour entra√Æner un mod√®le RL
          - Possibilit√© de g√©n√©rer des simulations pour l'entra√Ænement
        
        - **Performance maximale recherch√©e** :
          - Les r√®gles simples ne suffisent pas pour vos objectifs
          - Besoin d'une strat√©gie adaptative et optimis√©e
        """)
    
    with st.expander("üõ†Ô∏è Conseils pratiques", expanded=False):
        st.markdown("""
        ### Workflow recommand√©
        
        **1. Calibration initiale** :
        - Utiliser les valeurs par d√©faut comme point de d√©part
        - Observer le comportement sur une saison compl√®te
        - Documenter les performances (stress, consommation d'eau, drainage)
        
        **2. Ajustement it√©ratif** :
        - Ajuster un param√®tre √† la fois (seuil OU dose, pas les deux)
        - Tester sur plusieurs saisons avec conditions vari√©es
        - Comparer les m√©triques avant/apr√®s ajustement
        
        **3. Validation** :
        - V√©rifier la coh√©rence avec les connaissances agronomiques
        - Comparer avec les pratiques locales
        - Valider sur diff√©rentes conditions m√©t√©orologiques
        
        ### Troubleshooting
        
        **Probl√®me : Stress hydrique fr√©quent**
        - **Solution** : R√©duire le seuil de tension (ex: 50 ‚Üí 40 cbar) ou augmenter la dose
        
        **Probl√®me : Gaspillage d'eau**
        - **Solution** : Augmenter le seuil (ex: 50 ‚Üí 60 cbar) ou r√©duire la dose
        
        **Probl√®me : Drainage excessif**
        - **Solution** : R√©duire la dose d'irrigation ou augmenter le seuil
        
        **Probl√®me : Irrigation trop tardive**
        - **Solution** : Utiliser une r√®gle pr√©ventive ou r√©duire le seuil
        """)
    
    with st.expander("üîó Comparaison avec les autres sc√©narios", expanded=False):
        st.markdown("""
        ### Sc√©nario 1 vs Sc√©nario 2 (RL basique)
        
        **Sc√©nario 1** :
        - R√®gles fixes, pas d'apprentissage
        - Simple et rapide
        - Performance sous-optimale
        
        **Sc√©nario 2** :
        - Apprentissage automatique de la politique
        - Plus complexe mais meilleure performance
        - N√©cessite entra√Ænement
        
        **Quand choisir Sc√©nario 1** : Simplicit√© et rapidit√© prioritaires
        
        ### Sc√©nario 1 vs Sc√©narios 3-6
        
        **Sc√©nario 1** :
        - Baseline simple
        - Pas d'optimisation
        - Pas d'adaptation
        
        **Sc√©narios 3-6** :
        - Approches avanc√©es avec apprentissage
        - Optimisation et adaptation
        - Meilleure performance mais plus complexe
        
        **Relation** : Sc√©nario 1 sert de r√©f√©rence pour √©valuer les gains des autres sc√©narios
        """)
    
    st.markdown("### üéì Sc√©nario 2 ‚Äî RL sur mod√®le physique (avec $\\psi_t$ observ√©e)")
    st.markdown("""
    - **Principe**: un agent RL observe $\\psi_t$ (et le contexte m√©t√©o) et choisit $I_t$ dans un environnement simul√© par le mod√®le physique.
      - **Espace d'observation**: $o_t = (\\psi_t,\\ t/T,\\ R_{t-k:t},\\ ET0_t,\\ \\hat R_{t+1:t+h},\\ \\widehat{ET0}_{t+1:t+h})$
      - **Espace d'actions**: $I_t \\in [0,\\ I_{\\max}]$ (mm) - continu
      - **R√©compense**: $r_t = -\\alpha\\,\\text{stress}(\\psi_t) - \\beta\\, I_t - \\gamma\\, D(S_t)$
      - **Algorithme**: PPO (Proximal Policy Optimization)
    """)
    
    with st.expander("‚öñÔ∏è Avantages et limitations", expanded=False):
        st.markdown("""
        ### ‚úÖ Avantages du Sc√©nario 2
        
        - **Apprentissage d'une politique optimale** : 
          - Optimisation automatique : l'agent RL apprend √† minimiser le stress hydrique tout en √©conomisant l'eau
          - Compromis optimal : trouve automatiquement le meilleur √©quilibre entre performance agronomique et co√ªt de l'eau
          - Strat√©gie adaptative : s'ajuste selon les conditions m√©t√©orologiques et l'√©tat du sol
        
        - **Adaptabilit√© aux conditions** : 
          - Pr√©visions m√©t√©o : utilise les pr√©visions de pluie et d'ET0 pour anticiper et ajuster l'irrigation
          - Conditions variables : s'adapte aux variations saisonni√®res et aux √©v√©nements m√©t√©orologiques
          - Historique contextuel : prend en compte l'historique r√©cent (pluie, tension) pour des d√©cisions inform√©es
        
        - **Respect de la physique** : 
          - Mod√®le physique fiable : utilise un mod√®le bucket valid√© pour simuler la dynamique du sol
          - Courbe de r√©tention : respecte la relation $S \\leftrightarrow \\psi$ bas√©e sur les propri√©t√©s p√©dophysiques
          - Bilan hydrique coh√©rent : les √©quations physiques garantissent la coh√©rence des pr√©dictions
        
        - **Flexibilit√© des actions** : 
          - Actions continues : permet des doses d'irrigation pr√©cises et gradu√©es (pas seulement 0 ou dose fixe)
          - Doses adaptatives : ajuste la quantit√© d'eau selon l'intensit√© du stress et les conditions
          - Strat√©gie pr√©ventive : peut irriguer pr√©ventivement avant que le stress ne devienne critique
        
        - **Performance sup√©rieure** : 
          - Efficacit√© de l'eau : g√©n√©ralement meilleure que les r√®gles fixes en termes de consommation d'eau
          - R√©duction du stress : maintient mieux la tension dans la zone de confort
          - Minimisation du drainage : apprend √† √©viter les pertes d'eau par drainage excessif
        
        - **R√©utilisabilit√©** : 
          - Mod√®le entra√Æn√© : une fois entra√Æn√©, le mod√®le peut √™tre utilis√© sur diff√©rentes saisons
          - Transfert possible : peut √™tre adapt√© √† d'autres parcelles avec r√©-entra√Ænement
          - Am√©lioration continue : peut √™tre r√©-entra√Æn√© avec de nouvelles donn√©es pour s'am√©liorer
        
        ### ‚ö†Ô∏è Limitations du Sc√©nario 2
        
        - **D√©pendance √† la qualit√© du mod√®le physique** : 
          - Biais du mod√®le : si le mod√®le bucket a des biais (param√®tres mal calibr√©s, processus n√©glig√©s), 
            la politique apprise sera biais√©e
          - Erreurs de param√©trisation : des erreurs dans les param√®tres du sol ($S_{fc}$, $\\psi_{fc}$, $k_d$) 
            se propagent dans les d√©cisions
          - Processus non mod√©lis√©s : ph√©nom√®nes non captur√©s par le mod√®le (h√©t√©rog√©n√©it√© spatiale, 
            interactions complexes) ne sont pas pris en compte
        
        - **Phase d'entra√Ænement n√©cessaire** : 
          - Temps d'entra√Ænement : n√©cessite une phase d'apprentissage (plusieurs milliers de timesteps) 
            avant d'√™tre utilisable
          - Ressources computationnelles : entra√Ænement PPO n√©cessite des ressources CPU/GPU
          - Expertise technique : n√©cessite des comp√©tences en RL pour l'entra√Ænement et le r√©glage
        
        - **Donn√©es d'entra√Ænement** : 
          - Simulation requise : besoin de g√©n√©rer des donn√©es de simulation pour l'entra√Ænement
          - Qualit√© de la simulation : la qualit√© de l'entra√Ænement d√©pend de la qualit√© de la simulation m√©t√©o
          - Robustesse : n√©cessite d'entra√Æner sur plusieurs saisons/sc√©narios pour √™tre robuste
        
        - **Complexit√© de d√©ploiement** : 
          - Infrastructure : n√©cessite une infrastructure pour ex√©cuter le mod√®le entra√Æn√©
          - Maintenance : le mod√®le peut n√©cessiter un r√©-entra√Ænement p√©riodique
          - Interpr√©tabilit√© r√©duite : moins interpr√©table que les r√®gles simples (bo√Æte noire)
        
        - **Hyperparam√®tres √† r√©gler** : 
          - Tuning n√©cessaire : nombreux hyperparam√®tres √† ajuster (learning rate, gamma, GAE-$\\lambda$, etc.)
          - Sensibilit√© : la performance peut √™tre sensible aux choix d'hyperparam√®tres
          - Expertise requise : n√©cessite une compr√©hension du RL pour optimiser les hyperparam√®tres
        
        - **Stabilit√© de l'apprentissage** : 
          - Convergence : l'entra√Ænement peut ne pas converger ou converger vers un optimum local
          - Variabilit√© : la performance peut varier entre diff√©rentes ex√©cutions d'entra√Ænement
          - Normalisation : n√©cessite une normalisation soigneuse des observations et r√©compenses
        
        - **Observations coh√©rentes** : 
          - Alignement temporel : n√©cessite que les observations soient align√©es temporellement
          - Donn√©es manquantes : doit g√©rer les cas de donn√©es manquantes ou irr√©guli√®res
          - Pr√©visions m√©t√©o : d√©pend de la qualit√© des pr√©visions m√©t√©orologiques disponibles
        """)
    
    with st.expander("üîß Param√®tres recommand√©s et tuning", expanded=False):
        st.markdown("""
        ### Hyperparam√®tres PPO
        
        **Learning rate** : $3 \\times 10^{-4}$ (recommand√©)
        - Trop √©lev√© (> $10^{-3}$) : Instabilit√©, oscillations
        - Trop faible (< $10^{-5}$) : Apprentissage trop lent
        - Tuning : R√©duire si loss oscille, augmenter si convergence lente
        
        **Gamma (discount factor)** : 0.99 (recommand√©)
        - Contr√¥le l'importance des r√©compenses futures
        - √âlev√© (0.99) : Planification √† long terme
        - Faible (0.95) : Focus sur court terme
        
        **GAE lambda** : 0.95 (recommand√©)
        - Contr√¥le le biais/variance de l'estimation de la valeur
        - √âlev√© (0.95-0.99) : Moins de variance, plus de biais
        - Faible (0.8-0.9) : Plus de variance, moins de biais
        
        **Entropy coefficient** : 0.01-0.05
        - Encourage l'exploration
        - √âlev√© : Plus d'exploration, convergence plus lente
        - Faible : Moins d'exploration, risque de sous-optimum local
        
        **Clip range** : 0.2 (standard PPO)
        - Limite les changements de politique
        - √âlev√© : Permet plus de changements, moins stable
        - Faible : Changements limit√©s, plus stable
        
        **Batch size** : 64-256
        - Plus grand : Gradients plus stables mais plus de m√©moire
        - Plus petit : Moins de m√©moire mais gradients plus variables
        
        **Number of steps per rollout** : 2048
        - Plus grand : Meilleure estimation mais plus de m√©moire
        - Plus petit : Moins de m√©moire mais estimation moins pr√©cise
        
        ### Hyperparam√®tres de l'environnement
        
        **Param√®tres de r√©compense** :
        - $\\alpha$ (p√©nalit√© stress) : 1.0 (recommand√©)
        - $\\beta$ (p√©nalit√© irrigation) : 0.05 (recommand√©)
        - $\\gamma$ (p√©nalit√© drainage) : 0.01 (recommand√©)
        - Tuning : Ajuster selon priorit√©s (eau vs stress)
        
        **Param√®tres du sol** :
        - Utiliser les valeurs par d√©faut sauf si donn√©es sp√©cifiques disponibles
        - Calibrer $S_{fc}$, $\\psi_{fc}$ selon mesures r√©elles si possible
        
        ### Strat√©gie de tuning
        
        **1. Commencer avec valeurs par d√©faut** :
        - Utiliser les valeurs recommand√©es ci-dessus
        - Entra√Æner sur 50,000-100,000 timesteps
        
        **2. Observer les m√©triques** :
        - R√©compense moyenne : Doit augmenter
        - Longueur d'√©pisode : Doit √™tre stable
        - Variance des actions : Ne doit pas exploser
        
        **3. Ajuster si n√©cessaire** :
        - Si instabilit√© : R√©duire learning rate, augmenter clip range
        - Si convergence lente : Augmenter learning rate, r√©duire entropy
        - Si sous-optimum : Augmenter entropy, r√©duire clip range
        """)
    
    with st.expander("üß≠ Quand utiliser le Sc√©nario 2 ?", expanded=False):
        st.markdown("""
        ### ‚úÖ Choisir le Sc√©nario 2 si :
        
        - **Optimisation recherch√©e** :
          - Besoin de minimiser la consommation d'eau
          - Recherche du compromis optimal stress/co√ªt
          - Performance sup√©rieure aux r√®gles simples
        
        - **Donn√©es disponibles** :
          - Possibilit√© de g√©n√©rer des simulations pour l'entra√Ænement
          - Mod√®le physique fiable et bien calibr√©
          - Conditions m√©t√©orologiques vari√©es pour robustesse
        
        - **Ressources computationnelles** :
          - Infrastructure disponible pour l'entra√Ænement PPO
          - Temps d'entra√Ænement acceptable (quelques heures)
          - Expertise en RL disponible
        
        - **Adaptabilit√© n√©cessaire** :
          - Conditions variables n√©cessitant adaptation
          - Besoin de strat√©gie pr√©ventive
          - Optimisation selon objectifs multiples
        
        - **Point de d√©part pour approches avanc√©es** :
          - Baseline pour comparer avec Sc√©narios 3-6
          - Validation de l'approche RL avant complexification
        
        ### ‚ùå Ne pas choisir le Sc√©nario 2 si :
        
        - **Simplicit√© prioritaire** :
          - Besoin de solution simple et rapide
          - Pas d'infrastructure d'entra√Ænement
          - R√®gles simples suffisent
        
        - **Mod√®le physique incertain** :
          - Param√®tres du sol mal connus
          - Mod√®le physique non valid√©
          - Donn√©es de simulation de mauvaise qualit√©
        
        - **Donn√©es limit√©es** :
          - Pas de possibilit√© de g√©n√©rer des simulations
          - Conditions trop sp√©cifiques pour g√©n√©raliser
        
        - **Besoin de correction physique** :
          - Mod√®le physique a des biais connus
          - N√©cessit√© de corriger les pr√©dictions physiques
          - ‚Üí Pr√©f√©rer Sc√©narios 3-4
        """)
    
    with st.expander("üõ†Ô∏è Conseils pratiques", expanded=False):
        st.markdown("""
        ### Workflow recommand√©
        
        **1. Pr√©paration** :
        - V√©rifier la coh√©rence m√©t√©o (m√™mes seeds/params que Sc√©nario 1)
        - Valider le mod√®le physique sur quelques √©pisodes
        - Configurer les hyperparam√®tres avec valeurs par d√©faut
        
        **2. Entra√Ænement initial** :
        - Commencer avec 50,000 timesteps
        - Observer les m√©triques (r√©compense, longueur d'√©pisode)
        - V√©rifier la convergence
        
        **3. Tuning it√©ratif** :
        - Ajuster les hyperparam√®tres si n√©cessaire
        - R√©-entra√Æner avec nouveaux param√®tres
        - Comparer les performances
        
        **4. √âvaluation** :
        - Tester sur nouvelles saisons (seeds diff√©rents)
        - Comparer avec Sc√©nario 1 (baseline)
        - Analyser les d√©cisions prises
        
        ### Troubleshooting
        
        **Probl√®me : Instabilit√© de l'entra√Ænement**
        - **Sympt√¥me** : Loss oscille, r√©compense ne converge pas
        - **Solutions** :
          - R√©duire learning rate (ex: $3 \\times 10^{-4} \\to 10^{-4}$)
          - Augmenter clip range (ex: 0.2 ‚Üí 0.3)
          - Normaliser les observations et r√©compenses
        
        **Probl√®me : Convergence lente**
        - **Sympt√¥me** : R√©compense augmente tr√®s lentement
        - **Solutions** :
          - Augmenter learning rate (avec prudence)
          - Augmenter entropy coefficient pour plus d'exploration
          - V√©rifier la normalisation des r√©compenses
        
        **Probl√®me : Sous-optimum local**
        - **Sympt√¥me** : Performance plafonne √† un niveau sous-optimal
        - **Solutions** :
          - Augmenter entropy coefficient
          - R√©duire clip range pour permettre plus de changements
          - Augmenter le nombre de timesteps d'entra√Ænement
        
        **Probl√®me : Politique trop conservatrice**
        - **Sympt√¥me** : Irrigation insuffisante, stress hydrique
        - **Solutions** :
          - Ajuster les poids de r√©compense ($\\alpha$ vs $\\beta$)
          - Augmenter la p√©nalit√© de stress ($\\alpha$)
          - R√©duire la p√©nalit√© d'irrigation ($\\beta$)
        
        ### M√©triques √† surveiller
        
        - **R√©compense moyenne** : Doit augmenter avec l'entra√Ænement
        - **Longueur d'√©pisode** : Doit √™tre stable (‚âà longueur de saison)
        - **Variance des actions** : Ne doit pas exploser (signe d'instabilit√©)
        - **Policy loss** : Doit d√©cro√Ætre et converger
        - **Value loss** : Doit d√©cro√Ætre (estimation de la valeur)
        """)
    
    with st.expander("üîó Comparaison avec les autres sc√©narios", expanded=False):
        st.markdown("""
        ### Sc√©nario 2 vs Sc√©nario 1 (R√®gles simples)
        
        **Sc√©nario 1** :
        - R√®gles fixes, pas d'apprentissage
        - Simple et rapide
        - Performance sous-optimale
        
        **Sc√©nario 2** :
        - Apprentissage automatique
        - Plus complexe mais meilleure performance
        - N√©cessite entra√Ænement
        
        **Quand choisir Sc√©nario 2** : Optimisation et adaptabilit√© recherch√©es
        
        ### Sc√©nario 2 vs Sc√©narios 3-4 (Neural ODE/CDE)
        
        **Sc√©nario 2** :
        - RL direct sur mod√®le physique
        - Pas de correction du mod√®le physique
        - Plus simple
        
        **Sc√©narios 3-4** :
        - Correction r√©siduelle du mod√®le physique
        - Am√©liore la pr√©diction physique
        - Plus complexe
        
        **Quand choisir Sc√©narios 3-4** : Mod√®le physique a des biais connus
        """)
    
    st.markdown("### üî¨ Sc√©nario 3 ‚Äî RL sur mod√®le hybride Physique + Neural ODE")
    st.markdown("""
    - **Principe**: corriger la pr√©diction physique de $\\psi_{t+1}$ par une correction neuronale locale $\\Delta \\psi$ 
      apprise √† partir de $(\\psi_t, I_t, R_t, ET0_t)$.
    """)
    
    with st.expander("‚öñÔ∏è Avantages et limitations", expanded=False):
        st.markdown("""
        ### ‚úÖ Avantages du Neural ODE
        
        - **Combinaison physique et donn√©es** : Le Neural ODE (Ordinary Differential Equation) combine le meilleur des deux mondes :
          
          - **Mod√®le physique comme base** : Le mod√®le bucket fournit une pr√©diction initiale $\\psi_{t+1}^{phys}$ bas√©e sur 
            les lois physiques connues (bilan hydrique, courbe de r√©tention). Cette base garantit que les pr√©dictions 
            respectent les principes fondamentaux de la dynamique du sol.
          
          - **Correction neuronale adaptative** : Un r√©seau de neurones apprend une correction locale $\\Delta \\psi$ qui 
            ajuste la pr√©diction physique en fonction des donn√©es observ√©es. Cette correction capture les ph√©nom√®nes 
            non mod√©lis√©s ou mal param√©tr√©s dans le mod√®le physique.
        
        - **Correction des biais du mod√®le** : Le Neural ODE peut corriger plusieurs types de biais :
          
          - **Biais de param√©trisation** : Si les param√®tres du sol (par exemple $S_{fc}$, $\\psi_{fc}$, $k_d$) sont mal 
            estim√©s ou varient dans l'espace, le Neural ODE apprend √† compenser ces erreurs.
          
          - **Ph√©nom√®nes non mod√©lis√©s** : Certains processus physiques peuvent √™tre n√©glig√©s ou simplifi√©s dans le mod√®le 
            bucket (par exemple, h√©t√©rog√©n√©it√© spatiale, effets de la structure du sol, interactions racines-sol complexes). 
            Le Neural ODE peut apprendre √† capturer ces effets √† partir des donn√©es.
          
          - **Erreurs de mesure** : Les capteurs peuvent avoir des biais syst√©matiques ou des erreurs de calibration. 
            Le Neural ODE peut apprendre √† les corriger si ces erreurs sont coh√©rentes dans le temps.
        
        - **Apprentissage continu** : Contrairement aux mod√®les purement physiques qui sont statiques, le Neural ODE 
          peut √™tre r√©-entra√Æn√© avec de nouvelles donn√©es pour s'adapter √† l'√©volution des conditions (par exemple, 
          changement de structure du sol, vieillissement des capteurs).
        
        - **Interpr√©tabilit√©** : La structure hybride permet de s√©parer la contribution du mod√®le physique (interpr√©table) 
          de la correction neuronale (qui peut √™tre analys√©e pour comprendre quels ph√©nom√®nes sont mal captur√©s).
        
        - **Efficacit√© computationnelle** : En utilisant le mod√®le physique comme base, le Neural ODE n√©cessite moins 
          de donn√©es et d'entra√Ænement qu'un mod√®le purement neuronal, tout en √©tant plus performant qu'un mod√®le 
          purement physique.
        
        ### ‚ö†Ô∏è Limitations du Neural ODE
        
        - **Donn√©es n√©cessaires** : N√©cessite des donn√©es r√©elles pour l'apprentissage du r√©seau de correction. 
          Plus les donn√©es sont nombreuses et repr√©sentatives, meilleure sera la correction.
        
        - **Hypoth√®se de r√©gularit√©** : Le Neural ODE suppose g√©n√©ralement des observations √† intervalles r√©guliers. 
          Les donn√©es manquantes ou irr√©guli√®res n√©cessitent un pr√©-traitement (interpolation, imputation).
        
        - **Complexit√©** : Plus complexe qu'un mod√®le purement physique, n√©cessite une expertise en deep learning 
          pour l'entra√Ænement et le r√©glage des hyperparam√®tres.
        
        - **D√©pendance aux donn√©es d'entra√Ænement** : La qualit√© de la correction d√©pend de la repr√©sentativit√© 
          des donn√©es d'entra√Ænement. Si les conditions changent significativement (nouveau type de sol, nouvelle culture), 
          le mod√®le peut n√©cessiter un r√©-entra√Ænement.
        """)
    
    st.markdown("### üß† Sc√©nario 4 ‚Äî RL sur mod√®le hybride Physique + Neural CDE")
    st.markdown("""
    - **Principe**: exploiter des trajectoires (possiblement irr√©guli√®res) $[\\psi, I, R, ET0]$ via un √©tat latent (CDE) 
      pour produire une correction temporelle coh√©rente.
    """)
    
    with st.expander("‚öñÔ∏è Avantages et limitations", expanded=False):
        st.markdown("""
        ### ‚úÖ Avantages du Neural CDE
        
        - **Gestion des donn√©es irr√©guli√®res** : Le Neural CDE (Controlled Differential Equation) est particuli√®rement 
          adapt√© aux s√©ries temporelles avec des observations √† intervalles irr√©guliers. En pratique, cela signifie :
          
          - **Fiabilit√© des capteurs** : Les tensiom√®tres et autres capteurs peuvent avoir des pannes temporaires, 
            des d√©faillances de communication, ou n√©cessiter des calibrations p√©riodiques. Le Neural CDE peut 
            g√©rer ces p√©riodes de donn√©es manquantes sans n√©cessiter d'interpolation artificielle.
          
          - **Fr√©quence d'√©chantillonnage variable** : Contrairement aux mod√®les classiques qui supposent des mesures 
            √† intervalles r√©guliers (par exemple, toutes les heures ou tous les jours), le Neural CDE peut traiter 
            des donn√©es qui arrivent √† des moments diff√©rents (ex. : mesure √† 8h un jour, √† 10h le lendemain, puis 
            aucune mesure pendant 2 jours).
          
          - **Robustesse aux pannes** : Si un capteur tombe en panne pendant plusieurs jours, le mod√®le peut continuer 
            √† fonctionner en utilisant les derni√®res observations valides et en extrapolant de mani√®re coh√©rente gr√¢ce 
            √† l'√©tat latent du CDE.
        
        - **Meilleure mod√©lisation temporelle** : Le CDE mod√©lise explicitement l'√©volution continue du syst√®me, 
          ce qui permet une meilleure compr√©hension de la dynamique du sol entre les observations.
        
        - **Adaptation aux contraintes op√©rationnelles** : Dans un contexte r√©el, les mesures peuvent √™tre prises 
          √† des moments opportuns (visites de terrain, maintenance), pas n√©cessairement √† intervalles fixes.
        
        ### ‚ö†Ô∏è Limitations du Neural CDE
        
        - **Complexit√©** : Plus complexe √† impl√©menter et √† entra√Æner que les mod√®les pr√©c√©dents
        - **Donn√©es n√©cessaires** : N√©cessite plus de donn√©es pour l'apprentissage, notamment pour calibrer 
          l'√©tat latent du CDE
        - **Temps de calcul** : G√©n√©ralement plus co√ªteux en temps de calcul que les approches plus simples
        """)

    st.markdown("### üìä Comparaison des sc√©narios")
    st.markdown("""
    | Sc√©nario | Complexit√© | Adaptabilit√© | Donn√©es n√©cessaires | Performance attendue |
    |----------|------------|--------------|---------------------|----------------------|
    | 1. R√®gle simple | Faible | Faible | Aucune | Basique |
    | 2. RL physique | Moyenne | √âlev√©e | Simulation | Bonne |
    | 3. RL + Neural ODE | √âlev√©e | Tr√®s √©lev√©e | R√©elles + Simulation | Tr√®s bonne |
    | 4. RL + Neural CDE | Tr√®s √©lev√©e | Tr√®s √©lev√©e | R√©elles + Simulation | Excellente |
    """)
    
